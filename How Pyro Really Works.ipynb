{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Pyro Really Works\n",
    "\n",
    "## or How to Write Yourself a Probabilistic Programming Framwork\n",
    "## or Please Try This at Home\n",
    "\n",
    "Ever wondered how pyro (or probabilistic programming in general) really works? How does it do variational inference, and what is that in the first place? It's conceptually simpler than one might think, because it follows from the combination of a handful of rather simple ideas. In this tutorial we will look at each of them in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Joint, Marginal, and Conditional Distributions\n",
    "\n",
    "#### Summary:\n",
    "\n",
    "- If we have a *joint distribution* ($p(A,B)$),\n",
    "  we can express *marginal* ($p(A)$) and *conditional distributions* $p(A \\mid B)$ based on the joint distribution\n",
    "- Conversely, joint distributions can always be *factorized* into a product of conditional and marginal distributions\n",
    "  $$ p(A,B) = p(A \\mid B) p(B) $$\n",
    "- Sampling the variables in some order gives rise to a factorization\n",
    "  $$ p(x_1, \\ldots, x_n) = \\prod_i p(x_i \\mid x_1, \\ldots x_{i-1}) $$\n",
    "  and describes a *generative process*.\n",
    "\n",
    "### Basics\n",
    "\n",
    "Probability distributions may be defined on several random variables (RVs), e.g. $A$, $B$, and $C$. The distribution over all variables $p(A,B,C)$ is called the *joint distribution*, which is normalized such that the sum over all possible RV assignments is 1. (In the continuous case, we integrate instead of summing.)\n",
    "\n",
    "The *marginal distribution* of a subset of the RVs is obtained from the joint distribution by summing (or integrating) over all remaining variables. for example\n",
    "$$ p(A) = \\sum_B \\sum_C p(A,B,C) $$\n",
    "and\n",
    "$$ p(A,B) = \\sum_C p(A,B,C) $$\n",
    "\n",
    "A *conditional distribution* is the distribution of some of the RVs given that other RVs are known to have a certain value, e.g. $p(A,B \\mid C=c)$.\n",
    "If the value of the known RV is arbitrary, we write $p(A,B \\mid C)$.\n",
    "If we are only interested in the distribution of some of the unknown RVs, we can again marginalize out the remaining unknown RVs, e.g.\n",
    "$$ p(A \\mid C) = \\sum_B p(A, B | C) $$\n",
    "\n",
    "We can obtain the conditional distribution from the joint distribution by fixing the value of the known RVs ($p(A,B,C=c)$).\n",
    "Since we now only look at a subset of the RV assignment (e.g. those in which $C=c$), we have to renormalize by the probability to have one of these assignments $p(C=c)$:\n",
    "$$ p(A,B \\mid C=c) = \\dfrac{p(A,B,C=c)}{p(C=c)}$$\n",
    "or more generally\n",
    "$$ p(A,B \\mid C) = \\dfrac{p(A,B,C)}{p(C)} $$\n",
    "\n",
    "### Factorizing a Joint Distribution\n",
    "\n",
    "From the above section it follows that every joint distribution can be factorized into simpler conditional and marginal distributions.\n",
    "For example, we can turn the previous equation around and obtain\n",
    "$$ p(A,B,C) = p(A,B \\mid C) p(C) $$\n",
    "\n",
    "Since $p(A,B \\mid C)$ can be understood as another joint distribution (of $A$ and $B$, given $C$), it can be factorized again (given $C$!), e.g.:\n",
    "$$ p(A,B \\mid C) = p(A \\mid B,C) p(B \\mid C) $$\n",
    "\n",
    "and consequently\n",
    "$$ p(A,B,C) = p(A,B \\mid C) p(C) = p(A \\mid B,C) p(B \\mid C) p(C). $$\n",
    "\n",
    "Note that the order in which we factor out variables doesn't matter, so the following is equivalent:\n",
    "$$ p(A,B,C) = p(B,C \\mid A) p(A) = p(B \\mid C,A) p(C \\mid A) p(A). $$\n",
    "\n",
    "We can understand any factorization as an ordering of the RVs when sampling from the joint distribution.\n",
    "Instead of sampling all of them together, we sample each RV from its conditional distribution, starting with the one that is sampled from its unconditional marginal distribution.\n",
    "For example, in the first factorization above ($p(A \\mid B,C) p(B \\mid C) p(C)$, we first sample $C$ from $p(C)$, which doesn't depend on any of the other RVs.\n",
    "Then, knowing the value $c$ of $C$, we can sample $B$ from $p(B \\mid C=c)$, and finally (knowing $B=b$ and $C=c$) we can sample $A$ from $p(A \\mid B=b, C=c)$.\n",
    "Thus we have turned the joint distribution $p(A,B,C)$ into a *generative process* that produces each RV in turn.\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1. Given the joint distribution $p(A,B,C,D)$, what is the marginal distribution $p(A,C)$?\n",
    "1. Given the same joint distribution $p(A,B,C,D)$, what is the conditional distribution $p(A,B | C)$?\n",
    "   Express it only using the joint distribution.\n",
    "1. Let $p(X,Y,Z) = p(Z \\mid X,Y) p(X) p(Y)$. What is $p(X \\mid Y$? What is $p(Y \\mid Z, X)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bayesian Inference\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- Bayesian models are defined by a *joint distribution* $p(X,Z)$\n",
    "  over observed ($X$) and unobserved variables ($Z$).\n",
    "- Bayesian inference is computing the *posterior distribution* $p(Z \\mid X)$.\n",
    "- Sometimes, the joint distribution is given as factorized into a *likelihood* $p(X \\mid Z)$ and a *prior* $p(Z)$.\n",
    "\n",
    "### Bayesian Models\n",
    "\n",
    "Bayesian inference builds on the \"bayesian\" interpretation of probabilities as talking about the plausibility of statements.\n",
    "BI starts from the assumption that we can define a joint *plausibility distribution* over observed ($X$) and unobserved variables ($Z$): $p(X,Z)$, which assings a plausibility $p(X=x, Z=z)$ to any particular instantiation of $X$ and $Z$.\n",
    "Doing *inference* means to asses the plausibility of latent variable assignments, given that the observed variables take some (observed) value: $p(Z \\mid X=x)$.\n",
    "\n",
    "#### Example\n",
    "\n",
    "While the above may sound very abstract, a common situation in which it is useful is when we have a probabilistic model (i.e. a probability distribution) of some data $d$, i.e. a distribution so that we can interpret the data $d$ as sampled from a random variable $D$.\n",
    "Our distribution over $D$ has some parameters $\\theta$, so we can write it as $p_\\theta(D)$.\n",
    "If we change $\\theta$, we change the distribution.\n",
    "\n",
    "Now we want a good value for $\\theta$.\n",
    "Since the distribution is suppose to \"model\" the data, we want $\\theta$ to be chosen such that $D=d$ has a very high probability under $p_\\theta$.\n",
    "In other words, we want to maximize the function $\\theta \\to p_\\theta(D=d)$.\n",
    "This function is called the *likelihood function* and the optimal value for $\\theta$ is called the *maximum likelihood estimate* (MLE).\n",
    "\n",
    "While we know that the MLE is the \"best\" value of $\\theta$ given $d$, we don't really know how it compares to other possible values, i.e. *how* good it really is.\n",
    "In other words, we want to know how *plausible* it is that some value of $\\theta$ is the \"true\" $\\theta$ that was used to generate the data.\n",
    "We can quantify the plausibility of different $\\theta$ values as a plausibility distribution $p(\\theta \\mid D=d)$.\n",
    "Note the similarity to the general description of bayesian inference above, where we look for $p(Z \\mid X=x)$.\n",
    "Just like in the case above we can derive $p(Z \\mid X=x)$ from $p(X,Z)$, we can now derive $p(\\theta \\mid D=d)$ from the joint distribution $p(D, \\theta)$.\n",
    "This joint distribution describes the plausibility of certain values of $\\theta$ and $D$ occurring together.\n",
    "\n",
    "---\n",
    "\n",
    "#### Definition\n",
    "\n",
    "A **bayesian model** is defined by a joint distribution $p(X,Z)$ over *unobserved (/hidden/latent) variables* $Z$ (e.g. model parameters) and *observed variables* $X$.\n",
    "\n",
    "**Bayesian inference** is about infering a plausibility distribution over the $Z$ given observed values for the $X$: $p(Z \\mid X=x)$.\n",
    "This distribution is called the *posterior distribution*, because it expresses our beliefs about the latent variables *after* knowing the values of the observed variable.\n",
    "\n",
    "---\n",
    "\n",
    "### Inference in Bayesian Models\n",
    "\n",
    "How can we compute $p(Z \\mid X=x)$?\n",
    "We know from the definition of conditional probability that\n",
    "$$ p(Z \\mid X=x) = \\dfrac{p(X,Z)}{p(X)}, $$\n",
    "so we can in principle derive the posterior distribution from the joint distribution.\n",
    "But where does the join distribution come from?\n",
    "\n",
    "Remember that, in many cases, we know (or assume to know) the likelihood, i.e. $p(X \\mid Z)$.\n",
    "We also know that the likelihood relates to the joint distribution by\n",
    "$$ p(X,Z) = p(X \\mid Z) p(Z), $$\n",
    "i.e. the joint distribution *factorizes* into the likelihood and the *prior distribution*,\n",
    "the marginal distribution of the hidden variables.\n",
    "\n",
    "What is the prior distribution?\n",
    "We know that it is the marginal distribution of $Z$, so we can interpret it as describing the plausibility of values for $Z$ given that $X$ could take *any* values (we even sum/integrate over all values for $X$).\n",
    "In other words, $p(Z)$ expresses our beliefs about $Z$ irrespective of $X$, i.e. *before* observing $X$.\n",
    "If we can somehow encode our prior beliefs about $Z$ as a distribution, we can combine it the likelihood to obtain a joint distribution.\n",
    "\n",
    "Combining the likelihood and the prior, we can make our formula for the posterior more precise:\n",
    "$$ p(Z \\mid X=x) = \\dfrac{p(X \\mid Z) p(Z)}{p(X=x)}, $$\n",
    "which is known as *Bayes' theorem*.\n",
    "The marginal probability of the data $p(X=x)$ is called *evidence* and serves to normalize the posterior when expressed in terms of prior and likelihood.\n",
    "While it is often impossible to compute the evidence (because it requires summing/integrating over all $Z$),\n",
    "it is constant for a given dataset ($X=x$), so we might want to write the posterior as\n",
    "$$ p(Z \\mid X=x) \\propto p(X \\mid Z) p(Z). $$\n",
    "\n",
    "#### Side Remark\n",
    "\n",
    "Bayesian inference is often introduced in terms of Bayes' theorem, i.e. distinguishing likelihood and prior,\n",
    "this is just special case.\n",
    "Generally, what is required is only a joint distribution.\n",
    "Since we often know the likelihood and can construct reasonable priors, they just provide a convenient factorization of the joint distribution.\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1. Show that Bayes' theorem follows directly from the possibility to factorize a joint distribution in different ways.\n",
    "2. You have a dataset of measurements $d = [3, 4.5, 5, 3.5]$.\n",
    "   You think that each data point is drawn independently from the same normal distribution with mean $\\mu$ and standard deviation $\\sigma = 1$,\n",
    "   i.e. $p(d) = \\prod_i p(d_i)$ where $d_i \\sim \\text{Normal}(\\mu, 1)$.\n",
    "   \n",
    "   What is the MLE for $\\mu$?\n",
    "   Derive a general formula as well as the MLE for above dataset.\n",
    "   Remember that the density function for the normal distribution is\n",
    "   $$ f(x) = \\dfrac{1}{\\sigma \\sqrt{\\tau}} e^{-\\dfrac{1}{2}\\left(\\dfrac{x-\\mu}{\\sigma}\\right)^2}. $$\n",
    "   Use the fact that maximizing $l(x)$ is the same as maximizing $\\log(l(x))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Probabilistic Programming 1: Writing Probabilistic Programs\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- Any Python program that samples values defines a probability distribution.\n",
    "- We can obtain a sample from the distribution by running the program and tracking each randomly chosen value\n",
    "  using a `sample` function, which gives the value a name.\n",
    "- Each \"random value\" can be understood as an assignment of a random variable.\n",
    "\n",
    "### Wrting Probabilistic Programs\n",
    "\n",
    "The idea of probabilistic programming is to define a joint probability distribution over several RVs as a generative process, i.e. a programm that draws samples from the joint distribution.\n",
    "The advantage of this is that it's a lot easier to do and a lot more readble than defining a closed formula for the whole joint distribution.\n",
    "The disadvantage is that it it's much more difficult to perform analytic math on a program.\n",
    "FWIW, a program is something that we can execute but not much more, we can't really look inside.\n",
    "But let's first look at the advantages, we'll deal with the disadvantages later.\n",
    "\n",
    "Let's assume that we want to implement our own little probabilistic programming framework.\n",
    "What do we need?\n",
    "First of all, we need a way to write programs that produce some kind of output.\n",
    "Fortunately, programming languages are already very good at letting you write pograms.\n",
    "For example, we can use Python to write a function that computes and returns some kind of result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_function():\n",
    "    x = 10\n",
    "    y = x*2\n",
    "    return y\n",
    "\n",
    "my_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above function is a program that computes a deterministic value, a probabilistic program is a mix of deterministic computation and random sampling.\n",
    "For example, the following function flips a coin to determine whether `y` should be `x*2` or `x*3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def my_random_function(p=0.5):\n",
    "    x = 10\n",
    "    heads = random.random() < p\n",
    "    y = x*2 if heads else x*3\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run this function several times, it will return different results, sometimes 20 and sometimes 30, depending on the result of the coin flip.\n",
    "We can look at the distribution over the result by running the programm several times and collecting the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "results = [my_random_function() for i in range(100)]\n",
    "\n",
    "counts = Counter(results)\n",
    "plt.bar(counts.keys(), counts.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we provide a different coin probability, the result distribution will be different as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = [my_random_function(0.1) for i in range(100)]\n",
    "\n",
    "counts = Counter(results)\n",
    "plt.bar(counts.keys(), counts.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while the function returns `y`, which is randomly distributed,\n",
    "the actual sampling happens when we flip the coin, so we might want to say that `heads` is the actual random variable an `y` is just a deterministic transformation of it.\n",
    "However, since the only thing we can do with `my_random_function` is to execute it, we can't directly look at the distribution of `heads`.\n",
    "Let's change this by \"recording\" the value of `heads` every time we call `my_random_function`.\n",
    "\n",
    "We write a little helper function called `sample` that use to record the values we sampled for random variables.\n",
    "Just like we can use `print` to print any value as a side effect, we can use `sample` to record a random variable as a side effect.\n",
    "We store the RV values in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_dict = dict()\n",
    "\n",
    "def clear_rvs():\n",
    "    global rv_dict\n",
    "    rv_dict = dict()\n",
    "\n",
    "def sample(name, value):\n",
    "    rv_dict[name] = value\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we adapt `my_random_function` to use `sample` to record the value of `heads`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "{'heads': False}\n"
     ]
    }
   ],
   "source": [
    "def my_random_function(p=0.5):\n",
    "    x = 10\n",
    "    heads = sample(\"heads\", random.random() < p)\n",
    "    y = x*2 if heads else x*3\n",
    "    return y\n",
    "\n",
    "clear_rvs()\n",
    "result = my_random_function()\n",
    "print(result)\n",
    "print(rv_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, after running `my_random_function` again, the value of `heads` is recorded in `rv_dict` and the return value of `my_random_function` corresponds to the recorded value of heads.\n",
    "\n",
    "What if we want to record several RVs? We can do that too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(False, True): 49,\n",
       "         (True, True): 11,\n",
       "         (False, False): 30,\n",
       "         (True, False): 10})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flip_two_coins(p1=0.5, p2=0.5):\n",
    "    coin1 = sample(\"coin1\", random.random() < p1)\n",
    "    coin2 = sample(\"coin2\", random.random() < p2)\n",
    "\n",
    "results = []\n",
    "for i in range(100):\n",
    "    clear_rvs()              # clear the recorded values\n",
    "    flip_two_coins(0.2, 0.6) # record new values \n",
    "    results.append((rv_dict[\"coin1\"], rv_dict[\"coin2\"]))\n",
    "    \n",
    "counts = Counter(results)\n",
    "counts\n",
    "#plt.bar(counts.keys(), counts.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that creating many samples from this distribution (as we do above) gives us an estimate of how the random variables are distributed.\n",
    "In this case, we get the approximate frequency of every possible variable assignment, which converges to the true probability of every variable assignment when the number of samples goes to infinity.\n",
    "Even in the continuous case we can get an estimate of the shape of the distribution by drawing many samples,\n",
    "but we would have to draw many more samples (depending on the complexity of the shape) and the number of possible configurations grows exponentially with the number of variables, so just sampling is not always a good solution.\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1. Let's go bayesian! Write a probabilistic program that flips a coin with probability $p$,\n",
    "   but first samples the probability $p$ from a uniform distribution $\\text{U}(0,1)$.\n",
    "   Note how this defines a joint distribution over the parameter $p$ and the result of the coin.\n",
    "1. Extend your program to flip not just 1 coin but 100, each with probability $p$.\n",
    "   Make sure that each flipped coin is a recorded as a random variable.\n",
    "   How would you write the probability distribution for the sequence of coin flips mathematically?\n",
    "1. Write a more complex program that simulates how a chord is constructed from a profile.\n",
    "   First, sample the type of the chord $t$ from a categorical distribution over chord types.\n",
    "   Then sample a number of notes $n$ from a poisson distribution (add 1 to the value to avoid 0s).\n",
    "   Finally sample $n$ notes from a multinomial distribution with the weights that correspond to $h$.\n",
    "   The parameters (weights for $h$, rate for $n$ and weights for the notes for each $h$) are given as arguments to the function.\n",
    "1. Extend the previous program to also sample the parameters from a prior distribution.\n",
    "   (Hint: use Dirichlet priors for the weight vectors and a Gamma prior for the rate of the Poisson distribution)\n",
    "1. Extend the previous program to sample a flexible number of chords (provided as an argument)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Probabilistic Programming 2: Evaluating Probabilities\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- We want to be able to evaluate the probability of a given sample from the joint distribution.\n",
    "- We do that by introducing two new things:\n",
    "  - Random values are drawn from `Distribution`s which can\n",
    "    - generate a new random value and\n",
    "    - evaluate the probability of a given value.\n",
    "  - The `sample` function gets an additional interpretation that\n",
    "    - returns the given value of the RV and\n",
    "    - tracks that values local probability under the given distribution.\n",
    "- The joint probability of the sample is the product of all local probabilities.\n",
    "  - The local probabilities are conditional distributions\n",
    "  - The probabilistic program defines a factorization of the joint distribution into local conditional distributions.\n",
    "- Any probabilistic program written using `sample` and `Distributions` can be used to\n",
    "  - draw a sample\n",
    "  - evaluate the probability of a given sample\n",
    "  - despite containing arbitrary Python code with unknown control flow.\n",
    "- Drawing and evaluating samples is sufficient for inference.\n",
    "\n",
    "So far, we have defined a probability distribution implicitly as a program that produces samples from the distribution.\n",
    "We have also seen that by sampling a lot from this distribution, we can estimate it's shape.\n",
    "While this is already useful, it's not quite enough to make serious inference, especially for more complex models.\n",
    "There is one more thing that we need: In addition to sampling from the distribution, we also need to be able to evaluate the probability of a given sample.\n",
    "With these two things, we will be able to do pretty much everything we need for performing bayesian inference.\n",
    "(If it's not clear to you why, don't worry, it will be explained later).\n",
    "\n",
    "In their current form, our probabilistic programs are a bit too implicit to be used for evaluating the probability of a sample.\n",
    "However, we have already introduced a bit of structure by requiring random variables to be registered with the `sample` function.\n",
    "It turns out that if we add a tiny little bit more structure, we can do probability evaluation too,\n",
    "without changing our programs too much.\n",
    "\n",
    "The first thing we need to do is to combine the ability to *sample* a value for a RV with the ability to evaluate its *local probability*.\n",
    "Consider how we sampled the value of our coin in the prevous part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'heads': False}\n"
     ]
    }
   ],
   "source": [
    "def flip_coin(p=0.5):\n",
    "    sample(\"heads\", random.random() < p)\n",
    "\n",
    "clear_rvs()\n",
    "flip_coin()\n",
    "print(rv_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the RV \"heads\" we know exactly from which distribution we sample it.\n",
    "In this case, it's taken from a Bernoulli distribution.\n",
    "Since we know how we sample \"heads\", we also know the probability of each possible value of \"heads\":\n",
    "for `True` it's $p$ and for `False` it's $1-p$.\n",
    "\n",
    "Let's make this connection a bit more formal.\n",
    "We have an object (a probability distribution) that can do two things: provide a sample from it and evaluate the probability of a given sample.\n",
    "In python, we can express this idea using a class with two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract base class for distributions\n",
    "class Distribution:\n",
    "    \"Return a sample from the distribution.\"\n",
    "    def sample():\n",
    "        pass\n",
    "    \n",
    "    \"Return the log probability of a given sample from the distribution.\"\n",
    "    def log_p(value):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the second method returns the log probability instead of the probability.\n",
    "The reason for this is that probabilities sometimes get very small and can become 0 due to representation errors.\n",
    "This doesn't happen so easily in log space,\n",
    "and it's trivial to obtain the log probability when the normal probability is known (and vice versa).\n",
    "\n",
    "We can now implement this class for the Bernoulli distribution that formalizes our coin flip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "p(True)  = 0.4\n",
      "p(False) = 0.6\n"
     ]
    }
   ],
   "source": [
    "class Bernoulli(Distribution):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "    \n",
    "    def sample(self):\n",
    "        return random.random() < self.p\n",
    "    \n",
    "    def log_p(self, heads):\n",
    "        return np.log(self.p if heads else 1-self.p)\n",
    "\n",
    "coin = Bernoulli(0.4)\n",
    "print(coin.sample())\n",
    "print(\"p(True)  =\", np.exp(coin.log_p(True)))\n",
    "print(\"p(False) =\", np.exp(coin.log_p(False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to express primitive distributions that can be used for both sampling and evaluating the log probability, we require our probabilistic programs to use them.\n",
    "Let's change the definition of `sample` to take a distribution object and `.sample()` from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(name, dist):\n",
    "    value = dist.sample()\n",
    "    rv_dict[name] = value\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we also have to change our program to make use of the `Bernoulli` distribution and the new way `sample` works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'heads': False}\n"
     ]
    }
   ],
   "source": [
    "def flip_coin(p):\n",
    "    sample(\"heads\", Bernoulli(p))\n",
    "\n",
    "clear_rvs()\n",
    "flip_coin(0.4)\n",
    "print(rv_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works like a charm.\n",
    "\n",
    "We still can't make use of the `log_p` method to evaluate the probability of a value for \"heads\".\n",
    "However, since we require using `sample` to track random variables, we already have access to the place where RVs and distributions meet.\n",
    "We could in principle write a different `sample` function that doesn't actually sample and record a fresh value for some RV, but instead looks up the value from a table that we provide beforehand and records it's log probability under the distribution that is provided!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_values = {}\n",
    "recorded_probs = {}\n",
    "\n",
    "def clear_recorded_probs():\n",
    "    global recorded_probs\n",
    "    recorded_probs = {}\n",
    "\n",
    "# our reimplementation of sample that evaluates probabilities\n",
    "def sample(name, dist):\n",
    "    value = known_values[name]\n",
    "    recorded_probs[name] = dist.log_p(value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set `known_values` to contain values for all RVs and run our model again, we should now be able to obtain the local probabilities for each RV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(heads = True)  = 0.4\n",
      "p(heads = False) = 0.6\n"
     ]
    }
   ],
   "source": [
    "# run the model for 'heads' = True\n",
    "known_values = {'heads': True}\n",
    "clear_recorded_probs()\n",
    "flip_coin(0.4)\n",
    "print(\"p(heads = True)  =\", np.exp(recorded_probs['heads']))\n",
    "\n",
    "# run the model for 'heads' = False\n",
    "known_values = {'heads': False}\n",
    "clear_recorded_probs()\n",
    "flip_coin(0.4)\n",
    "print(\"p(heads = False) =\", np.exp(recorded_probs['heads']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's still a bit inconvenient that we have to redefine `sample` depending on whether we want to sample or to evaluate.\n",
    "Ideally, we should be able to abstract over the interpretation of `sample`.\n",
    "For example, we can make it look it's functionality up in a variable that we can change to \"sample\" or \"evaluate\" or some other actual interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_function = None\n",
    "\n",
    "def sample(name, dist):\n",
    "    return sample_function(name, dist)\n",
    "\n",
    "# implementation of \"sample\" that samples and records a value (standard interpretation)\n",
    "def sample_sample(name, dist):\n",
    "    value = dist.sample()\n",
    "    rv_dict[name] = value\n",
    "    return value\n",
    "\n",
    "# implementation of \"sample\" that returns the already know value for a RV and records its local probability\n",
    "def sample_eval(name, dist):\n",
    "    value = known_values[name]\n",
    "    recorded_probs[name] = dist.log_p(value)\n",
    "    return value\n",
    "\n",
    "# we set sample_function to be sample_sample by default\n",
    "sample_function = sample_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can freely change the interpretation `sample` to draw samples or evaluate probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'heads': False}\n",
      "{'heads': -0.916290731874155}\n"
     ]
    }
   ],
   "source": [
    "# draw a sample\n",
    "clear_rvs()\n",
    "\n",
    "sample_function = sample_sample\n",
    "flip_coin(0.4)\n",
    "\n",
    "print(rv_dict)\n",
    "\n",
    "# evaluate a sample\n",
    "known_values = {'heads': True}\n",
    "clear_recorded_probs()\n",
    "\n",
    "sample_function = sample_eval\n",
    "flip_coin(0.4)\n",
    "\n",
    "print(recorded_probs) #print the log probs for all RVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a bit fragile (we could forget to set the correct sample function),\n",
    "let's put the sampling and evaluation procedures into their own functions.\n",
    "To each of these functions we can pass the model and its arguments and it will wrap a call to the model with the necessary setup and teardown.\n",
    "(A more elegant way would be to use `with` statements, which is what pyro does, but functions will do fine here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_model(model, *args, **kwargs):\n",
    "    \"\"\"Runs `model(*args, **kwargs)` in sampling mode.\n",
    "    \n",
    "    Returns the sampled random variables.\n",
    "    \"\"\"\n",
    "    clear_rvs()\n",
    "    global sample_function\n",
    "    sample_function = sample_sample\n",
    "    model(*args, **kwargs)\n",
    "    return rv_dict\n",
    "\n",
    "def eval_model(model, observed, *args, **kwargs):\n",
    "    \"\"\"Runs `model(*args, **kwargs)` in evaluation mode with `observed` as the variable assignment.\n",
    "    \n",
    "    Returns the local probabilities of the random variables.\n",
    "    \"\"\"\n",
    "    clear_recorded_probs()\n",
    "    global sample_function\n",
    "    sample_function = sample_eval\n",
    "    global known_values\n",
    "    known_values = observed\n",
    "    model(*args, **kwargs)\n",
    "    return recorded_probs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run the same experiment as above but this time it's a lot shorter and we can't make a mistake so easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'heads': True}\n",
      "{'heads': -0.916290731874155}\n"
     ]
    }
   ],
   "source": [
    "# sample\n",
    "rvs = run_model(flip_coin, 0.4)\n",
    "print(rvs)\n",
    "\n",
    "# evaluate\n",
    "probs = eval_model(flip_coin, {'heads': True}, 0.4)\n",
    "print(probs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the same for a more complex model.\n",
    "We flip two coins, but the probability of the second depends on the outcome of the first.\n",
    "Fortunately, `sample_eval` returns the observed value for the first coin, the when running the model with observed values, it will choose the correct probability parameters for the second coin, just as if the value of the first coin would have been sampled randomly!\n",
    "\n",
    "We can try this out by first drawing a sample and then evaluating the probability of that sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coin1': True, 'coin2': True}\n",
      "{'coin1': 0.5, 'coin2': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# our shiny two-coin model\n",
    "def flip_two_coins():\n",
    "    coin1 = sample('coin1', Bernoulli(0.5))\n",
    "    coin2 = sample('coin2', Bernoulli(0.8 if coin1 else 0.2))\n",
    "\n",
    "# draw a sample\n",
    "rvs = run_model(flip_two_coins)\n",
    "print(rvs)\n",
    "\n",
    "# evaluate the sample\n",
    "probs = eval_model(flip_two_coins, rvs) # take the sample from the previous run\n",
    "print({name: np.exp(prob) for (name,prob) in probs.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the probability of the second coin is evaluated correctly wrt. the first coin!\n",
    "That's because when the model is run again, it picks the parameters for the second Bernoulli distribution according to the observed value of the first coin.\n",
    "The second `sample` call therefore is given the Bernoulli distribution with the correct parameters and can evaluate the probability of the second coin's value correctly.\n",
    "\n",
    "Congratulations, you have now implemented a probabilistic programming language!\n",
    "We already have everything we need to sample from and evaluate samples under joint distributions that are defined as arbitrary Python programs.\n",
    "As long as the actual random decisions are made using `sample` and from a `Distribution`, the remainder of the model can be arbitrary Python code.\n",
    "In fact, `sample` and `Distribution`s are the only \"language\" constructs we needed to add to ordinary Python for writing models.\n",
    "All the inference stuff can now be done outside of the models in a generic, model-independent way.\n",
    "\n",
    "\"But wait,\" you might object \"we only computed the 'local probability' of each RV, not the joint probability of the whole assignment! And what is that 'local probability' even supposed to mean?\"\n",
    "It turns out that the local probablities are all we need to compute the joint probability of the assignment.\n",
    "All we have to do is to multiply them (or rather sum the log probabilities and take the exponential)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(coin1 = True , coin2 = True ) =  0.4\n"
     ]
    }
   ],
   "source": [
    "def joint_log_p(rv_log_probs):\n",
    "    return sum(rv_log_probs.values())\n",
    "\n",
    "prob = np.exp(joint_log_p(probs))\n",
    "print(\"p(coin1 =\", rvs['coin1'], \", coin2 =\", rvs['coin2'], \") = \", prob) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does this work?\n",
    "\n",
    "Remember that any joint probability distribution can be turned into a generative process by factorizing it into a sequence of conditional distributions.\n",
    "For example $p(A,B,C)$ can be factorized into $p(A) p(B \\mid A) p(C \\mid A, B)$,\n",
    "which corresponds to a process that first samples $A$ then, $B$ given $A$, and finally $C$ given $A$ and $B$.\n",
    "\n",
    "A probabilistic program is exactly such a factorization, corresponding to the order in which the RVs are sampled in the program.\n",
    "For example, the model `flip_two_coins` first samples `coin1` from $p(\\text{coin1})$ and then `coin2` from $p(\\text{coin2} \\mid \\text{coin1})$,\n",
    "and we know that $p(\\text{coin1}) p(\\text{coin2} \\mid \\text{coin1}) = p(\\text{coin1}, \\text{coin2})$.\n",
    "Now we can see that the \"local probability\" of a RV actually is the *conditional probability* of that RV given all the RVs that were sampled before!\n",
    "\n",
    "Note that a variable after another variable does not need to depend on that earlier variable.\n",
    "For example, we might sample $A$, $B$, and $C$ in that order, but maybe $B$ is sampled from a distribution that is fixed and independent of $A$, so the distribution factorizes into $p(A) p(B) p(C \\mid A,B)$.\n",
    "However, if $A$ and $B$ are independent, then it holds that $p(B \\mid A) = p(B)$,\n",
    "so $p(A) p(B) p(C \\mid A,B) = p(A) p(B \\mid A) p(C \\mid A,B)$!\n",
    "\n",
    "Finally, the order need not be defined beforehand. Consider, for example the following model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': True, 'B': False, 'C': False}\n"
     ]
    }
   ],
   "source": [
    "def weird_model():\n",
    "    a = sample(\"A\", Bernoulli(0.5))\n",
    "    if a:\n",
    "        b = sample(\"B\", Bernoulli(0.3))\n",
    "        c = sample(\"C\", Bernoulli(0.2 if b else 0.6))\n",
    "    else:\n",
    "        c = sample(\"C\", Bernoulli(0.4))\n",
    "        b = sample(\"B\", Bernoulli(0.1 if c else 0.2))\n",
    "\n",
    "rvs = run_model(weird_model)\n",
    "print(rvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, the factorization depends on $A$.\n",
    "If $A$ is `True`, then $B$ is sampled first and $C$ depends on $B$\n",
    "$$ p(A = \\text{True},B,C) = p(A = \\text{True}) p(B \\mid A = \\text{True}) p(C \\mid A = \\text{True}, B) $$\n",
    "\n",
    "If $A$ is `False`, it's the other way round.\n",
    "$$ p(A = \\text{False},B,C) = p(A = \\text{False}) p(C \\mid A = \\text{False}) p(B \\mid C, A = \\text{False}) $$\n",
    "\n",
    "Since we only want to evaluate complete assignments, it's sufficient to know that the \"local\" probabilities are *a* factorization of the joint distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "tbd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Inference\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- we want to know $p(Z \\mid X=x)$ \n",
    "  - can't be derived analytically\n",
    "  - we could sample, but it's expensive\n",
    "- approximation\n",
    "  - choose a variational familiy (\"guide\" in pyro) $q_\\phi(Z)$\n",
    "  - try to optimize $\\phi$ to minimize $\\text{KL}(q(Z) \\parallel p(Z \\mid X=x))$\n",
    "  - we can't compute the KL directly, but we can optimize it via the equivalent \"ELBO\".\n",
    "  - the mean-field family is often a good choice for the guide.\n",
    "\n",
    "In the previous sections we have learned how we can write probabilistic programs in a way that allows us to (a) sample from the joint distribution and (b) evaluate the probability of a sample (i.e. a full variable assingment).\n",
    "But how do we obtain the posterior distribution when conditioning on observations of some RVs, i.e. $p(Z \\mid X=x)$?\n",
    "\n",
    "### Sampling\n",
    "\n",
    "First let's note that while we can use our probabilistic program to sample from the joint distribution, we can't use it directly to sample from the posterior distribution.\n",
    "A naive attempt at doing so could be to add a new implementation of `sample`\n",
    "- samples unobserved RVs normally, and\n",
    "- returns the observed value for observed variables.\n",
    "\n",
    "However, this would not sample from the conditional distribution.\n",
    "To see why, consider the following program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_coins():\n",
    "    c1 = sample(\"c1\", Bernoulli(0.5))\n",
    "    c2 = sample(\"c2\", Bernoulli(0.9 if c1 else 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we observe that $c_2$ is heads, what do you think $c_1$ was?\n",
    "It's more likely that $c_1$ was heads (`True`) than tails (`False`) since heads for $c_1$ makes heads for $c_2$ much more likely.\n",
    "We can even quantify the distribution $p(c_1 \\mid c_2 = \\text{heads})$ exactly using Bayes' formula:\n",
    "$$\n",
    "  p(c_1 \\mid c_2 = \\text{heads}) = \\dfrac{p(c_2 = \\text{heads} \\mid c_1) p(c_1)}{p(c_2 = \\text{heads})}\\\\\n",
    "  p(c_1 = \\text{heads} \\mid c_2 = \\text{heads}) = \\dfrac{p(c_2 = \\text{heads} \\mid c_1 = \\text{heads}) p(c_1 = \\text{heads})}{p(c_2 = \\text{heads})} = \\dfrac{0.9 \\cdot 0.5}{0.5 \\cdot 0.9 + 0.5 \\cdot 0.1} = 0.9\\\\\n",
    "  p(c_1 = \\text{tails} \\mid c_2 = \\text{heads}) = \\dfrac{p(c_2 = \\text{heads} \\mid c_1 = \\text{tails}) p(c_1 = \\text{tails})}{p(c_2 = \\text{heads})} = \\dfrac{0.1 \\cdot 0.5}{0.5 \\cdot 0.9 + 0.5 \\cdot 0.1} = 0.1\\\\\n",
    "$$\n",
    "But imagine we would sample from `simple_coins` as described above, simply fixing \"c2\" to be heads.\n",
    "We would still sample $c_1$ from $\\text{Bernoulli}(0.5)$, although $c_1 \\mid (c_2 = \\text{heads}) \\sim \\text{Bernoulli}(0.9)$!\n",
    "Generally, there is a difference between \"intervention\" (simply fixing the values of the observed RVs without touching the others) and \"conditioning\" (changing the distribution of the unobserved variables to reflect the conditional distribution).\n",
    "\n",
    "However, there are other methods by which we can use a program like `simple_coins` to sample from the posterior distribution.\n",
    "The most simple and universal one is called *rejection sampling*.\n",
    "It draws normal samples from joint distribution, but since we are only interested in the cases where some \"condition\" holds (e.g. that $c_2 = \\text{heads}$), it \"rejects\" all samples where the condition is not true.\n",
    "This is as simple to implement as it sounds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({True: 45, False: 4})\n",
      "49\n"
     ]
    }
   ],
   "source": [
    "n = 100\n",
    "samples_c1 = []\n",
    "\n",
    "# draw n samples\n",
    "for _ in range(n):\n",
    "    # draw the sample\n",
    "    rvs = run_model(simple_coins)\n",
    "    # check if the condition is satisfied\n",
    "    if rvs[\"c2\"] == True:\n",
    "        # record the value of c1\n",
    "        samples_c1.append(rvs[\"c1\"])\n",
    "\n",
    "# count and plot the outcome\n",
    "counts_c1 = Counter(samples_c1)\n",
    "plt.bar([str(k) for k in counts_c1.keys()], counts_c1.values())\n",
    "plt.show()\n",
    "print(counts_c1)\n",
    "print(len(samples_c1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the samples of $c_1$ are distributed roughly according to the posterior distribution (with $p=0.9$ rather than $0.5$).\n",
    "But you probably have also noticed, that we have to throw away roughly half of our samples\n",
    "since the condition is only satisfied with probability 0.5!\n",
    "As you can imagine, if the probability of the condition becomes less likely, rejection sampling becomes rather inefficient.\n",
    "For example, our condition is normally the observation of a large dataset,\n",
    "and the probability of the condition is the probability to generate exactly that dataset from the model by chance!\n",
    "\n",
    "There are smarter ways of sampling from the conditional distribution such as [Metropolis-Hastings](https://en.wikipedia.org/wiki/Metropolis%E2%80%93Hastings_algorithm) sampling,\n",
    "which requires the ability to evaluate the probability of a sample (which we know how to do).\n",
    "However, sampling methods have some general disadvantages,\n",
    "for example that they don't provide us with a closed formula for (an approximation of) the posterior,\n",
    "unless combined with other methods such as kernel-density estimation.\n",
    "Therefore, we will now focus on a different method called \"variational inference\".\n",
    "It is, however, important to note that our way of defining probabilistic programs using `sample` is in principle sufficient to use advanced sampling algorithms on arbitrary models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Inference\n",
    "\n",
    "(This section is basically a short summary of [Blei et al., 2017](https://www.tandfonline.com/doi/full/10.1080/01621459.2017.1285773))\n",
    "\n",
    "Remember that our goal is to determine the posterior distribution $p(Z \\mid X=x)$.\n",
    "While the posterior distribution can in principle be obtained via Bayes' formula,\n",
    "it requires to compute the normalization factor $p(X=x)$ (the \"evidence\"),\n",
    "which doesn't always have a closed form.\n",
    "\n",
    "Variational inference is based on the idea that instead of computing the posterior $p(Z \\mid X=x)$ directly, we *approximate* it by a different distribution $q(Z)$.\n",
    "We can do this by choosing a *familiy* of distributions $q_\\phi(Z)$,\n",
    "for which we try to find the optimal parameters $\\phi$ so that $q_\\phi(Z)$ is as similar to $p(Z \\mid X=x)$ as possible.\n",
    "What does \"similar\" mean?\n",
    "Since we talk about the similarity of distributions, a useful measure is the Kullback-Leibler divergence,\n",
    "which measures the \"expected\" difference of one distribution from another distribution:\n",
    "\n",
    "$$\n",
    "    \\text{KL}(p_1 \\parallel p_2)\n",
    "    = \\sum_x p_1(X=x) \\log \\left( \\dfrac{p_1(X=x)}{p_2(X=x)} \\right)\n",
    "    = \\mathbb{E}_{p_1}[\\log p_1(X)] - \\mathbb{E}_{p_1}[\\log p_2(X)].\n",
    "$$\n",
    "\n",
    "Note that the KL divergence is not symmetric, since the expected value is taken with respect to $p_1$!\n",
    "\n",
    "If we apply the KL divergence to the variational distribution $q_\\phi(Z)$ and the posterior $p(Z \\mid X=x)$,\n",
    "we get\n",
    "\n",
    "$$\n",
    "    \\text{KL}(q_\\phi(Z) \\parallel p(Z \\mid X=x)) = \\mathbb{E}_{q_\\phi}[\\log q_\\phi(Z)] - \\mathbb{E}_{q_\\phi}[\\log p(Z \\mid X=x)]\n",
    "$$\n",
    "\n",
    "Finding the best $\\phi$ now amounts to minimizing the KL divergence as stated above, so we have an objective function for optimizing $\\phi$!\n",
    "The problem is that we can't evaluate this objective function since we can't evaluate $p(Z \\mid X=x)$;\n",
    "our probabilistic programs only allow us to evaluate probabilities wrt. the joint distribution $p(X,Z)$.\n",
    "However, remember that we can rewrite $p(Z \\mid X=x)$ as\n",
    "$\\dfrac{p(X=x, Z)}{p(X=x)}.$\n",
    "The numerator is given by the joint distribution, so we can evaluate it.\n",
    "The denominator is the \"evidence\"; we can't evaluate it (which creates the whole problem in the first place), but we know that, for a given $x$, $p(X=x)$ is constant, and constants are irrelevant to an optimiziation objective.\n",
    "\n",
    "Let's apply this insight to our objective function:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{KL}(q_\\phi(Z) \\parallel p(Z \\mid X=x))\n",
    "    &= \\mathbb{E}_{q_\\phi}[\\log q_\\phi(Z)] - \\mathbb{E}_{q_\\phi}[\\log p(X=x, Z) - \\log p(X=x)]\\\\\n",
    "    &= \\mathbb{E}_{q_\\phi}[\\log q_\\phi(Z)] - \\mathbb{E}_{q_\\phi}[\\log p(X=x,Z)] + \\log p(X=x).\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Since the term $\\log p(X=x)$ is constant, we can ignore it for optimization and define the \"ELBO\" as:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\text{ELBO}(q_\\phi) &= \\log p(X=x) - \\text{KL}(q(Z) \\parallel p(Z \\mid X=x))\\\\\n",
    "    &= \\mathbb{E}_{q_\\phi}[\\log p(X=x,Z)] - \\mathbb{E}_{q_\\phi}[\\log q_\\phi(Z)].\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note the change of sign: Instead of minimizing the KL divergence, we maximize the the ELBO, but the optimal parameters $\\phi$ are the same for both.\n",
    "ELBO stands for \"evidence lower bound\", because it is a lower bound to the log evidence $\\log p(X=x)$,\n",
    "as we can see from the equation above.\n",
    "If $q_\\phi$ and the posterior distribution are the same,\n",
    "then the KL divergence is 0 and  $\\text{ELBO}(q_\\phi) = \\log p(X=x)$.\n",
    "If $q_\\phi$ is an imperfect approximation of the posterior,\n",
    "then the KL divergence is positive and $\\text{ELBO}(q_\\phi) < \\log p(X=x)$,\n",
    "so generally\n",
    "\n",
    "$$\n",
    "    \\text{ELBO}(q_\\phi) \\leq \\log p(X=x).\n",
    "$$\n",
    "\n",
    "Note that the ELBO requires us to compute expected values wrt. $q_\\phi$.\n",
    "While this may be intractable (because we have to sum/integrate over $Z$),\n",
    "we can estimate these expectation by drawing samples of $Z$ from $q_\\phi$.\n",
    "Fortunately, we already know how express distributions that we can sample from,\n",
    "namely as probabilistic programs.\n",
    "We don't know yet how to optimize them, but that's the topic of the next section.\n",
    "\n",
    "The question remains how we choose the family $q_\\phi$.\n",
    "It needs to be flexible enough to allow for good approximations of the posterior while being simple enough to be optimized efficiently.\n",
    "A common strategy is to use a *mean-field* family, which treats each latent variable as independent.\n",
    "This family is easy to optimize and can capture any marginal distribution of individual variables,\n",
    "but it can't express correlations between latent variables.\n",
    "However, since we are often interested in the marginal posteriors of individual latent variables (e.g. to asses the certainty about an inferred parameter of our model), the mean-field family is very useful.\n",
    "\n",
    "In general, we can write the mean-field family as\n",
    "\n",
    "$$\n",
    "    q(Z) = \\prod_i q_i(Z_i),\n",
    "$$\n",
    "\n",
    "where each $q_i$ comes with its own parameters.\n",
    "We then choose a plausible $q_i$ for each latent variable $Z_i$.\n",
    "If you don't know anything else, the family of the variables conjugate prior is a good choice,\n",
    "but in principle any distribution with the same support as $Z_i$ works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Examples\n",
    "\n",
    "Let's look at some concrete examples now.\n",
    "First, we go back to our problem above, where we tried to infer the outcome of the first coin flip from the second.\n",
    "As a reminder, here is our model again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_coins():\n",
    "    c1 = sample(\"c1\", Bernoulli(0.5))\n",
    "    c2 = sample(\"c2\", Bernoulli(0.9 if c1 else 0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we choose our variational family $q_\\phi$?\n",
    "First of all, we know that it only concerns one variable, namely $c_1$, which is a binary variable.\n",
    "We also know that every possible distribution over a single binary varible is a Bernoulli distribution.\n",
    "Therefore, we know that the posterior $p(c_1 \\mid c_2)$ must also be a Bernoulli distribution,\n",
    "so the Bernoulli family is our best choice for $q_\\phi$, with $\\phi$ being the Bernoulli parameter.\n",
    "\n",
    "Since $q_\\phi$ is a family of distributions, we can now express it as a probabilistic program that takes some parameters ($\\phi$).\n",
    "Everytime we call this program with certain parameters, it describes a distribution, a member of the family $q_\\phi$.\n",
    "In pyro, the variational family is called *guide*, so we are going to follow this convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_coins_guide(phi):\n",
    "    sample(\"c1\", Bernoulli(phi))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how we `sample` $c_1$ just as we do in `simple_coins`.\n",
    "That's because we want to express that the RVs of the guide are the same as the unobserved variables of the model.\n",
    "The goal is now to find a `phi` that optimizes then ELBO between `simple_coins_guide` and the posterior of `simple_coins`.\n",
    "How we can do that will be the topic of the next section.\n",
    "\n",
    "Now let's look at another, slightly more complex example.\n",
    "Imagine that instead of assuming a fixed probability $\\theta$ for the first coin,\n",
    "we would like to infer the probability from observations.\n",
    "As we have learned in the first sections, this means including the probability into our model as a random varibale and define a joint distribution over $\\theta$, $c_1$, and $c_2$.\n",
    "The model `simple_coins` already tells us how to sample $c_1, c_2 \\mid \\theta$, so all we need is a prior for $\\theta$.\n",
    "We are going to go with the [Beta distribution](https://en.wikipedia.org/wiki/Beta_distribution) for $\\theta$, which is conjugate prior to the Bernoulli distribution in which $\\theta$ is used.\n",
    "Instead of defining this distribution ourselves (which we could do!), we are going to take it from `pyro.distributions`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'theta': tensor(0.4052), 'c1': tensor(False), 'c2': False}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyro.distributions import Beta\n",
    "\n",
    "def complicated_coins():\n",
    "    theta = sample(\"theta\", Beta(1,1)) # this is just a uniform distribution between 0 and 1\n",
    "    c1 = sample(\"c1\", Bernoulli(theta))\n",
    "    c2 = sample(\"c2\", Bernoulli(0.9 if c1 else 0.1))\n",
    "\n",
    "run_model(complicated_coins)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Huh! For some reason, using the Beta distribution from pyro makes our random values become some kind of `tensor` thing (at least some of them).\n",
    "We will figure out why in the next section, but for now we can ignore it.\n",
    "\n",
    "Now it's very difficult to obtain a reliable estimate of $\\theta$ from a single observation of $c_2$.\n",
    "In practice, we would have a whole dataset of observations that we can use to infer a parameter that underlies all of them.\n",
    "In this case, for example, we could flip $c_1$ and $c_2$ several times, assuming that the fairness of $c_1$ stays the same.\n",
    "Let's include this idea into our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'theta': tensor(0.6939),\n",
       " 'c1_0': tensor(True),\n",
       " 'c2_0': True,\n",
       " 'c1_1': tensor(True),\n",
       " 'c2_1': True,\n",
       " 'c1_2': tensor(True),\n",
       " 'c2_2': True}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def complicated_coins(n):\n",
    "    # sample theta once\n",
    "    theta = sample(\"theta\", Beta(1,1))\n",
    "    for i in range(n):\n",
    "        c1 = sample(\"c1_{}\".format(i), Bernoulli(theta))\n",
    "        c2 = sample(\"c2_{}\".format(i), Bernoulli(0.9 if c1 else 0.1))\n",
    "\n",
    "run_model(complicated_coins, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this version of `complicated_coins` we can provide an additional parameter `n` that tells us how many coin pairs we flip.\n",
    "Since we want to be able to distinguish the outcomes of each pair, we add an index to `c1` and `c2` indicating the trial.\n",
    "\n",
    "Now to the guide.\n",
    "Since we only want to observe the $c_2$s, we need to provide a guide for both $\\theta$ and the $c_1$ of each trial.\n",
    "For the $c_1$s we already know that we want a Bernoulli distribution,\n",
    "and for $\\theta$ we again choose a beta distribution, which has two parameters $\\alpha$ and $\\beta$.\n",
    "According to the mean-field strategy,\n",
    "we assume the $c_1$s and $\\theta$ to be independent in the posterior, so the guide is\n",
    "\n",
    "$$\n",
    "    q_\\phi(\\vec{c_1}, \\theta) = p(\\theta ;  \\alpha, \\beta) \\prod_i p(c_{1i} ; \\varphi_i),\n",
    "$$\n",
    "\n",
    "and $\\phi = (\\alpha, \\beta, \\vec{\\varphi})$.\n",
    "\n",
    "This is easy enough to write as a program:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complicated_coins_guide(n, alpha, beta, c1probs):\n",
    "    sample(\"theta\", Beta(alpha, beta))\n",
    "    for i in range(n):\n",
    "        sample(\"c1_{}\".format(i), Bernoulli(c1probs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing that is not so nice is that the number of samples and the parameters of the family both appear as function parameters.\n",
    "While `n` is just used to make the model adaptable to the size of the dataset,\n",
    "`alpha`, `beta`, and `c1probs` are parameters that we want to optimize.\n",
    "To make this explicit (and to allow automatic optimization, which will be discussed later),\n",
    "pyro provides another function called `param`, which takes a parameter name and a starting value.\n",
    "It registers the parameter in a parameter store similar to how `sample` registers random variables.\n",
    "We can rewrite our guide to a form that matches the one used in pyro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def param(name, initial_value):\n",
    "    return initial_value # does nothing interesting for now\n",
    "\n",
    "def complicated_coins_guide(n):\n",
    "    # params and distribution for theta\n",
    "    alpha = param(\"alpha\", 1.)\n",
    "    beta  = param(\"beta\",  1.)\n",
    "    sample(\"theta\", Beta(alpha, beta))\n",
    "    # params and distribution for c1\n",
    "    c1probs = param(\"c1probs\", [.5 for _ in n])\n",
    "    for i in range(n):\n",
    "        sample(\"c1_{}\".format(i), Bernoulli(c1probs[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how `complicated_coins_guide` now has the same signature as `complicated_coins`.\n",
    "This makes it easy to ensure that we always generate the same model structure (e.g. the same number of coin flips) in the model and the guide, by defining their arguments once and passing these arguments to both functions.\n",
    "\n",
    "The only thing that is missing now is how `param` works and how we can use it to optimize the parameters of the guide.\n",
    "This will be the topic of the last two sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Exercises\n",
    "\n",
    "tbd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimization via PyTorch [WIP]\n",
    "\n",
    "- differentiation of arbitrary functions\n",
    "  - possible if composed out of simpler functions\n",
    "  - chain rule\n",
    "- backpropagation\n",
    "  - smart application of chain rule\n",
    "  - computes gradient at a given point\n",
    "  - forward pass: compute function, track intermediate varibales\n",
    "  - backward pass: compute gradients recursively using forward data\n",
    "- PyTorch\n",
    "  - \"tensors\" track values that come from differentiable computations\n",
    "  - computation similar to vectorized NumPy code\n",
    "  - implements backpropagation on tensors\n",
    "- optimize using gradient descent\n",
    "  - start from some parameter assignment, follow gradient to local minimum\n",
    "  - ready-made optimizers in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem that was left over from the previous section was how to find optimal parameters $\\phi$\n",
    "to maximize the ELBO of our variational distribution (or \"guide\") $q_\\phi$.\n",
    "A very general framework for optimizing some parameters ($\\phi$)\n",
    "wrt. some objective function ($\\text{ELBO}(q_\\phi)$) is *gradient descent*.\n",
    "The idea is to follow the *gradient* of the parameters,\n",
    "i.e. to repeatedly evaluate the derivative of the objective function at the current parameters\n",
    "and update the parameters in descending direction until a local minimum is reached.\n",
    "While gradient methods canonically *minimize* the function, we try to *maximize* the ELBO,\n",
    "but that's not a problem because maximizing the ELBO is the same as minimizing the negative ELBO.\n",
    "\n",
    "There are two caveats to using gradient descent.\n",
    "First, following the gradient only leads to a local minimum,\n",
    "we have no guarantee that we have reached a global minimum or even a \"good\" local minimum.\n",
    "However, finding the global minimum (or some approximation with a guaranteed quality) is generally intractable for the kind of problem that we are trying to solve,\n",
    "so a local minimum is basically the best we can hope for.\n",
    "Second, our objective function needs to be (locally) differentiable.\n",
    "\n",
    "A brief look at the ELBO tells us that we might have a bit of a problem here.\n",
    "\n",
    "$$\n",
    "    \\text{ELBO}(q_\\phi) = \\mathbb{E}_{q_\\phi} [p(Z \\mid X=x)] - \\mathbb{E}_{q_\\phi}[q(Z)]\n",
    "$$\n",
    "\n",
    "In order to optimize $\\phi$, we have to know the derivative of $\\text{ELBO}(q_\\phi)$ wrt. $\\phi$,\n",
    "but this depends on expected values over $q_\\phi$, and $q_\\phi$ is only given as a program, not as a formula that we could analyze.\n",
    "\n",
    "### Automatic Differentiation\n",
    "\n",
    "Pyro solves this program using a technique that's called *automatic differentiation*.\n",
    "In fact, it doesn't do that on its own but instead relies on an existing library, [PyTorch](https://pytorch.org/).\n",
    "The idea behind automatic differentiation is very similar to the idea behind probabilistic programming:\n",
    "We allow the programmer to define more or less arbitrary programs while imposing just enough restrictions on these programs to be able to do what we want (i.e. differentiation and probabilistic inference, respectively).\n",
    "\n",
    "So how does automatic differentiation work?\n",
    "It's most important ingredient is called the *chain rule*, which describes how derivation behaves under function composition, i.e. when two differentiable functions are combined by feeding the result of the first function into the second:\n",
    "\n",
    "$$\n",
    "    h = g \\circ f \\Longleftrightarrow h(x) = g(f(x)).\n",
    "$$\n",
    "\n",
    "Function composition $g \\circ f$ (read: \"$g$ after $f$\") is the most simple and universal form in which functions can be constructed out of simpler functions.\n",
    "The chain rule tells us how to obtain the derivative of the composed function from the derivatives of the simpler functions:\n",
    "\n",
    "$$\n",
    "    h' = (g \\circ f)' = (g' \\circ f) \\cdot f'\\\\\n",
    "    \\Longleftrightarrow\\\\\n",
    "    h'(x) = g'(f(x)) \\cdot f'(x)\n",
    "$$\n",
    "\n",
    "The chain rule allows us to compute the derivative of any function as long as it can be expressed as a composition of simpler functions, for which we know the derivative.\n",
    "Let's look at an example\n",
    "\n",
    "$$\n",
    "    h(x) = (x + 1)^2\n",
    "$$\n",
    "\n",
    "We can decompose $h$ into two functions $f$ and $g$ such that $h = g \\circ f$ by setting\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  f(x) &= x + 1\\\\\n",
    "  g(y) &= y^2\\\\\n",
    "  g(f(x)) &= f(x)^2 = (x+1)^2.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "The chain rule tells us how to take the derivative of $h$ using the derivatives of $f$ and $g$:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  f'(x) &= 1\\\\\n",
    "  g'(y) &= 2y\\\\\n",
    "  h'(x) &= g'(f(x)) \\cdot f'(x) = 2(x+1) \\cdot 1.\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "We can evaluate this derivative at a certain point to get the gradient of $h$ at that point, for example:\n",
    "\n",
    "$$\n",
    "    h'(5) = 2(5+1) \\cdot 1 = 12.\n",
    "$$\n",
    "\n",
    "Torch lets us do all of this automatically if we write functions that are composed of simple functions that have a known derivative.\n",
    "Let's write the above function $h$ as a python function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def h(x):\n",
    "    return (x+1)**2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how do we get the derivative of this function?\n",
    "It turns out we can't really get that, at least not in a symbolic form,\n",
    "but we get something that is as good for our purpose:\n",
    "Whenever we feed $h$ with a concrete value, we can obtain the gradient at that value.\n",
    "Here is how:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(12.)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(5., requires_grad=True)\n",
    "z = h(x)\n",
    "z.backward()\n",
    "x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what is going on here?\n",
    "First, let's note that we don't feed $h$ with a number,\n",
    "but we first wrap the number into a `tensor`.\n",
    "Tensors are what allows PyTorch to track the functions that are called to compute the final result.\n",
    "They are very similar to NumPy arrays in that they overload all the basic arithmetic functions,\n",
    "which allows the programmer to use normal python operators but provides different implementation to them.\n",
    "While NumPy provides efficient implementations for numeric arrays,\n",
    "PyTorch adds the tracking that is required for automatic differentiation.\n",
    "\n",
    "(PyTorch tensors can be multidimensional arrays, as the name indicates, in the example above we just used a zero-dimensional array, i.e. a scalar value.\n",
    "You can even convert NumPy arrays and PyTorch tensors into each other.)\n",
    "\n",
    "So what kind of additional information is needed for differentiation?\n",
    "Consider the order in which the operations are applied to the input to generate the output:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  x &&&= 5\\\\\n",
    "  y &= x+1 &&= 6\\\\\n",
    "  z &= y^2 &&= 36. \\\\\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "This computation is called the *forward pass*.\n",
    "Each of these variables needs to know three things:\n",
    "- its value (e.g. $36$ for $z$)\n",
    "- its inputs (e.g. $y$)\n",
    "- the function that computes its value from the inputs (e.g. $g(y) = x^2$)\n",
    "\n",
    "This information is also what PyTorch tensors track, but they do so in a slightly indirect way that's not so easy to inspect.\n",
    "However, we can still see that $z$ is the result of a power function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<PowBackward0 at 0x7f4d6fe953a0>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z._grad_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `backward` now computes the gradient of the input values (i.e. $x$) wrt. the output $z = h(x)$.\n",
    "It does so by starting at $z$ and recursively computing the gradient of each tensor\n",
    "using the information collected in the forward pass.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\nabla_z z &= 1\\\\\n",
    "  \\nabla_y z &= g'(y) &&= 2 \\cdot 6 &&= 12\\\\\n",
    "  \\nabla_x z &= g'(f(x)) \\cdot f'(x) &&= (2 \\cdot (5+1)) \\cdot 1 &&= 12\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Note how computing the gradients uses information from the forward pass:\n",
    "$\\Delta y$ depends on $y$ and $\\Delta x$ depends on both $x$ and $f(x) = y$!\n",
    "However, we also reuse information from the previous step of the backward pass.\n",
    "That's easy to see in the last step, where we use $g'(f(x)) = g(y) = \\Delta y$.\n",
    "In the second step it's not so obvious,\n",
    "because the gradient of $z$ wrt. $z$ is the gradient of the identity function, which is 1 everywhere.\n",
    "However, we can rewrite the above calculation to reveal the pattern at each point:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "  \\nabla_z z &= id'(z) &&&&= 1\\\\\n",
    "  \\nabla_y z &= id'(g(y)) \\cdot g'(y) &&= \\nabla_z z \\cdot g'(y) &&= 12\\\\\n",
    "  \\nabla_x z &= id'(g(f(x)) \\cdot g'(f(x)) \\cdot f'(x) &&= \\nabla_y z \\cdot f'(x) &&= 12\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "From the second column we can see how the gradient results from an application of the chain rule.\n",
    "The third column, however, reveals how each gradient can be easily computed from the previous gradient and the local information in the forward pass.\n",
    "This algorithm is known as *backpropagation*, and it's exactly what the `backward` method does:\n",
    "Compute all the local gradients recursively down to the input tensors.\n",
    "\n",
    "They key for this algorithm to work is that the derivatives of the individual component functions (here $f$ and $g$) are known.\n",
    "PyTorch solves this using a `Function` class which has a `forward` and a `backward` method.\n",
    "The `forward` method returns the result of the function when provided with an input.\n",
    "The `backward` method takes the gradient of the output and the results of the forward pass and returns the gradient of the input.\n",
    "Notice the similarity to the `Distribution` class with it's `sample` and `log_p` methods, where one is used for running the probabilistic program (\"forward\") and the other is used for inference over a program using existing values (kind of analogous to \"backward\").\n",
    "The main difference is that the differentiable tensors need to be connected explicitly (by using one as an input of the other), while the random variables in a probabilistic program can more indirectly depend on each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimization using Gradient Descent\n",
    "\n",
    "Now that we know the gradient for a function's parameter at some point, how can we use it to optimize the function?\n",
    "We use an algorithm that is called *gradient descent* and does exactly that:\n",
    "It follows the gradient until it finds a local minimum.\n",
    "How does it do that?\n",
    "\n",
    "First of all, the gradient tells us the direction towards a minimum.\n",
    "If the gradient is positive, the minimum is to the left; if it's negative, the minimum is to the right.\n",
    "So we know the direction in which we have to go, but not how far.\n",
    "However, we know that the gradient is exactly $0$ at the minimum but larger (absolutely) between a minimum and the next maximum or saddle point.\n",
    "Therefore, the magnitude of the gradient can be taken as a rough estimate of how far away we are from the minimum.\n",
    "The larger the magnitude, the bigger the step that we want to take.\n",
    "However, since we don't know reliably how far the minimum is actually away and we don't want to overshoot it (potentially ending up farther away from it then before), we scale the gradient by a small factor, called the *learning rate* $\\lambda$.\n",
    "\n",
    "In total, we end up with the following formula for updating our parameter $x$:\n",
    "$$\n",
    "    x' = x - \\lambda \\cdot \\nabla_x h(x).\n",
    "$$\n",
    "\n",
    "This parameter update is repeated either for a fixed number of iterations or until we get a very small gradient, indicating that we are close to a minimum.\n",
    "\n",
    "Let's see how this algorithm can be applied to our function $h$.\n",
    "Note that we add a bit of code so that we can inspect what happens during optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal x    = -0.9999998807907104\n",
      "optimal h(x) = 1.4210854715202004e-14\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## gradient descent\n",
    "n = 100  # number of iterations\n",
    "zs = []  # for tracking the value of z=h(x)\n",
    "xs = []  # for tracking the value of x\n",
    "lr = 0.1 # learning rate\n",
    "\n",
    "# initial value of x\n",
    "x = torch.tensor(5., requires_grad=True)\n",
    "\n",
    "for _ in range(n):\n",
    "    z = h(x)                   # forward\n",
    "    z.backward()               # backward pass\n",
    "    zs.append(z.item())        # record z value\n",
    "    xs.append(x.item())        # record x value\n",
    "    x.data -= lr * x.grad.data # parameter update\n",
    "    x.grad.zero_() # reset the gradient, this is required because PyTorch adds to the existing gradient\n",
    "\n",
    "# output\n",
    "print(\"optimal x    =\", x.item())\n",
    "print(\"optimal h(x) =\", h(x.item()))\n",
    "\n",
    "# plots the histories of z and x\n",
    "\n",
    "plt.plot(zs)\n",
    "plt.title(\"value of z over time\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"z\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(xs)\n",
    "plt.title(\"value of x over time\")\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"x\")\n",
    "plt.show()\n",
    "\n",
    "pltxs = np.linspace(-6.,6.,100)\n",
    "pltzs = h(pltxs)\n",
    "plt.plot(pltxs, pltzs)\n",
    "plt.scatter(xs,zs)\n",
    "#plt.annotate(xy=())\n",
    "for i in range(n-1):\n",
    "    plt.annotate(\"\", xy=(xs[i+1], zs[i+1]), xytext=(xs[i], zs[i]),\n",
    "                 arrowprops=dict(arrowstyle=\"->\"))\n",
    "plt.title(\"exploration of h\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"z\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see above, the value of $x$ quickly approaches the optimum of $-1$.\n",
    "However, we also picked a particularly simple problem.\n",
    "In more complex situations, vanilla gradient descent might be too slow,\n",
    "especially if we have to pick a smaller learning rate (0.1 is rather large).\n",
    "Fortunately, we can substitute our update rule for [smarter methods such as Adam or RMSProp](https://ruder.io/optimizing-gradient-descent/index.html).\n",
    "All of those still just update the parameters using the gradient,\n",
    "but they might use some form of memory over the past gradients to change the update in clever ways.\n",
    "\n",
    "You don't even have to implement any of these yourself, as PyTorch provides them it its [torch.optim module](https://pytorch.org/docs/stable/optim.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal x    = -0.9999998807907104\n",
      "optimal h(x) = 1.4210854715202004e-14\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# example using optimizers\n",
    "import torch.optim\n",
    "\n",
    "x = torch.tensor(5., requires_grad=True)\n",
    "optimizer = torch.optim.SGD([x], lr=0.1) # stochastic gradient descent, equivalent to GD in our case.\n",
    "\n",
    "n = 100\n",
    "xs = []\n",
    "zs = []\n",
    "for _ in range(n):\n",
    "    optimizer.zero_grad() # automatically clears all gradients\n",
    "    z = h(x)\n",
    "    z.backward()\n",
    "    xs.append(x.item())\n",
    "    zs.append(z.item())\n",
    "    optimizer.step() # performs the parameter updates\n",
    "\n",
    "print(\"optimal x    =\", x.item())\n",
    "print(\"optimal h(x) =\", z.item())\n",
    "\n",
    "plt.plot(xs)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(zs)\n",
    "plt.show()\n",
    "\n",
    "plt.plot(pltxs, pltzs)\n",
    "plt.scatter(xs, zs)\n",
    "for i in range(n-1):\n",
    "    plt.annotate(\"\", xy=(xs[i+1], zs[i+1]), xytext=(xs[i], zs[i]),\n",
    "                 arrowprops=dict(arrowstyle=\"->\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how does PyTorch fit into our own little pyro implementation?\n",
    "Remember that what we try to do is to find the optimal parameters of our guide distribution $q_\\phi$ with respect to the ELBO loss function.\n",
    "We express both the model and the quide as probabilistic programs that can be used to compute the probability of a sample under the model or guide distribution, respectively.\n",
    "The full path from the ELBO loss to the parameters of the guide will be detailed in the next section,\n",
    "but it's important to see how PyTorch's automatic differentiation and pyro's distributions interact.\n",
    "\n",
    "It turns out that they already work together perfectly!\n",
    "Due to operator overloading, all we have to do is use tensors instead of raw numbers is distribution parameters.\n",
    "This even works for our own distribution implementations as long as they use operations that are supported by PyTorch, which means that we might have to replace some NumPy functions with the corresponding PyTorch functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log prob      = -0.9162907004356384\n",
      "prob          = 0.4000000059604645\n",
      "gradient of p = 2.5\n"
     ]
    }
   ],
   "source": [
    "class Bernoulli(Distribution):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "    \n",
    "    def sample(self):\n",
    "        return random.random() < self.p\n",
    "    \n",
    "    def log_p(self, heads):\n",
    "        return torch.log(self.p if heads else 1-self.p)\n",
    "\n",
    "p = torch.tensor(0.4, requires_grad=True)\n",
    "dist = Bernoulli(p)\n",
    "logprob = dist.log_p(True)\n",
    "\n",
    "print(\"log prob      =\", logprob.item())\n",
    "print(\"prob          =\", torch.exp(logprob).item())\n",
    "\n",
    "logprob.backward()\n",
    "print(\"gradient of p =\", p.grad.item())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we could, for example, go on and try to optimize $p$ wrt. the (log) probability of $X=\\text{heads}$.\n",
    "We would have to be careful though to keep $p$ between 0 and 1!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizing the ELBO\n",
    "\n",
    "- ELBO can be MC estimated\n",
    "- can't take gradient of the naive ELBO estimate\n",
    "  - dependency to parameters is \"realized\" with randomness\n",
    "  - better: MC estimate of the gradient!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lighting the Whole Thing Up\n",
    "\n",
    "- torch + \"sample\" + distributions = pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stochastic_elbo(model, guide, observations, args, kwargs):\n",
    "    # obtain a sample from the guide\n",
    "    run_model(guide, *args, **kwargs)\n",
    "    \n",
    "    # evaluate the sample under the model\n",
    "    observed = {**observations, **sample}\n",
    "    model_probs = eval_model(model, observed, *args, **kwargs)\n",
    "    log_p = joint_log_p(model_probs)\n",
    "    \n",
    "    # evaluate the sample under the guide\n",
    "    guide_probs = eval_model(guide, sample, *args, **kwargs)\n",
    "    log_q = joint_log_p\n",
    "    \n",
    "    # compute the elbo\n",
    "    elbo = log_p - log_q\n",
    "    \n",
    "    return -elbo # ELBO must be maximized, but torch wants to minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def differentiable_stochastic_elbo(model, guide, observations, args, kwargs):\n",
    "    # obtain a sample from the guide\n",
    "    run_model(guide, *args, **kwargs)\n",
    "    \n",
    "    # evaluate the sample under the model\n",
    "    observed = {**observations, **sample}\n",
    "    model_probs = eval_model(model, observed, *args, **kwargs)\n",
    "    log_p = joint_log_p(model_probs)\n",
    "    \n",
    "    # evaluate the sample under the guide\n",
    "    guide_probs = eval_model(guide, sample, *args, **kwargs)\n",
    "    log_q = joint_log_p(guide_probs)\n",
    "    \n",
    "    # compute the differentiable elbo\n",
    "    r = log_p - log_q\n",
    "    diff_elbo = log_q                * r.detach()\n",
    "    #           ^ this is autodiffed   ^ this is treated as a constant that scales the gradient\n",
    "    #                                  .detach() prevents the parameters from being included via r\n",
    "    \n",
    "    return -elbo # ELBO is maximized, -ELBO is minimized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_store = {}\n",
    "\n",
    "def clear_param_store():\n",
    "    global param_store\n",
    "    param_store = {}\n",
    "\n",
    "def get_param_store():\n",
    "    return param_store.copy()\n",
    "\n",
    "def param(name, initial_value):\n",
    "    \"\"\"Register a parameter in the parameter store.\n",
    "    \n",
    "    Returns the value of the parameter or `initial_value` if the parameter does not exist.\"\"\"\n",
    "    return para_store.setdefault(name, initial_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Optimizer:\n",
    "    def __init__(self, opt, *args, **kwargs):\n",
    "        self.opt = opt\n",
    "        self.args = args\n",
    "        self.kwargs = kwargs\n",
    "        self.param_opts = {} # stores a separate optimizer instance for each parameter\n",
    "    \n",
    "    def step(self, params):\n",
    "        for name, param in params.items():\n",
    "            # optimize each parameter\n",
    "            if name in self.param_dict:\n",
    "                # already seen? then just retrieve its optimizer\n",
    "                param_opt = self.param_opts[name]\n",
    "            else:\n",
    "                # not seen yet? then create a new optimizer and store it\n",
    "                param_opt = self.opt([param], *self.args, **self.kwargs)\n",
    "                self.param_opts[name] = param_opt\n",
    "            \n",
    "            # use the optimizer to perfom the parameter update\n",
    "            param_opt.step()\n",
    "            param_opt.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVI:\n",
    "    def __init__(self, model, guide, obs, opt, loss):\n",
    "        self.model = model\n",
    "        self.guide = guide\n",
    "        self.obs   = obs\n",
    "        self.opt   = opt\n",
    "        self.loss  = loss\n",
    "        \n",
    "        # for simplicity we assume a fixed set of parameters that we get by running the guide once.\n",
    "        self.guide()\n",
    "    \n",
    "    def step(self, *args, **kwargs):\n",
    "        # run the loss function and trace the parameters\n",
    "        clear_param_store()\n",
    "        loss = self.loss(self.model, self.guide, self.obs, args, kwargs)\n",
    "        params = get_param_store()\n",
    "        \n",
    "        # compute the gradient\n",
    "        loss.backward()\n",
    "        \n",
    "        # run the optimizer\n",
    "        self.opt.step()\n",
    "        \n",
    "        # return the loss value\n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "kernelspec": {
   "display_name": "harmony-model",
   "language": "python",
   "name": "harmony-model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
