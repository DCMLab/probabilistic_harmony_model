{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Pyro Really Works\n",
    "\n",
    "## or How to Write Yourself a Probabilistic Programming Framwork\n",
    "\n",
    "Ever wondered how pyro really works? How does it do variational inference, and what is that in the first place? It's conceptually simpler than one might think, because it follows from the combination of a handful of rather simple ideas. In this tutorial we will look at each of them in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Joint, Marginal, and Conditional Distributions\n",
    "\n",
    "### Basics\n",
    "\n",
    "Probability distributions may be defined on several random variables (RVs), e.g. $a$, $b$, and $c$. The distribution over all variables $p(a,b,c)$ is called the *joint distribution*, which is normalized such that the sum over all possible RV assignments is 1. (In the continuous case, we integrate instead of summing.)\n",
    "\n",
    "The *marginal distribution* of a subset of the RVs is obtained from the joint distribution by summing (or integrating) over all remaining variables. for example\n",
    "$$ p(a) = \\sum_b \\sum_c p(a,b,c) $$\n",
    "and\n",
    "$$ p(a,b) = \\sum_c p(a,b,c) $$\n",
    "\n",
    "A *conditional distribution* is the distribution of some of the RVs given that other RVs are known to have a certain value, e.g. $p(a,b \\mid c=1)$.\n",
    "If the value of the known is arbitrary, we write $p(a,b \\mid c)$.\n",
    "If we are only interested in the distribution of some of the unknown RVs, we can again marginalize out the remaining unknown RVs, e.g.\n",
    "$$ p(a \\mid c) = \\sum_b p(a, b | c) $$\n",
    "\n",
    "We can obtain the conditional distribution from the joint distribution by fixing the value of the known RVs ($p(a,b,c=1)$).\n",
    "Since we now only look at a subset of the RV assignment (e.g. those in which $c=1$), we have to renormalize by the probability to have one of these assignments $p(c=1)$:\n",
    "$$ p(a,b \\mid c=1) = \\dfrac{p(a,b,c=1)}{p(c=1)}$$\n",
    "or more generally\n",
    "$$ p(a,b \\mid c) = \\dfrac{p(a,b,c)}{p(c)} $$\n",
    "\n",
    "### Factorizing a Joint Distribution\n",
    "\n",
    "From the above section it follows that every joint distribution can be factorized into simpler conditional and marginal distributions.\n",
    "For example, we can turn the previous equation around and obtain\n",
    "$$ p(a,b,c) = p(a,b \\mid c) p(c) $$\n",
    "Since $p(a,b \\mid c)$ can be understood as another joint distribution (of $a$ and $b$, given $c$), it can be factorized again (given $c$!), e.g.:\n",
    "$$ p(a,b,c) = p(a,b \\mid c) p(c) = p(a \\mid b,c) p(b \\mid c) p(c). $$\n",
    "Note that the order in which we factor out variables doesn't matter, so the following is equivalent:\n",
    "$$ p(a,b,c) = p(b, c \\mid a) p(a) = p(b \\mid c,a) p(c \\mid a) p(a). $$\n",
    "\n",
    "We can understand any factorization as an ordering of the RVs when sampling from the joint distribution.\n",
    "Instead of sampling all of them together, we sample each RV from its conditional distribution, starting with the one that is sampled from its unconditional marginal distribution.\n",
    "For example, in the first factorization above ($p(a \\mid b,c) p(b \\mid c) p(c)$, we first sample $c$ from $p(c)$, which doesn't depend on any of the other RVs.\n",
    "Then, knowing the value of $c$, we can sample $b$ from $p(b \\mid c)$, and finally (knowing $b$ and $c$) we can sample $a$ from $p(a \\mid b,c)$.\n",
    "Thus we have turned the joint distribution $p(a,b,c)$ into a *generative process* that produces each RV in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bayesian Inference\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 0.5 * x**2 + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = f(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad = True\n",
    "y = f(x)\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient =  tensor(1.)\n",
      "x =  tensor(0.9000)\n",
      "gradient =  tensor(0.9000)\n",
      "x =  tensor(0.8100)\n",
      "gradient =  tensor(0.8100)\n",
      "x =  tensor(0.7290)\n",
      "gradient =  tensor(0.7290)\n",
      "x =  tensor(0.6561)\n",
      "gradient =  tensor(0.6561)\n",
      "x =  tensor(0.5905)\n",
      "gradient =  tensor(0.5905)\n",
      "x =  tensor(0.5314)\n",
      "gradient =  tensor(0.5314)\n",
      "x =  tensor(0.4783)\n",
      "gradient =  tensor(0.4783)\n",
      "x =  tensor(0.4305)\n",
      "gradient =  tensor(0.4305)\n",
      "x =  tensor(0.3874)\n",
      "gradient =  tensor(0.3874)\n",
      "x =  tensor(0.3487)\n",
      "gradient =  tensor(0.3487)\n",
      "x =  tensor(0.3138)\n",
      "gradient =  tensor(0.3138)\n",
      "x =  tensor(0.2824)\n",
      "gradient =  tensor(0.2824)\n",
      "x =  tensor(0.2542)\n",
      "gradient =  tensor(0.2542)\n",
      "x =  tensor(0.2288)\n",
      "gradient =  tensor(0.2288)\n",
      "x =  tensor(0.2059)\n",
      "gradient =  tensor(0.2059)\n",
      "x =  tensor(0.1853)\n",
      "gradient =  tensor(0.1853)\n",
      "x =  tensor(0.1668)\n",
      "gradient =  tensor(0.1668)\n",
      "x =  tensor(0.1501)\n",
      "gradient =  tensor(0.1501)\n",
      "x =  tensor(0.1351)\n",
      "gradient =  tensor(0.1351)\n",
      "x =  tensor(0.1216)\n",
      "gradient =  tensor(0.1216)\n",
      "x =  tensor(0.1094)\n",
      "gradient =  tensor(0.1094)\n",
      "x =  tensor(0.0985)\n",
      "gradient =  tensor(0.0985)\n",
      "x =  tensor(0.0886)\n",
      "gradient =  tensor(0.0886)\n",
      "x =  tensor(0.0798)\n",
      "gradient =  tensor(0.0798)\n",
      "x =  tensor(0.0718)\n",
      "gradient =  tensor(0.0718)\n",
      "x =  tensor(0.0646)\n",
      "gradient =  tensor(0.0646)\n",
      "x =  tensor(0.0581)\n",
      "gradient =  tensor(0.0581)\n",
      "x =  tensor(0.0523)\n",
      "gradient =  tensor(0.0523)\n",
      "x =  tensor(0.0471)\n",
      "gradient =  tensor(0.0471)\n",
      "x =  tensor(0.0424)\n",
      "gradient =  tensor(0.0424)\n",
      "x =  tensor(0.0382)\n",
      "gradient =  tensor(0.0382)\n",
      "x =  tensor(0.0343)\n",
      "gradient =  tensor(0.0343)\n",
      "x =  tensor(0.0309)\n",
      "gradient =  tensor(0.0309)\n",
      "x =  tensor(0.0278)\n",
      "gradient =  tensor(0.0278)\n",
      "x =  tensor(0.0250)\n",
      "gradient =  tensor(0.0250)\n",
      "x =  tensor(0.0225)\n",
      "gradient =  tensor(0.0225)\n",
      "x =  tensor(0.0203)\n",
      "gradient =  tensor(0.0203)\n",
      "x =  tensor(0.0182)\n",
      "gradient =  tensor(0.0182)\n",
      "x =  tensor(0.0164)\n",
      "gradient =  tensor(0.0164)\n",
      "x =  tensor(0.0148)\n",
      "gradient =  tensor(0.0148)\n",
      "x =  tensor(0.0133)\n",
      "gradient =  tensor(0.0133)\n",
      "x =  tensor(0.0120)\n",
      "gradient =  tensor(0.0120)\n",
      "x =  tensor(0.0108)\n",
      "gradient =  tensor(0.0108)\n",
      "x =  tensor(0.0097)\n",
      "gradient =  tensor(0.0097)\n",
      "x =  tensor(0.0087)\n",
      "gradient =  tensor(0.0087)\n",
      "x =  tensor(0.0079)\n",
      "gradient =  tensor(0.0079)\n",
      "x =  tensor(0.0071)\n",
      "gradient =  tensor(0.0071)\n",
      "x =  tensor(0.0064)\n",
      "gradient =  tensor(0.0064)\n",
      "x =  tensor(0.0057)\n",
      "gradient =  tensor(0.0057)\n",
      "x =  tensor(0.0052)\n",
      "gradient =  tensor(0.0052)\n",
      "x =  tensor(0.0046)\n",
      "gradient =  tensor(0.0046)\n",
      "x =  tensor(0.0042)\n",
      "gradient =  tensor(0.0042)\n",
      "x =  tensor(0.0038)\n",
      "gradient =  tensor(0.0038)\n",
      "x =  tensor(0.0034)\n",
      "gradient =  tensor(0.0034)\n",
      "x =  tensor(0.0030)\n",
      "gradient =  tensor(0.0030)\n",
      "x =  tensor(0.0027)\n",
      "gradient =  tensor(0.0027)\n",
      "x =  tensor(0.0025)\n",
      "gradient =  tensor(0.0025)\n",
      "x =  tensor(0.0022)\n",
      "gradient =  tensor(0.0022)\n",
      "x =  tensor(0.0020)\n",
      "gradient =  tensor(0.0020)\n",
      "x =  tensor(0.0018)\n",
      "gradient =  tensor(0.0018)\n",
      "x =  tensor(0.0016)\n",
      "gradient =  tensor(0.0016)\n",
      "x =  tensor(0.0015)\n",
      "gradient =  tensor(0.0015)\n",
      "x =  tensor(0.0013)\n",
      "gradient =  tensor(0.0013)\n",
      "x =  tensor(0.0012)\n",
      "gradient =  tensor(0.0012)\n",
      "x =  tensor(0.0011)\n",
      "gradient =  tensor(0.0011)\n",
      "x =  tensor(0.0010)\n",
      "gradient =  tensor(0.0010)\n",
      "x =  tensor(0.0009)\n",
      "gradient =  tensor(0.0009)\n",
      "x =  tensor(0.0008)\n",
      "gradient =  tensor(0.0008)\n",
      "x =  tensor(0.0007)\n",
      "gradient =  tensor(0.0007)\n",
      "x =  tensor(0.0006)\n",
      "gradient =  tensor(0.0006)\n",
      "x =  tensor(0.0006)\n",
      "gradient =  tensor(0.0006)\n",
      "x =  tensor(0.0005)\n",
      "gradient =  tensor(0.0005)\n",
      "x =  tensor(0.0005)\n",
      "gradient =  tensor(0.0005)\n",
      "x =  tensor(0.0004)\n",
      "gradient =  tensor(0.0004)\n",
      "x =  tensor(0.0004)\n",
      "gradient =  tensor(0.0004)\n",
      "x =  tensor(0.0003)\n",
      "gradient =  tensor(0.0003)\n",
      "x =  tensor(0.0003)\n",
      "gradient =  tensor(0.0003)\n",
      "x =  tensor(0.0003)\n",
      "gradient =  tensor(0.0003)\n",
      "x =  tensor(0.0002)\n",
      "gradient =  tensor(0.0002)\n",
      "x =  tensor(0.0002)\n",
      "gradient =  tensor(0.0002)\n",
      "x =  tensor(0.0002)\n",
      "gradient =  tensor(0.0002)\n",
      "x =  tensor(0.0002)\n",
      "gradient =  tensor(0.0002)\n",
      "x =  tensor(0.0002)\n",
      "gradient =  tensor(0.0002)\n",
      "x =  tensor(0.0001)\n",
      "gradient =  tensor(0.0001)\n",
      "x =  tensor(0.0001)\n",
      "gradient =  tensor(0.0001)\n",
      "x =  tensor(0.0001)\n",
      "gradient =  tensor(0.0001)\n",
      "x =  tensor(0.0001)\n",
      "gradient =  tensor(0.0001)\n",
      "x =  tensor(9.4046e-05)\n",
      "gradient =  tensor(9.4046e-05)\n",
      "x =  tensor(8.4642e-05)\n",
      "gradient =  tensor(8.4642e-05)\n",
      "x =  tensor(7.6177e-05)\n",
      "gradient =  tensor(7.6177e-05)\n",
      "x =  tensor(6.8560e-05)\n",
      "gradient =  tensor(6.8560e-05)\n",
      "x =  tensor(6.1704e-05)\n",
      "gradient =  tensor(6.1704e-05)\n",
      "x =  tensor(5.5533e-05)\n",
      "gradient =  tensor(5.5533e-05)\n",
      "x =  tensor(4.9980e-05)\n",
      "gradient =  tensor(4.9980e-05)\n",
      "x =  tensor(4.4982e-05)\n",
      "gradient =  tensor(4.4982e-05)\n",
      "x =  tensor(4.0484e-05)\n",
      "gradient =  tensor(4.0484e-05)\n",
      "x =  tensor(3.6435e-05)\n",
      "gradient =  tensor(3.6435e-05)\n",
      "x =  tensor(3.2792e-05)\n",
      "gradient =  tensor(3.2792e-05)\n",
      "x =  tensor(2.9513e-05)\n",
      "gradient =  tensor(2.9513e-05)\n",
      "x =  tensor(2.6561e-05)\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "lr = 0.1\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "for i in range(100):\n",
    "    y = f(x)\n",
    "    y.backward()\n",
    "    print('gradient = ', x.grad)\n",
    "    x.data -= lr * x.grad\n",
    "    print('x = ', x.data)\n",
    "    x.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chfin/dateien/dev/python/harmony-model/env/lib/python3.8/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harmony-model",
   "language": "python",
   "name": "harmony-model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
