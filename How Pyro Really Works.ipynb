{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How Pyro Really Works\n",
    "\n",
    "## or How to Write Yourself a Probabilistic Programming Framwork\n",
    "## or Please Try This at Home\n",
    "\n",
    "Ever wondered how pyro (or probabilistic programming in general) really works? How does it do variational inference, and what is that in the first place? It's conceptually simpler than one might think, because it follows from the combination of a handful of rather simple ideas. In this tutorial we will look at each of them in turn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Joint, Marginal, and Conditional Distributions\n",
    "\n",
    "#### Summary:\n",
    "\n",
    "- If we have a *joint distribution* ($p(A,B)$),\n",
    "  we can express *marginal* ($p(A)$) and *conditional distributions* $p(A \\mid B)$ based on the joint distribution\n",
    "- Conversely, joint distributions can always be *factorized* into a product of conditional and marginal distributions\n",
    "  $$ p(A,B) = p(A \\mid B) p(B) $$\n",
    "- Sampling the variables in some order gives rise to a factorization\n",
    "  $$ p(x_1, \\ldots, x_n) = \\prod_i p(x_i \\mid x_1, \\ldots x_{i-1}) $$\n",
    "  and describes a *generative process*.\n",
    "\n",
    "### Basics\n",
    "\n",
    "Probability distributions may be defined on several random variables (RVs), e.g. $A$, $B$, and $C$. The distribution over all variables $p(A,B,C)$ is called the *joint distribution*, which is normalized such that the sum over all possible RV assignments is 1. (In the continuous case, we integrate instead of summing.)\n",
    "\n",
    "The *marginal distribution* of a subset of the RVs is obtained from the joint distribution by summing (or integrating) over all remaining variables. for example\n",
    "$$ p(A) = \\sum_B \\sum_C p(A,B,C) $$\n",
    "and\n",
    "$$ p(A,B) = \\sum_C p(A,B,C) $$\n",
    "\n",
    "A *conditional distribution* is the distribution of some of the RVs given that other RVs are known to have a certain value, e.g. $p(A,B \\mid C=c)$.\n",
    "If the value of the known RV is arbitrary, we write $p(A,B \\mid C)$.\n",
    "If we are only interested in the distribution of some of the unknown RVs, we can again marginalize out the remaining unknown RVs, e.g.\n",
    "$$ p(A \\mid C) = \\sum_B p(A, B | C) $$\n",
    "\n",
    "We can obtain the conditional distribution from the joint distribution by fixing the value of the known RVs ($p(A,B,C=c)$).\n",
    "Since we now only look at a subset of the RV assignment (e.g. those in which $C=c$), we have to renormalize by the probability to have one of these assignments $p(C=c)$:\n",
    "$$ p(A,B \\mid C=c) = \\dfrac{p(A,B,C=c)}{p(C=c)}$$\n",
    "or more generally\n",
    "$$ p(A,B \\mid C) = \\dfrac{p(A,B,C)}{p(C)} $$\n",
    "\n",
    "### Factorizing a Joint Distribution\n",
    "\n",
    "From the above section it follows that every joint distribution can be factorized into simpler conditional and marginal distributions.\n",
    "For example, we can turn the previous equation around and obtain\n",
    "$$ p(A,B,C) = p(A,B \\mid C) p(C) $$\n",
    "\n",
    "Since $p(A,B \\mid C)$ can be understood as another joint distribution (of $A$ and $B$, given $C$), it can be factorized again (given $C$!), e.g.:\n",
    "$$ p(A,B \\mid C) = p(A \\mid B,C) p(B \\mid C) $$\n",
    "\n",
    "and consequently\n",
    "$$ p(A,B,C) = p(A,B \\mid C) p(C) = p(A \\mid B,C) p(B \\mid C) p(C). $$\n",
    "\n",
    "Note that the order in which we factor out variables doesn't matter, so the following is equivalent:\n",
    "$$ p(A,B,C) = p(B,C \\mid A) p(A) = p(B \\mid C,A) p(C \\mid A) p(A). $$\n",
    "\n",
    "We can understand any factorization as an ordering of the RVs when sampling from the joint distribution.\n",
    "Instead of sampling all of them together, we sample each RV from its conditional distribution, starting with the one that is sampled from its unconditional marginal distribution.\n",
    "For example, in the first factorization above ($p(A \\mid B,C) p(B \\mid C) p(C)$, we first sample $C$ from $p(C)$, which doesn't depend on any of the other RVs.\n",
    "Then, knowing the value $c$ of $C$, we can sample $B$ from $p(B \\mid C=c)$, and finally (knowing $B=b$ and $C=c$) we can sample $A$ from $p(A \\mid B=b, C=c)$.\n",
    "Thus we have turned the joint distribution $p(A,B,C)$ into a *generative process* that produces each RV in turn.\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1. Given the joint distribution $p(A,B,C,D)$, what is the marginal distribution $p(A,C)$?\n",
    "1. Given the same joint distribution $p(A,B,C,D)$, what is the conditional distribution $p(A,B | C)$?\n",
    "   Express it only using the joint distribution.\n",
    "1. Let $p(X,Y,Z) = p(Z \\mid X,Y) p(X) p(Y)$. What is $p(X \\mid Y$? What is $p(Y \\mid Z, X)$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Bayesian Inference\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- Bayesian models are defined by a *joint distribution* $p(X,Z)$\n",
    "  over observed ($X$) and unobserved variables ($Z$).\n",
    "- Bayesian inference is computing the *posterior distribution* $p(Z \\mid X)$.\n",
    "- Sometimes, the joint distribution is given as factorized into a *likelihood* $p(X \\mid Z)$ and a *prior* $p(Z)$.\n",
    "\n",
    "### Bayesian Models\n",
    "\n",
    "Bayesian inference builds on the \"bayesian\" interpretation of probabilities as talking about the plausibility of statements.\n",
    "BI starts from the assumption that we can define a joint *plausibility distribution* over observed ($X$) and unobserved variables ($Z$): $p(X,Z)$, which assings a plausibility $p(X=x, Z=z)$ to any particular instantiation of $X$ and $Z$.\n",
    "Doing *inference* means to asses the plausibility of latent variable assignments, given that the observed variables take some (observed) value: $p(Z \\mid X=x)$.\n",
    "\n",
    "#### Example\n",
    "\n",
    "While the above may sound very abstract, a common situation in which it is useful is when we have a probabilistic model (i.e. a probability distribution) of some data $d$, i.e. a distribution so that we can interpret the data $d$ as sampled from a random variable $D$.\n",
    "Our distribution over $D$ has some parameters $\\theta$, so we can write it as $p_\\theta(D)$.\n",
    "If we change $\\theta$, we change the distribution.\n",
    "\n",
    "Now we want a good value for $\\theta$.\n",
    "Since the distribution is suppose to \"model\" the data, we want $\\theta$ to be chosen such that $D=d$ has a very high probability under $p_\\theta$.\n",
    "In other words, we want to maximize the function $\\theta \\to p_\\theta(D=d)$.\n",
    "This function is called the *likelihood function* and the optimal value for $\\theta$ is called the *maximum likelihood estimate* (MLE).\n",
    "\n",
    "While we know that the MLE is the \"best\" value of $\\theta$ given $d$, we don't really know how it compares to other possible values, i.e. *how* good it really is.\n",
    "In other words, we want to know how *plausible* it is that some value of $\\theta$ is the \"true\" $\\theta$ that was used to generate the data.\n",
    "We can quantify the plausibility of different $\\theta$ values as a plausibility distribution $p(\\theta \\mid D=d)$.\n",
    "Note the similarity to the general description of bayesian inference above, where we look for $p(Z \\mid X=x)$.\n",
    "Just like in the case above we can derive $p(Z \\mid X=x)$ from $p(X,Z)$, we can now derive $p(\\theta \\mid D=d)$ from the joint distribution $p(D, \\theta)$.\n",
    "This joint distribution describes the plausibility of certain values of $\\theta$ and $D$ occurring together.\n",
    "\n",
    "---\n",
    "\n",
    "#### Definition\n",
    "\n",
    "A **bayesian model** is defined by a joint distribution $p(X,Z)$ over *unobserved (/hidden/latent) variables* $Z$ (e.g. model parameters) and *observed variables* $X$.\n",
    "\n",
    "**Bayesian inference** is about infering a plausibility distribution over the $Z$ given observed values for the $X$: $p(Z \\mid X=x)$.\n",
    "This distribution is called the *posterior distribution*, because it expresses our beliefs about the latent variables *after* knowing the values of the observed variable.\n",
    "\n",
    "---\n",
    "\n",
    "### Inference in Bayesian Models\n",
    "\n",
    "How can we compute $p(Z \\mid X=x)$?\n",
    "We know from the definition of conditional probability that\n",
    "$$ p(Z \\mid X=x) = \\dfrac{p(X,Z)}{p(X)}, $$\n",
    "so we can in principle derive the posterior distribution from the joint distribution.\n",
    "But where does the join distribution come from?\n",
    "\n",
    "Remember that, in many cases, we know (or assume to know) the likelihood, i.e. $p(X \\mid Z)$.\n",
    "We also know that the likelihood relates to the joint distribution by\n",
    "$$ p(X,Z) = p(X \\mid Z) p(Z), $$\n",
    "i.e. the joint distribution *factorizes* into the likelihood and the *prior distribution*,\n",
    "the marginal distribution of the hidden variables.\n",
    "\n",
    "What is the prior distribution?\n",
    "We know that it is the marginal distribution of $Z$, so we can interpret it as describing the plausibility of values for $Z$ given that $X$ could take *any* values (we even sum/integrate over all values for $X$).\n",
    "In other words, $p(Z)$ expresses our beliefs about $Z$ irrespective of $X$, i.e. *before* observing $X$.\n",
    "If we can somehow encode our prior beliefs about $Z$ as a distribution, we can combine it the likelihood to obtain a joint distribution.\n",
    "\n",
    "Combining the likelihood and the prior, we can make our formula for the posterior more precise:\n",
    "$$ p(Z \\mid X=x) = \\dfrac{p(X \\mid Z) p(Z)}{p(X=x)}, $$\n",
    "which is known as *Bayes' theorem*.\n",
    "The marginal probability of the data $p(X=x)$ is called *evidence* and serves to normalize the posterior when expressed in terms of prior and likelihood.\n",
    "While it is often impossible to compute the evidence (because it requires summing/integrating over all $Z$),\n",
    "it is constant for a given dataset ($X=x$), so we might want to write the posterior as\n",
    "$$ p(Z \\mid X=x) \\propto p(X \\mid Z) p(Z). $$\n",
    "\n",
    "#### Side Remark\n",
    "\n",
    "Bayesian inference is often introduced in terms of Bayes' theorem, i.e. distinguishing likelihood and prior,\n",
    "this is just special case.\n",
    "Generally, what is required is only a joint distribution.\n",
    "Since we often know the likelihood and can construct reasonable priors, they just provide a convenient factorization of the joint distribution.\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1. Show that Bayes' theorem follows directly from the possibility to factorize a joint distribution in different ways.\n",
    "2. You have a dataset of measurements $d = [3, 4.5, 5, 3.5]$.\n",
    "   You think that each data point is drawn independently from the same normal distribution with mean $\\mu$ and standard deviation $\\sigma = 1$,\n",
    "   i.e. $p(d) = \\prod_i p(d_i)$ where $d_i \\sim \\text{Normal}(\\mu, 1)$.\n",
    "   \n",
    "   What is the MLE for $\\mu$?\n",
    "   Derive a general formula as well as the MLE for above dataset.\n",
    "   Remember that the density function for the normal distribution is\n",
    "   $$ f(x) = \\dfrac{1}{\\sigma \\sqrt{\\tau}} e^{-\\dfrac{1}{2}\\left(\\dfrac{x-\\mu}{\\sigma}\\right)^2}. $$\n",
    "   Use the fact that maximizing $l(x)$ is the same as maximizing $\\log(l(x))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Probabilistic Programming 1: Writing Probabilistic Programs\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- Any Python program that samples values defines a probability distribution.\n",
    "- We can obtain a sample from the distribution by running the program and tracking each randomly chosen value\n",
    "  using a `sample` function, which gives the value a name.\n",
    "- Each \"random value\" can be understood as an assignment of a random variable.\n",
    "\n",
    "### Wrting Probabilistic Programs\n",
    "\n",
    "The idea of probabilistic programming is to define a joint probability distribution over several RVs as a generative process, i.e. a programm that draws samples from the joint distribution.\n",
    "The advantage of this is that it's a lot easier to do and a lot more readble than defining a closed formula for the whole joint distribution.\n",
    "The disadvantage is that it it's much more difficult to perform analytic math on a program.\n",
    "FWIW, a program is something that we can execute but not much more, we can't really look inside.\n",
    "But let's first look at the advantages, we'll deal with the disadvantages later.\n",
    "\n",
    "Let's assume that we want to implement our own little probabilistic programming framework.\n",
    "What do we need?\n",
    "First of all, we need a way to write programs that produce some kind of output.\n",
    "Fortunately, programming languages are already very good at letting you write pograms.\n",
    "For example, we can use Python to write a function that computes and returns some kind of result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def my_function():\n",
    "    x = 10\n",
    "    y = x*2\n",
    "    return y\n",
    "\n",
    "my_function()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the above function is a program that computes a deterministic value, a probabilistic program is a mix of deterministic computation and random sampling.\n",
    "For example, the following function flips a coin to determine whether `y` should be `x*2` or `x*3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def my_random_function(p=0.5):\n",
    "    x = 10\n",
    "    heads = random.random() < p\n",
    "    y = x*2 if heads else x*3\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we run this function several times, it will return different results, sometimes 20 and sometimes 30, depending on the result of the coin flip.\n",
    "We can look at the distribution over the result by running the programm several times and collecting the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "results = [my_random_function() for i in range(100)]\n",
    "\n",
    "counts = Counter(results)\n",
    "plt.bar(counts.keys(), counts.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we provide a different coin probability, the result distribution will be different as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXAAAAD4CAYAAAD1jb0+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAAM4UlEQVR4nO3dbYyl9VnH8e/PXegTWqA7xRXQWS21IUaBrEiDDxHUUGkKLwjBVLMqCRG1Aq2225rY6Ctom2JNjGbTbbMviAUpCrE+lFIw+oKtszyIsK0gQlm6wFSLtBrFtZcvzk2YLrMzh5lz5vTafj/JZs65z33mXP+c4Tv33GfOkKpCktTPt816AEnS2hhwSWrKgEtSUwZckpoy4JLU1OaNfLAtW7bU/Pz8Rj6kJLW3b9++L1fV3OHbNzTg8/PzLCwsbORDSlJ7SR5fbrunUCSpKQMuSU0ZcElqyoBLUlMGXJKaMuCS1JQBl6SmDLgkNWXAJampDX0npiR9M5vf+ampfN7Hrr1wKp/XI3BJasqAS1JTBlySmjLgktSUAZekpgy4JDVlwCWpKQMuSU0ZcElqyoBLUlMGXJKaMuCS1JQBl6SmDLgkNWXAJakpAy5JTRlwSWrKgEtSUwZckpoy4JLUlAGXpKYMuCQ1ZcAlqSkDLklNGXBJasqAS1JTBlySmjLgktTUWAFPck2SB5P8U5I/SfLKJNuS7E3ySJIbkxw77WElSS9aNeBJTgZ+A9heVT8AbAIuA64Drq+qNwBfAS6f5qCSpG807imUzcCrkmwGXg0cBM4Dbh5u3wNcPPnxJElHsmrAq+pJ4EPAFxmF+z+AfcCzVXVo2O0AcPJy909yRZKFJAuLi4uTmVqSNNYplBOAi4BtwHcBrwEuGPcBqmpXVW2vqu1zc3NrHlSS9I3GOYXyU8C/VtViVf0vcAtwLnD8cEoF4BTgySnNKElaxjgB/yJwTpJXJwlwPvAQcCdwybDPDuDW6YwoSVrOOOfA9zJ6sfIe4IHhPruA9wDvTPII8Dpg9xTnlCQdZvPqu0BVvR94/2GbHwXOnvhEkqSx+E5MSWrKgEtSUwZckpoy4JLUlAGXpKYMuCQ1ZcAlqSkDLklNGXBJasqAS1JTBlySmjLgktSUAZekpgy4JDVlwCWpKQMuSU0ZcElqyoBLUlMGXJKaMuCS1JQBl6SmDLgkNWXAJakpAy5JTRlwSWrKgEtSUwZckpoy4JLUlAGXpKYMuCQ1ZcAlqSkDLklNGXBJasqAS1JTBlySmhor4EmOT3Jzks8n2Z/kzUlOTHJ7koeHjydMe1hJ0ovGPQL/CPDXVfUm4IeA/cBO4I6qOg24Y7guSdogqwY8yWuBHwd2A1TV81X1LHARsGfYbQ9w8bSGlCS91DhH4NuAReDjSe5N8tEkrwFOqqqDwz5PASctd+ckVyRZSLKwuLg4maklSWMFfDNwFvBHVXUm8J8cdrqkqgqo5e5cVbuqantVbZ+bm1vvvJKkwTgBPwAcqKq9w/WbGQX96SRbAYaPz0xnREnSclYNeFU9BTyR5PuHTecDDwG3ATuGbTuAW6cyoSRpWZvH3O8dwA1JjgUeBX6JUfxvSnI58Dhw6XRGlCQtZ6yAV9V9wPZlbjp/suNIksblOzElqSkDLklNGXBJasqAS1JTBlySmjLgktSUAZekpgy4JDVlwCWpKQMuSU0ZcElqyoBLUlMGXJKaMuCS1JQBl6SmDLgkNWXAJakpAy5JTRlwSWrKgEtSUwZckpoy4JLUlAGXpKYMuCQ1ZcAlqSkDLklNGXBJasqAS1JTBlySmjLgktSUAZekpgy4JDVlwCWpKQMuSU2NHfAkm5Lcm+QvhuvbkuxN8kiSG5McO70xJUmHezlH4FcB+5dcvw64vqreAHwFuHySg0mSVjZWwJOcAlwIfHS4HuA84OZhlz3AxdMYUJK0vHGPwH8feDfw9eH664Bnq+rQcP0AcPKEZ5MkrWDVgCd5K/BMVe1bywMkuSLJQpKFxcXFtXwKSdIyxjkCPxd4W5LHgE8wOnXyEeD4JJuHfU4BnlzuzlW1q6q2V9X2ubm5CYwsSYIxAl5V762qU6pqHrgM+GxVvR24E7hk2G0HcOvUppQkvcR6fg/8PcA7kzzC6Jz47smMJEkax+bVd3lRVd0F3DVcfhQ4e/IjSZLG4TsxJakpAy5JTRlwSWrKgEtSUwZckpoy4JLUlAGXpKYMuCQ1ZcAlqSkDLklNGXBJasqAS1JTBlySmjLgktSUAZekpgy4JDVlwCWpKQMuSU0ZcElqyoBLUlMGXJKaMuCS1JQBl6SmDLgkNWXAJakpAy5JTRlwSWrKgEtSUwZckpoy4JLUlAGXpKYMuCQ1ZcAlqSkDLklNGXBJamrVgCc5NcmdSR5K8mCSq4btJya5PcnDw8cTpj+uJOkF4xyBHwLeVVWnA+cAv5bkdGAncEdVnQbcMVyXJG2QVQNeVQer6p7h8leB/cDJwEXAnmG3PcDF0xpSkvRSL+sceJJ54ExgL3BSVR0cbnoKOOkI97kiyUKShcXFxXWMKklaauyAJzkO+CRwdVU9t/S2qiqglrtfVe2qqu1VtX1ubm5dw0qSXjRWwJMcwyjeN1TVLcPmp5NsHW7fCjwznRElScsZ57dQAuwG9lfVh5fcdBuwY7i8A7h18uNJko5k8xj7nAv8AvBAkvuGbe8DrgVuSnI58Dhw6XRGlCQtZ9WAV9XfAznCzedPdhxJ0rh8J6YkNWXAJakpAy5JTRlwSWrKgEtSUwZckpoy4JLUlAGXpKYMuCQ1ZcAlqSkDLklNGXBJasqAS1JTBlySmjLgktSUAZekpgy4JDVlwCWpKQMuSU0ZcElqyoBLUlMGXJKaMuCS1JQBl6SmDLgkNWXAJakpAy5JTW2e9QDjmt/5qal97seuvXBqn1uSpsUjcElqyoBLUlMGXJKaMuCS1JQBl6SmDLgkNWXAJampdQU8yQVJvpDkkSQ7JzWUJGl1aw54kk3AHwJvAU4Hfi7J6ZMaTJK0svUcgZ8NPFJVj1bV88AngIsmM5YkaTXreSv9ycATS64fAH7k8J2SXAFcMVz9WpIvrOMxx7UF+PK4O+e6KU4yPS9rjQ25vv6O9jWOvb4JNOZ7lts49b+FUlW7gF3TfpylkixU1faNfMyNdrSv0fX1d7Sv8Zthfes5hfIkcOqS66cM2yRJG2A9Af8H4LQk25IcC1wG3DaZsSRJq1nzKZSqOpTk14G/ATYBH6uqByc22fps6CmbGTna1+j6+jva1zjz9aWqZj2DJGkNfCemJDVlwCWpqfYBT3JqkjuTPJTkwSRXDdtPTHJ7koeHjyfMeta1WGF9H0zy+ST/mOTPkhw/61nX4kjrW3L7u5JUki2zmnG9VlpjkncMz+ODST4wyznXaoWv0TOS3J3kviQLSc6e9axrleSVST6X5P5hjb87bN+WZO/w50RuHH6hY+NUVet/wFbgrOHytwP/zOit/R8Adg7bdwLXzXrWCa/vZ4DNw/brjrb1DddPZfQi+ePAllnPOoXn8CeBzwCvGG57/axnnfD6Pg28Zdj+s8Bds551HWsMcNxw+RhgL3AOcBNw2bD9j4ErN3Ku9kfgVXWwqu4ZLn8V2M/oXaIXAXuG3fYAF89mwvU50vqq6tNVdWjY7W5Gv4ffzgrPH8D1wLuB1q+0r7DGK4Frq+p/htuemd2Ua7fC+gr4jmG31wJfms2E61cjXxuuHjP8K+A84OZh+4Z3pn3Al0oyD5zJ6LvjSVV1cLjpKeCkGY01MYetb6lfBv5qo+eZtKXrS3IR8GRV3T/ToSbssOfwjcCPDT+C/22SH57lbJNw2PquBj6Y5AngQ8B7ZzfZ+iXZlOQ+4BngduBfgGeXHEgd4MWDjw1x1AQ8yXHAJ4Grq+q5pbfV6Oeb1kdxR1pfkt8GDgE3zGq2SVi6PkbreR/wOzMdasKWeQ43Aycy+lH8t4CbkmSGI67LMuu7Erimqk4FrgF2z3K+9aqq/6uqMxj9tHs28KYZj3R0BDzJMYy+cG6oqluGzU8n2TrcvpXRd82WjrA+kvwi8Fbg7cM3qZaWWd/3AduA+5M8xug/mHuSfOfsplyfIzyHB4Bbhh/PPwd8ndEfSGrnCOvbAbxw+U8ZRa+9qnoWuBN4M3B8khfeELnhf06kfcCHI5bdwP6q+vCSm25j9AXE8PHWjZ5tEo60viQXMDo//Laq+q9Zzbdey62vqh6oqtdX1XxVzTMK3VlV9dQMR12zFb5G/5zRC5kkeSNwLA3/et8K6/sS8BPD5fOAhzd6tklJMvfCb3oleRXw04zO9d8JXDLstuGdaf9OzCQ/Cvwd8ACjIxgY/fi9l9ErxN/N6LcYLq2qf5/JkOuwwvr+AHgF8G/Dtrur6lc2fsL1OdL6quovl+zzGLC9qtrFDVZ8Dj8DfAw4A3ge+M2q+uxMhlyHFdb3HPARRqeK/hv41araN5Mh1ynJDzJ6kXITowPfm6rq95J8L6P/F8KJwL3Az7/wovSGzNU94JL0rar9KRRJ+lZlwCWpKQMuSU0ZcElqyoBLUlMGXJKaMuCS1NT/A4Aw4bv/iD3QAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "results = [my_random_function(0.1) for i in range(100)]\n",
    "\n",
    "counts = Counter(results)\n",
    "plt.bar(counts.keys(), counts.values())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that while the function returns `y`, which is randomly distributed,\n",
    "the actual sampling happens when we flip the coin, so we might want to say that `heads` is the actual random variable an `y` is just a deterministic transformation of it.\n",
    "However, since the only thing we can do with `my_random_function` is to execute it, we can't directly look at the distribution of `heads`.\n",
    "Let's change this by \"recording\" the value of `heads` every time we call `my_random_function`.\n",
    "\n",
    "We write a little helper function called `sample` that use to record the values we sampled for random variables.\n",
    "Just like we can use `print` to print any value as a side effect, we can use `sample` to record a random variable as a side effect.\n",
    "We store the RV values in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "rv_dict = dict()\n",
    "\n",
    "def clear_rvs():\n",
    "    global rv_dict\n",
    "    rv_dict = dict()\n",
    "\n",
    "def sample(name, value):\n",
    "    rv_dict[name] = value\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we adapt `my_random_function` to use `sample` to record the value of `heads`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n",
      "{'heads': False}\n"
     ]
    }
   ],
   "source": [
    "def my_random_function(p=0.5):\n",
    "    x = 10\n",
    "    heads = sample(\"heads\", random.random() < p)\n",
    "    y = x*2 if heads else x*3\n",
    "    return y\n",
    "\n",
    "clear_rvs()\n",
    "result = my_random_function()\n",
    "print(result)\n",
    "print(rv_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, after running `my_random_function` again, the value of `heads` is recorded in `rv_dict` and the return value of `my_random_function` corresponds to the recorded value of heads.\n",
    "\n",
    "What if we want to record several RVs? We can do that too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({(False, True): 50,\n",
       "         (True, True): 14,\n",
       "         (False, False): 32,\n",
       "         (True, False): 4})"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def flip_two_coins(p1=0.5, p2=0.5):\n",
    "    coin1 = sample(\"coin1\", random.random() < p1)\n",
    "    coin2 = sample(\"coin2\", random.random() < p2)\n",
    "\n",
    "results = []\n",
    "for i in range(100):\n",
    "    clear_rvs()              # clear the recorded values\n",
    "    flip_two_coins(0.2, 0.6) # record new values \n",
    "    results.append((rv_dict[\"coin1\"], rv_dict[\"coin2\"]))\n",
    "    \n",
    "counts = Counter(results)\n",
    "counts\n",
    "#plt.bar(counts.keys(), counts.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that creating many samples from this distribution (as we do above) gives us an estimate of how the random variables are distributed.\n",
    "In this case, we get the approximate frequency of every possible variable assignment, which converges to the true probability of every variable assignment when the number of samples goes to infinity.\n",
    "Even in the continuous case we can get an estimate of the shape of the distribution by drawing many samples,\n",
    "but we would have to draw many more samples (depending on the complexity of the shape) and the number of possible configurations grows exponentially with the number of variables, so just sampling is not always a good solution.\n",
    "\n",
    "### Exercises\n",
    "\n",
    "1. Let's go bayesian! Write a probabilistic program that flips a coin with probability $p$,\n",
    "   but first samples the probability $p$ from a uniform distribution $\\text{U}(0,1)$.\n",
    "   Note how this defines a joint distribution over the parameter $p$ and the result of the coin.\n",
    "1. Extend your program to flip not just 1 coin but 100, each with probability $p$.\n",
    "   Make sure that each flipped coin is a recorded as a random variable.\n",
    "   How would you write the probability distribution for the sequence of coin flips mathematically?\n",
    "1. Write a more complex program that simulates how a chord is constructed from a profile.\n",
    "   First, sample the type of the chord $t$ from a categorical distribution over chord types.\n",
    "   Then sample a number of notes $n$ from a poisson distribution (add 1 to the value to avoid 0s).\n",
    "   Finally sample $n$ notes from a multinomial distribution with the weights that correspond to $h$.\n",
    "   The parameters (weights for $h$, rate for $n$ and weights for the notes for each $h$) are given as arguments to the function.\n",
    "1. Extend the previous program to also sample the parameters from a prior distribution.\n",
    "   (Hint: use Dirichlet priors for the weight vectors and a Gamma prior for the rate of the Poisson distribution)\n",
    "1. Extend the previous program to sample a flexible number of chords (provided as an argument)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Probabilistic Programming 2: Evaluating Probabilities\n",
    "\n",
    "#### Summary\n",
    "\n",
    "- We want to be able to evaluate the probability of a given sample from the joint distribution.\n",
    "- We do that by introducing two new things:\n",
    "  - Random values are drawn from `Distribution`s which can\n",
    "    - generate a new random value and\n",
    "    - evaluate the probability of a given value.\n",
    "  - The `sample` function gets an additional interpretation that\n",
    "    - returns the given value of the RV and\n",
    "    - tracks that values local probability under the given distribution.\n",
    "- The joint probability of the sample is the product of all local probabilities.\n",
    "  - The local probabilities are conditional distributions\n",
    "  - The probabilistic program defines a factorization of the joint distribution into local conditional distributions.\n",
    "- Any probabilistic program written using `sample` and `Distributions` can be used to\n",
    "  - draw a sample\n",
    "  - evaluate the probability of a given sample\n",
    "  - despite containing arbitrary Python code with unknown control flow.\n",
    "- Drawing and evaluating samples is sufficient for inference.\n",
    "\n",
    "So far, we have defined a probability distribution implicitly as a program that produces samples from the distribution.\n",
    "We have also seen that by sampling a lot from this distribution, we can estimate it's shape.\n",
    "While this is already useful, it's not quite enough to make serious inference, especially for more complex models.\n",
    "There is one more thing that we need: In addition to sampling from the distribution, we also need to be able to evaluate the probability of a given sample.\n",
    "With these two things, we will be able to do pretty much everything we need for performing bayesian inference.\n",
    "(If it's not clear to you why, don't worry, it will be explained later).\n",
    "\n",
    "In their current form, our probabilistic programs are a bit too implicit to be used for evaluating the probability of a sample.\n",
    "However, we have already introduced a bit of structure by requiring random variables to be registered with the `sample` function.\n",
    "It turns out that if we add a tiny little bit more structure, we can do probability evaluation too,\n",
    "without changing our programs too much.\n",
    "\n",
    "The first thing we need to do is to combine the ability to *sample* a value for a RV with the ability to evaluate its *local probability*.\n",
    "Consider how we sampled the value of our coin in the prevous part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'heads': True}\n"
     ]
    }
   ],
   "source": [
    "def flip_coin(p=0.5):\n",
    "    sample(\"heads\", random.random() < p)\n",
    "\n",
    "clear_rvs()\n",
    "flip_coin()\n",
    "print(rv_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that for the RV \"heads\" we know exactly from which distribution we sample it.\n",
    "In this case, it's taken from a Bernoulli distribution.\n",
    "Since we know how we sample \"heads\", we also know the probability of each possible value of \"heads\":\n",
    "for `True` it's $p$ and for `False` it's $1-p$.\n",
    "\n",
    "Let's make this connection a bit more formal.\n",
    "We have an object (a probability distribution) that can do two things: provide a sample from it and evaluate the probability of a given sample.\n",
    "In python, we can express this idea using a class with two methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# abstract base class for distributions\n",
    "class Distribution:\n",
    "    \"Return a sample from the distribution.\"\n",
    "    def sample():\n",
    "        pass\n",
    "    \n",
    "    \"Return the log probability of a given sample from the distribution.\"\n",
    "    def log_p(value):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the second method returns the log probability instead of the probability.\n",
    "The reason for this is that probabilities sometimes get very small and can become 0 due to representation errors.\n",
    "This doesn't happen so easily in log space,\n",
    "and it's trivial to obtain the log probability when the normal probability is known (and vice versa).\n",
    "\n",
    "We can now implement this class for the Bernoulli distribution that formalizes our coin flip."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "p(True)  = 0.4\n",
      "p(False) = 0.6\n"
     ]
    }
   ],
   "source": [
    "class Bernoulli(Distribution):\n",
    "    def __init__(self, p):\n",
    "        self.p = p\n",
    "    \n",
    "    def sample(self):\n",
    "        return random.random() < self.p\n",
    "    \n",
    "    def log_p(self, heads):\n",
    "        return np.log(self.p if heads else 1-self.p)\n",
    "\n",
    "coin = Bernoulli(0.4)\n",
    "print(coin.sample())\n",
    "print(\"p(True)  =\", np.exp(coin.log_p(True)))\n",
    "print(\"p(False) =\", np.exp(coin.log_p(False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a way to express primitive distributions that can be used for both sampling and evaluating the log probability, we require our probabilistic programs to use them.\n",
    "Let's change the definition of `sample` to take a distribution object and `.sample()` from it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(name, dist):\n",
    "    value = dist.sample()\n",
    "    rv_dict[name] = value\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, we also have to change our program to make use of the `Bernoulli` distribution and the new way `sample` works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'heads': False}\n"
     ]
    }
   ],
   "source": [
    "def flip_coin(p):\n",
    "    sample(\"heads\", Bernoulli(p))\n",
    "\n",
    "clear_rvs()\n",
    "flip_coin(0.4)\n",
    "print(rv_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Works like a charm.\n",
    "\n",
    "We still can't make use of the `log_p` method to evaluate the probability of a value for \"heads\".\n",
    "However, since we require using `sample` to track random variables, we already have access to the place where RVs and distributions meet.\n",
    "We could in principle write a different `sample` function that doesn't actually sample and record a fresh value for some RV, but instead looks up the value from a table that we provide beforehand and records it's log probability under the distribution that is provided!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "known_values = {}\n",
    "recorded_probs = {}\n",
    "\n",
    "def clear_recorded_probs():\n",
    "    global recorded_probs\n",
    "    recorded_probs = {}\n",
    "\n",
    "# our reimplementation of sample that evaluates probabilities\n",
    "def sample(name, dist):\n",
    "    value = known_values[name]\n",
    "    recorded_probs[name] = dist.log_p(value)\n",
    "    return value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set `known_values` to contain values for all RVs and run our model again, we should now be able to obtain the local probabilities for each RV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(heads = True)  = 0.4\n",
      "p(heads = False) = 0.6\n"
     ]
    }
   ],
   "source": [
    "# run the model for 'heads' = True\n",
    "known_values = {'heads': True}\n",
    "reset_recorded_probs()\n",
    "flip_coin(0.4)\n",
    "print(\"p(heads = True)  =\", np.exp(recorded_probs['heads']))\n",
    "\n",
    "# run the model for 'heads' = False\n",
    "known_values = {'heads': False}\n",
    "clear_recorded_probs()\n",
    "flip_coin(0.4)\n",
    "print(\"p(heads = False) =\", np.exp(recorded_probs['heads']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's still a bit inconvenient that we have to redefine `sample` depending on whether we want to sample or to evaluate.\n",
    "Ideally, we should be able to abstract over the interpretation of `sample`.\n",
    "For example, we can make it look it's functionality up in a variable that we can change to \"sample\" or \"evaluate\" or some other actual interpretation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_function = None\n",
    "\n",
    "def sample(name, dist):\n",
    "    return sample_function(name, dist)\n",
    "\n",
    "# implementation of \"sample\" that samples and records a value (standard interpretation)\n",
    "def sample_sample(name, dist):\n",
    "    value = dist.sample()\n",
    "    rv_dict[name] = value\n",
    "    return value\n",
    "\n",
    "# implementation of \"sample\" that returns the already know value for a RV and records its local probability\n",
    "def sample_eval(name, dist):\n",
    "    value = known_values[name]\n",
    "    recorded_probs[name] = dist.log_p(value)\n",
    "    return value\n",
    "\n",
    "# we set sample_function to be sample_sample by default\n",
    "sample_function = sample_sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can freely change the interpretation `sample` to draw samples or evaluate probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'heads': False}\n",
      "{'heads': -0.916290731874155}\n"
     ]
    }
   ],
   "source": [
    "# draw a sample\n",
    "clear_rvs()\n",
    "\n",
    "sample_function = sample_sample\n",
    "flip_coin(0.4)\n",
    "\n",
    "print(rv_dict)\n",
    "\n",
    "# evaluate a sample\n",
    "known_values = {'heads': True}\n",
    "clear_recorded_probs()\n",
    "\n",
    "sample_function = sample_eval\n",
    "flip_coin(0.4)\n",
    "\n",
    "print(recorded_probs) #print the log probs for all RVs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try the same for a more complex model.\n",
    "We flip two coins, but the probability of the second depends on the outcome of the first.\n",
    "Fortunately, `sample_eval` returns the observed value for the first coin, the when running the model with observed values, it will choose the correct probability parameters for the second coin, just as if the value of the first coin would have been sampled randomly!\n",
    "\n",
    "We can try this out by first drawing a sample and then evaluating the probability of that sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coin1': True, 'coin2': True}\n",
      "{'coin1': 0.5, 'coin2': 0.8}\n"
     ]
    }
   ],
   "source": [
    "# our shiny two-coin model\n",
    "def flip_two_coins():\n",
    "    coin1 = sample('coin1', Bernoulli(0.5))\n",
    "    coin2 = sample('coin2', Bernoulli(0.8 if coin1 else 0.2))\n",
    "\n",
    "# draw a sample\n",
    "clear_rvs()\n",
    "sample_function = sample_sample\n",
    "flip_two_coins()\n",
    "print(rv_dict)\n",
    "\n",
    "# evaluate the sample\n",
    "clear_recorded_probs()\n",
    "known_values = rv_dict # take the sample from the previous run\n",
    "sample_function = sample_eval\n",
    "flip_two_coins()\n",
    "print({name: np.exp(prob) for (name,prob) in recorded_probs.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note how the probability of the second coin is evaluated correctly wrt. the first coin!\n",
    "That's because when the model is run again, it picks the parameters for the second Bernoulli distribution according to the observed value of the first coin.\n",
    "The second `sample` call therefore is given the Bernoulli distribution with the correct parameters and can evaluate the probability of the second coin's value correctly.\n",
    "\n",
    "Congratulations, you have now implemented a probabilistic programming language!\n",
    "We already have everything we need to sample from and evaluate samples under joint distributions that are defined as arbitrary Python programs.\n",
    "As long as the actual random decisions are made using `sample` and from a `Distribution`, the remainder of the model can be arbitrary Python code.\n",
    "In fact, `sample` and `Distribution`s are the only \"language\" constructs we needed to add to ordinary Python for writing models.\n",
    "All the inference stuff can now be done outside of the models in a generic, model-independent way.\n",
    "\n",
    "\"But wait,\" you might object \"we only computed the 'local probability' of each RV, not the joint probability of the whole assignment! And what is that 'local probability' even supposed to mean?\"\n",
    "It turns out that the local probablities are all we need to compute the joint probability of the assignment.\n",
    "All we have to do is to multiply them (or rather sum the log probabilities and take the exponential)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p(coin1 = True , coin2 = True ) =  0.4\n"
     ]
    }
   ],
   "source": [
    "def joint_log_p(rv_log_probs):\n",
    "    return sum(rv_log_probs.values())\n",
    "\n",
    "prob = np.exp(joint_log_p(recorded_probs))\n",
    "print(\"p(coin1 =\", rv_dict['coin1'], \", coin2 =\", rv_dict['coin2'], \") = \", prob) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why does this work?\n",
    "\n",
    "Remember that any joint probability distribution can be turned into a generative process by factorizing it into a sequence of conditional distributions.\n",
    "For example $p(A,B,C)$ can be factorized into $p(A) p(B \\mid A) p(C \\mid A, B)$,\n",
    "which corresponds to a process that first samples $A$ then, $B$ given $A$, and finally $C$ given $A$ and $B$.\n",
    "\n",
    "A probabilistic program is exactly such a factorization, corresponding to the order in which the RVs are sampled in the program.\n",
    "For example, the model `flip_two_coins` first samples `coin1` from $p(\\text{coin1})$ and then `coin2` from $p(\\text{coin2} \\mid \\text{coin1})$,\n",
    "and we know that $p(\\text{coin1}) p(\\text{coin2} \\mid \\text{coin1}) = p(\\text{coin1}, \\text{coin2})$.\n",
    "Now we can see that the \"local probability\" of a RV actually is the *conditional probability* of that RV given all the RVs that were sampled before!\n",
    "\n",
    "Note that a variable after another variable does not need to depend on that earlier variable.\n",
    "For example, we might sample $A$, $B$, and $C$ in that order, but maybe $B$ is sampled from a distribution that is fixed and independent of $A$, so the distribution factorizes into $p(A) p(B) p(C \\mid A,B)$.\n",
    "However, if $A$ and $B$ are independent, then it holds that $p(B \\mid A) = p(B)$,\n",
    "so $p(A) p(B) p(C \\mid A,B) = p(A) p(B \\mid A) p(C \\mid A,B)$!\n",
    "\n",
    "Finally, the order need not be defined beforehand. Consider, for example the following model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': False, 'C': False, 'B': False}\n"
     ]
    }
   ],
   "source": [
    "def weird_model():\n",
    "    a = sample(\"A\", Bernoulli(0.5))\n",
    "    if a:\n",
    "        b = sample(\"B\", Bernoulli(0.3))\n",
    "        c = sample(\"C\", Bernoulli(0.2 if b else 0.6))\n",
    "    else:\n",
    "        c = sample(\"C\", Bernoulli(0.4))\n",
    "        b = sample(\"B\", Bernoulli(0.1 if c else 0.2))\n",
    "\n",
    "clear_rvs()\n",
    "sample_function = sample_sample\n",
    "weird_model()\n",
    "print(rv_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this model, the factorization depends on $A$.\n",
    "If $A$ is `True`, then $B$ is sampled first and $C$ depends on $B$\n",
    "$$ p(A = \\text{True},B,C) = p(A = \\text{True}) p(B \\mid A = \\text{True}) p(C \\mid A = \\text{True}, B) $$\n",
    "\n",
    "If $A$ is `False`, it's the other way round.\n",
    "$$ p(A = \\text{False},B,C) = p(A = \\text{False}) p(C \\mid A = \\text{False}) p(B \\mid C, A = \\text{False}) $$\n",
    "\n",
    "Since we only want to evaluate complete assignments, it's sufficient to know that the \"local\" probabilities are *a* factorization of the joint distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercises\n",
    "\n",
    "tbd."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Variational Inference\n",
    "\n",
    "- we want to know $p(Z \\mid X=x)$ \n",
    "  - can't be derived analytically\n",
    "  - can't be sampled from easily (we can, but it's expensive)\n",
    "- approximation\n",
    "  - we choose a variational familiy (\"guide\" in pyro) $q_\\theta(Z)$\n",
    "  - we try to optimize $\\theta$ to minimize $KL\\left(q(Z) \\parallel p(Z \\mid X=x)\\right)$\n",
    "  - we can't compute the KL directly, but we can optimize it via the \"ELBO\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Optimization via PyTorch\n",
    "\n",
    "- tensors\n",
    "- autodiff\n",
    "- backprop\n",
    "- optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 0.5 * x**2 + 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.5000)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = f(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.requires_grad = True\n",
    "y = f(x)\n",
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gradient =  tensor(1.)\n",
      "x =  tensor(0.9000)\n",
      "gradient =  tensor(0.9000)\n",
      "x =  tensor(0.8100)\n",
      "gradient =  tensor(0.8100)\n",
      "x =  tensor(0.7290)\n",
      "gradient =  tensor(0.7290)\n",
      "x =  tensor(0.6561)\n",
      "gradient =  tensor(0.6561)\n",
      "x =  tensor(0.5905)\n",
      "gradient =  tensor(0.5905)\n",
      "x =  tensor(0.5314)\n",
      "gradient =  tensor(0.5314)\n",
      "x =  tensor(0.4783)\n",
      "gradient =  tensor(0.4783)\n",
      "x =  tensor(0.4305)\n",
      "gradient =  tensor(0.4305)\n",
      "x =  tensor(0.3874)\n",
      "gradient =  tensor(0.3874)\n",
      "x =  tensor(0.3487)\n",
      "gradient =  tensor(0.3487)\n",
      "x =  tensor(0.3138)\n",
      "gradient =  tensor(0.3138)\n",
      "x =  tensor(0.2824)\n",
      "gradient =  tensor(0.2824)\n",
      "x =  tensor(0.2542)\n",
      "gradient =  tensor(0.2542)\n",
      "x =  tensor(0.2288)\n",
      "gradient =  tensor(0.2288)\n",
      "x =  tensor(0.2059)\n",
      "gradient =  tensor(0.2059)\n",
      "x =  tensor(0.1853)\n",
      "gradient =  tensor(0.1853)\n",
      "x =  tensor(0.1668)\n",
      "gradient =  tensor(0.1668)\n",
      "x =  tensor(0.1501)\n",
      "gradient =  tensor(0.1501)\n",
      "x =  tensor(0.1351)\n",
      "gradient =  tensor(0.1351)\n",
      "x =  tensor(0.1216)\n",
      "gradient =  tensor(0.1216)\n",
      "x =  tensor(0.1094)\n",
      "gradient =  tensor(0.1094)\n",
      "x =  tensor(0.0985)\n",
      "gradient =  tensor(0.0985)\n",
      "x =  tensor(0.0886)\n",
      "gradient =  tensor(0.0886)\n",
      "x =  tensor(0.0798)\n",
      "gradient =  tensor(0.0798)\n",
      "x =  tensor(0.0718)\n",
      "gradient =  tensor(0.0718)\n",
      "x =  tensor(0.0646)\n",
      "gradient =  tensor(0.0646)\n",
      "x =  tensor(0.0581)\n",
      "gradient =  tensor(0.0581)\n",
      "x =  tensor(0.0523)\n",
      "gradient =  tensor(0.0523)\n",
      "x =  tensor(0.0471)\n",
      "gradient =  tensor(0.0471)\n",
      "x =  tensor(0.0424)\n",
      "gradient =  tensor(0.0424)\n",
      "x =  tensor(0.0382)\n",
      "gradient =  tensor(0.0382)\n",
      "x =  tensor(0.0343)\n",
      "gradient =  tensor(0.0343)\n",
      "x =  tensor(0.0309)\n",
      "gradient =  tensor(0.0309)\n",
      "x =  tensor(0.0278)\n",
      "gradient =  tensor(0.0278)\n",
      "x =  tensor(0.0250)\n",
      "gradient =  tensor(0.0250)\n",
      "x =  tensor(0.0225)\n",
      "gradient =  tensor(0.0225)\n",
      "x =  tensor(0.0203)\n",
      "gradient =  tensor(0.0203)\n",
      "x =  tensor(0.0182)\n",
      "gradient =  tensor(0.0182)\n",
      "x =  tensor(0.0164)\n",
      "gradient =  tensor(0.0164)\n",
      "x =  tensor(0.0148)\n",
      "gradient =  tensor(0.0148)\n",
      "x =  tensor(0.0133)\n",
      "gradient =  tensor(0.0133)\n",
      "x =  tensor(0.0120)\n",
      "gradient =  tensor(0.0120)\n",
      "x =  tensor(0.0108)\n",
      "gradient =  tensor(0.0108)\n",
      "x =  tensor(0.0097)\n",
      "gradient =  tensor(0.0097)\n",
      "x =  tensor(0.0087)\n",
      "gradient =  tensor(0.0087)\n",
      "x =  tensor(0.0079)\n",
      "gradient =  tensor(0.0079)\n",
      "x =  tensor(0.0071)\n",
      "gradient =  tensor(0.0071)\n",
      "x =  tensor(0.0064)\n",
      "gradient =  tensor(0.0064)\n",
      "x =  tensor(0.0057)\n",
      "gradient =  tensor(0.0057)\n",
      "x =  tensor(0.0052)\n",
      "gradient =  tensor(0.0052)\n",
      "x =  tensor(0.0046)\n",
      "gradient =  tensor(0.0046)\n",
      "x =  tensor(0.0042)\n",
      "gradient =  tensor(0.0042)\n",
      "x =  tensor(0.0038)\n",
      "gradient =  tensor(0.0038)\n",
      "x =  tensor(0.0034)\n",
      "gradient =  tensor(0.0034)\n",
      "x =  tensor(0.0030)\n",
      "gradient =  tensor(0.0030)\n",
      "x =  tensor(0.0027)\n",
      "gradient =  tensor(0.0027)\n",
      "x =  tensor(0.0025)\n",
      "gradient =  tensor(0.0025)\n",
      "x =  tensor(0.0022)\n",
      "gradient =  tensor(0.0022)\n",
      "x =  tensor(0.0020)\n",
      "gradient =  tensor(0.0020)\n",
      "x =  tensor(0.0018)\n",
      "gradient =  tensor(0.0018)\n",
      "x =  tensor(0.0016)\n",
      "gradient =  tensor(0.0016)\n",
      "x =  tensor(0.0015)\n",
      "gradient =  tensor(0.0015)\n",
      "x =  tensor(0.0013)\n",
      "gradient =  tensor(0.0013)\n",
      "x =  tensor(0.0012)\n",
      "gradient =  tensor(0.0012)\n",
      "x =  tensor(0.0011)\n",
      "gradient =  tensor(0.0011)\n",
      "x =  tensor(0.0010)\n",
      "gradient =  tensor(0.0010)\n",
      "x =  tensor(0.0009)\n",
      "gradient =  tensor(0.0009)\n",
      "x =  tensor(0.0008)\n",
      "gradient =  tensor(0.0008)\n",
      "x =  tensor(0.0007)\n",
      "gradient =  tensor(0.0007)\n",
      "x =  tensor(0.0006)\n",
      "gradient =  tensor(0.0006)\n",
      "x =  tensor(0.0006)\n",
      "gradient =  tensor(0.0006)\n",
      "x =  tensor(0.0005)\n",
      "gradient =  tensor(0.0005)\n",
      "x =  tensor(0.0005)\n",
      "gradient =  tensor(0.0005)\n",
      "x =  tensor(0.0004)\n",
      "gradient =  tensor(0.0004)\n",
      "x =  tensor(0.0004)\n",
      "gradient =  tensor(0.0004)\n",
      "x =  tensor(0.0003)\n",
      "gradient =  tensor(0.0003)\n",
      "x =  tensor(0.0003)\n",
      "gradient =  tensor(0.0003)\n",
      "x =  tensor(0.0003)\n",
      "gradient =  tensor(0.0003)\n",
      "x =  tensor(0.0002)\n",
      "gradient =  tensor(0.0002)\n",
      "x =  tensor(0.0002)\n",
      "gradient =  tensor(0.0002)\n",
      "x =  tensor(0.0002)\n",
      "gradient =  tensor(0.0002)\n",
      "x =  tensor(0.0002)\n",
      "gradient =  tensor(0.0002)\n",
      "x =  tensor(0.0002)\n",
      "gradient =  tensor(0.0002)\n",
      "x =  tensor(0.0001)\n",
      "gradient =  tensor(0.0001)\n",
      "x =  tensor(0.0001)\n",
      "gradient =  tensor(0.0001)\n",
      "x =  tensor(0.0001)\n",
      "gradient =  tensor(0.0001)\n",
      "x =  tensor(0.0001)\n",
      "gradient =  tensor(0.0001)\n",
      "x =  tensor(9.4046e-05)\n",
      "gradient =  tensor(9.4046e-05)\n",
      "x =  tensor(8.4642e-05)\n",
      "gradient =  tensor(8.4642e-05)\n",
      "x =  tensor(7.6177e-05)\n",
      "gradient =  tensor(7.6177e-05)\n",
      "x =  tensor(6.8560e-05)\n",
      "gradient =  tensor(6.8560e-05)\n",
      "x =  tensor(6.1704e-05)\n",
      "gradient =  tensor(6.1704e-05)\n",
      "x =  tensor(5.5533e-05)\n",
      "gradient =  tensor(5.5533e-05)\n",
      "x =  tensor(4.9980e-05)\n",
      "gradient =  tensor(4.9980e-05)\n",
      "x =  tensor(4.4982e-05)\n",
      "gradient =  tensor(4.4982e-05)\n",
      "x =  tensor(4.0484e-05)\n",
      "gradient =  tensor(4.0484e-05)\n",
      "x =  tensor(3.6435e-05)\n",
      "gradient =  tensor(3.6435e-05)\n",
      "x =  tensor(3.2792e-05)\n",
      "gradient =  tensor(3.2792e-05)\n",
      "x =  tensor(2.9513e-05)\n",
      "gradient =  tensor(2.9513e-05)\n",
      "x =  tensor(2.6561e-05)\n"
     ]
    }
   ],
   "source": [
    "# gradient descent\n",
    "lr = 0.1\n",
    "x = torch.tensor(1., requires_grad=True)\n",
    "for i in range(100):\n",
    "    y = f(x)\n",
    "    y.backward()\n",
    "    print('gradient = ', x.grad)\n",
    "    x.data -= lr * x.grad\n",
    "    print('x = ', x.data)\n",
    "    x.grad.data.zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/chfin/dateien/dev/python/harmony-model/env/lib/python3.8/site-packages/torch/tensor.py:746: UserWarning: The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad attribute won't be populated during autograd.backward(). If you indeed want the gradient for a non-leaf Tensor, use .retain_grad() on the non-leaf Tensor. If you access the non-leaf Tensor by mistake, make sure you access the leaf Tensor instead. See github.com/pytorch/pytorch/pull/30531 for more informations.\n",
      "  warnings.warn(\"The .grad attribute of a Tensor that is not a leaf Tensor is being accessed. Its .grad \"\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Lighting the Whole Thing Up\n",
    "\n",
    "- torch + \"sample\" + distributions = pyro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "harmony-model",
   "language": "python",
   "name": "harmony-model"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
