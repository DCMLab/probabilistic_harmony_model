{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Cognitive Model or Harmonic Types - Clustering Experiment\n",
    "\n",
    "The experiments in this notebook take a while to compute, so it is recommended to run them on a machine with GPU support (e.g. on Google Colab)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xlRQd_23WdPq"
   },
   "outputs": [],
   "source": [
    "# if on google colab, uncomment and run the following line to install dependencies\n",
    "# !pip install pyro-ppl pitchtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9TQyrKApfYfo"
   },
   "outputs": [],
   "source": [
    "# if on colab, run the following lines to mount your google drive\n",
    "# from google.colab import drive\n",
    "# drive._mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MkG0irQ2gy9i"
   },
   "outputs": [],
   "source": [
    "# set the data path to the directory with the input data\n",
    "data_path = \"/path/to/data\"\n",
    "\n",
    "# use a path like this for google colab/drive\n",
    "# data_path = \"/content/drive/Shareddrives/DCML/Projects/ERC/harmony-ornamentation/clustering\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uh1Hgg6E6MM2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "from pyro.distributions import *\n",
    "#from collections import Counter\n",
    "import pyro.infer\n",
    "import pyro.optim\n",
    "import pyro.util\n",
    "pyro.enable_validation(True)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "from IPython import display\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import scipy.special as special\n",
    "import math\n",
    "\n",
    "import os.path as path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import utils\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmkL2zHp6MNH"
   },
   "outputs": [],
   "source": [
    "cpu = torch.cuda.is_available()\n",
    "\n",
    "# TODO: set the GPU you want to use\n",
    "gpu_n = 0\n",
    "\n",
    "device = torch.device(f'cuda:{gpu_n}' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMWmdkVhHGcR"
   },
   "outputs": [],
   "source": [
    "def debug_tensors():\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj) and obj.device == torch.device(\"cpu\"):\n",
    "            print(f\"{obj.device}, {obj.dtype}, {obj.shape}\")\n",
    "\n",
    "debug_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_2bYyV56MNJ"
   },
   "outputs": [],
   "source": [
    "def save_rng_state(name):\n",
    "    fn = name + '-' + datetime.today().isoformat() + '.state'\n",
    "    state = pyro.util.get_rng_state()\n",
    "    with open('rng-' + fn, 'w') as f:\n",
    "        print(state, file=f)\n",
    "    torch.save(state['torch'], 'torch-' + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrhA9BQz6MNK"
   },
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# set random seeds\n",
    "pyro.set_rng_seed(0)\n",
    "#torch.set_deterministic(True)\n",
    "#torch.set_num_threads(1)\n",
    "#torch.set_num_interop_threads(1)\n",
    "\n",
    "# fix the range of pitches we consider\n",
    "fifth_range = 2*7                  # 2 diatonics\n",
    "npcs = 2*fifth_range+1             # around C: Cbb to C## on LoF\n",
    "utils.set_fifth_range(fifth_range) # used to make helper functions work correctly\n",
    "nclusters = 14                      # Try clustering down to how many clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_0Ihvk26MNL"
   },
   "source": [
    "# 1. Experimental Setup\n",
    "\n",
    "## Model\n",
    "\n",
    "The model is identical to the base experiment except that several chord types can share a single parameter for the ornamentation prevalence. The sharing is controlled through the parameter `cluster_assignment`.\n",
    "\n",
    "Additionally, a custom prefix can be used for the random varibles in the trace.\n",
    "This allows us to run the same model function several times in a combined meta model.\n",
    "We use this to compute the bayes factor between model variants with different chord type clustering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kuiOGLz6MNO"
   },
   "outputs": [],
   "source": [
    "def chord_model(npcs, nharmonies, cluster_assignment, data, pfx=None, subsamples=500, pobserve=0.5, **kwargs):\n",
    "    if pfx is None or pfx == \"\":\n",
    "        pfx = \"\"\n",
    "    else:\n",
    "        pfx += \"_\"\n",
    "    nclusters = max(cluster_assignment) + 1\n",
    "    # parameters priors:\n",
    "    # distribution of the harmonies\n",
    "    p_harmony = pyro.sample(f'{pfx}p_harmony', Dirichlet(0.5 * torch.ones(nharmonies, device=device)))\n",
    "    # each cluster \n",
    "    with pyro.plate(f'{pfx}clusters', nclusters) as ind:\n",
    "        # distribution of ornament probability\n",
    "        p_is_chordtone = pyro.sample(f'{pfx}p_is_chordtone', Beta(torch.tensor(1., device=device),torch.tensor(1., device=device)))\n",
    "    # distribution of notes in the harmonies\n",
    "    with pyro.plate(f'{pfx}harmonies', nharmonies) as ind:\n",
    "        #cluster = pyro.sample('cluster', Categorical(torch.ones(nclusters)))\n",
    "        cluster = cluster_assignment[ind]\n",
    "        p_is_chordtone_c = p_is_chordtone[cluster]\n",
    "        #print(p_is_chordtone.shape)\n",
    "        # distribution of notes per note type\n",
    "        p_chordtones = pyro.sample(f'{pfx}p_chordtones', Dirichlet(0.5 * torch.ones(npcs, device=device)))\n",
    "        p_ornaments  = pyro.sample(f'{pfx}p_ornaments', Dirichlet(0.5 * torch.ones(npcs, device=device)))\n",
    "        #print(p_chordtones.shape)\n",
    "        # we build a big categorical out of the chordtones and ornaments,\n",
    "        # including notes of unknown type (marginalizing over the categories)\n",
    "        #p_ct = p_is_chordtone_c       * p_chordtones\n",
    "        #p_or = (1 - p_is_chordtone_c) * p_ornaments\n",
    "        p_ct = torch.mm(torch.diag(p_is_chordtone_c), p_chordtones)\n",
    "        p_or = torch.mm(torch.diag(1 - p_is_chordtone_c), p_ornaments)\n",
    "        p_unobserved = p_ct + p_or\n",
    "        p_tones = torch.cat([pobserve * p_ct, pobserve * p_or, (1-pobserve) * p_unobserved], dim=1)\n",
    "    # distribution of note rate in chords\n",
    "    rate_notes = pyro.sample(f'{pfx}rate_notes', Gamma(torch.tensor(3., device=device),torch.tensor(1., device=device)))\n",
    "    \n",
    "    # sampling the data:\n",
    "    nchords = len(data['c'])\n",
    "    subs = min(nchords,subsamples) if subsamples != None else None\n",
    "    with pyro.plate(f'{pfx}data', nchords, subsample_size=subs) as ind:\n",
    "        # pick a harmony\n",
    "        c = pyro.sample(f'{pfx}c', Categorical(p_harmony), obs=data['c'][ind])\n",
    "        # pick a number of notes\n",
    "        nnotes = 1 + pyro.sample(f'{pfx}n', Poisson(rate_notes), obs=data['n'][ind]).int()\n",
    "        # sample chordtones\n",
    "        # Normally we would sample nnotes notes for each chord, but that doesn't work vectorized.\n",
    "        # However, evaluating the probability ignores n, so we can just provide 1 here.\n",
    "        notes = pyro.sample(f'{pfx}chord', Multinomial(1, p_tones[c], validate_args=False), obs=data['notes'][ind])\n",
    "        chords = {'c': c,\n",
    "                  'n': nnotes,\n",
    "                  'counts': notes.reshape(-1,npcs)}\n",
    "    return chords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0t-yDIwZ6MNO"
   },
   "source": [
    "## Guide\n",
    "\n",
    "A simple guide that assumes the latent variables to be distributed independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbmjeUf66MNP"
   },
   "outputs": [],
   "source": [
    "def chord_guide(npcs, nharmonies, cluster_assignment, data, pfx=None, subsamples=500, pobserve=0.5, init=dict(), optimize=True):\n",
    "    if pfx is None or pfx == \"\":\n",
    "        pfx = \"\"\n",
    "    else:\n",
    "        pfx += \"_\"\n",
    "    nclusters = max(cluster_assignment) + 1\n",
    "    \n",
    "    # posterior of p_harmony\n",
    "    params_p_harmony = init['harmonies'].to(device) if 'harmonies' in init else 0.5 * torch.ones(nharmonies, device=device)\n",
    "    if optimize:\n",
    "        params_p_harmony = pyro.param(f'{pfx}params_p_harmony',\n",
    "                                      params_p_harmony,\n",
    "                                      constraint=constraints.positive)\n",
    "    pyro.sample(f'{pfx}p_harmony', Dirichlet(params_p_harmony))\n",
    "    \n",
    "    # posteriors of notes dists in harmonies (parameters)\n",
    "    params_p_chordtones = init['chordtones'].to(device) if  'chordtones' in init else 0.5 * torch.ones(nharmonies, npcs, device=device)\n",
    "    params_p_ornaments = init['ornaments'].to(device) if 'ornaments' in init else 0.5 * torch.ones(nharmonies, npcs, device=device)\n",
    "    if optimize:\n",
    "        params_p_chordtones = pyro.param(f'{pfx}params_p_chordtones',\n",
    "                                         params_p_chordtones,\n",
    "                                         constraint=constraints.positive)\n",
    "        params_p_ornaments = pyro.param(f'{pfx}params_p_ornaments',\n",
    "                                        params_p_ornaments,\n",
    "                                        constraint=constraints.positive)\n",
    "    \n",
    "    # posterior of ornament probability (parameters)\n",
    "    alpha_p_ict = init['is_ct'].to(device) if 'is_ct' in init else torch.ones(nclusters, device=device)\n",
    "    beta_p_ict = init['is_or'].to(device) if 'is_or' in init else torch.ones(nclusters, device=device)\n",
    "    if optimize:\n",
    "        alpha_p_ict = pyro.param(f'{pfx}alpha_p_ict',\n",
    "                                 alpha_p_ict,\n",
    "                                 constraint=constraints.positive)\n",
    "        beta_p_ict = pyro.param(f'{pfx}beta_p_ict',\n",
    "                                beta_p_ict,\n",
    "                                constraint=constraints.positive)\n",
    "    \n",
    "    with pyro.plate(f'{pfx}clusters', nclusters) as ind:\n",
    "        pyro.sample(f'{pfx}p_is_chordtone', Beta(alpha_p_ict, beta_p_ict))\n",
    "        \n",
    "    # posteriors of ornament probability and note distributions\n",
    "    with pyro.plate(f'{pfx}harmonies', nharmonies) as ind:\n",
    "        pyro.sample(f'{pfx}p_chordtones', Dirichlet(params_p_chordtones[ind]))\n",
    "        pyro.sample(f'{pfx}p_ornaments', Dirichlet(params_p_ornaments[ind]))\n",
    "        \n",
    "    #posterior of note rate\n",
    "    alpha_rate_notes = init['sum_chords'].to(device) if 'sum_chords' in init else torch.tensor(3., device=device)\n",
    "    beta_rate_notes = init['n_chords'].to(device) if 'n_chords' in init else torch.tensor(1., device=device)\n",
    "    if optimize:\n",
    "        alpha_rate_notes = pyro.param(f'{pfx}alpha_rate_notes',\n",
    "                                      alpha_rate_notes,\n",
    "                                      constraint=constraints.positive)\n",
    "        beta_rate_notes = pyro.param(f'{pfx}beta_rate_notes',\n",
    "                                     beta_rate_notes,\n",
    "                                     constraint=constraints.positive)\n",
    "    rate_notes = pyro.sample(f'{pfx}rate_notes', Gamma(alpha_rate_notes, beta_rate_notes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFNJAYta6MNQ"
   },
   "source": [
    "## Data and Conditioning\n",
    "\n",
    "### Data Format\n",
    "\n",
    "Same as in the base experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOUEowXA6MNR"
   },
   "outputs": [],
   "source": [
    "def chord_tensor(notes):\n",
    "    \"\"\"Takes a list of notes as (fifth, type) pairs and returns a vector of counts.\"\"\"\n",
    "    notetype = {'chordtone': 0, 'ornament': 1, 'unknown': 2}\n",
    "    chord = torch.zeros((3, npcs), device=device)\n",
    "    for (fifth, t) in notes:\n",
    "      try:\n",
    "        chord[notetype[t], utils.fifth_to_index(fifth)] += 1\n",
    "      except Exception as e:\n",
    "        print(fifth, utils.fifth_to_index(fifth))\n",
    "        raise e\n",
    "    return chord\n",
    "\n",
    "def annot_data_obs(chords):\n",
    "    \"\"\"Helper function to turn a list of chord dictionary into a dictionary of observation vectors.\"\"\"\n",
    "    obs = {}\n",
    "    obs[\"notes\"] = torch.cat([chord_tensor(c['notes']).reshape((1,-1)) for c in chords], dim=0)\n",
    "    obs[\"c\"] = torch.tensor([c['label'] for c in chords], device=device)\n",
    "    obs[\"n\"] = torch.tensor([len(c['notes']) - 1. for c in chords], device=device)\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNqDL5HD6MNR"
   },
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "The data is loaded from a TSV file that.\n",
    "The resulting dataframe is converted to the observation format that we pass to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2H2_wNx6MNT"
   },
   "outputs": [],
   "source": [
    "chordtypes_common = np.array(\n",
    "    [\"major\", \"minor\", \"dominant-7th\", \"diminished\",\n",
    "     \"full-diminished\", \"minor-7th\", \"half-diminished\", \"major-7th\",\n",
    "     \"augmented\"],\n",
    "    dtype=\"object\")\n",
    "dcml_chordtype_map = {\n",
    "    \"M\": \"major\",\n",
    "    \"m\": \"minor\",\n",
    "    \"Mm7\": \"dominant-7th\",\n",
    "    \"o\": \"diminished\",\n",
    "    \"o7\": \"full-diminished\",\n",
    "    \"mm7\": \"minor-7th\",\n",
    "    \"%7\": \"half-diminished\",\n",
    "    \"MM7\": \"major-7th\",\n",
    "    \"+\": \"augmented\",\n",
    "    #\"mM7\": \"minor-major-7th\",\n",
    "    #\"+7\": \"augmented-7th\",\n",
    "}\n",
    "wiki_chordtype_map = {\n",
    "    \"major\": \"major\",\n",
    "    \"minor\": \"minor\",\n",
    "    \"dominant\": \"dominant-7th\",\n",
    "    \"diminished\": \"diminished\",\n",
    "    \"diminished-seventh\": \"full-diminished\",\n",
    "    \"minor-seventh\": \"minor-7th\",\n",
    "    \"half-diminished\": \"half-diminished\",\n",
    "    \"major-seventh\": \"major-7th\",\n",
    "    \"augmented\": \"augmented\",\n",
    "    #\"major-minor\": \"minor-major-7th\",\n",
    "    #\"augmented-seventh\": \"augmented-7th\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZ-19ybr6MNT"
   },
   "outputs": [],
   "source": [
    "def load_dataset(filename, alt_labels):\n",
    "    filename = path.join(data_path, filename)\n",
    "    print(\"loading dataset...\") \n",
    "    df = utils.load_csv(filename)\n",
    "    df = df[df.label.map(lambda l: l in alt_labels)]\n",
    "    sizes = df.groupby(['chordid', 'label']).size()\n",
    "    type_counts = sizes.groupby('label').size().sort_values(ascending=False)\n",
    "    chordtypes = type_counts.index.tolist()\n",
    "    df['numlabel'] = df.label.map(chordtypes.index)\n",
    "    df['alt_label'] = df.label.map(alt_labels)\n",
    "    \n",
    "    # check if precomputed tensor data is available:\n",
    "    prefn = filename + \"_common_precomp.pt\"\n",
    "    if path.exists(prefn) and path.getmtime(prefn) > path.getmtime(filename):\n",
    "        print(\"using precomputed tensor data.\")\n",
    "        obs = torch.load(prefn)\n",
    "        obs[\"notes\"] = obs[\"notes\"].to(device)\n",
    "        obs[\"c\"] = obs[\"c\"].to(device)\n",
    "        obs[\"n\"] = obs[\"n\"].to(device)\n",
    "        print(device)\n",
    "        print([(k, v.device) for k, v in obs.items()])\n",
    "    else:\n",
    "        print('extracting chords...')\n",
    "        chords = [{'label': label, 'notes': list(zip(grp.fifth, grp.type))}\n",
    "                  for (_, label), grp in tqdm.tqdm(df.groupby(['chordid', 'numlabel']))]\n",
    "        print('converting chords to tensors...')\n",
    "        obs = annot_data_obs(chords)\n",
    "        torch.save(obs, prefn)\n",
    "    \n",
    "    print(len(chordtypes), \"chord types\")\n",
    "    print(len(obs[\"c\"]), \"chords\")\n",
    "    return df, obs, [alt_labels[l] for l in chordtypes]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is important to initialize the posterior parameters with good guesses, close to their final values.\n",
    "This function estimates the final values by ignoring unknown note types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NruYLmo-6MNU"
   },
   "outputs": [],
   "source": [
    "\n",
    "        \n",
    "#dist = mean_dist\n",
    "\n",
    "def get_init_params(df, nharms, npcs):\n",
    "    init = dict()\n",
    "    \n",
    "    init['harmonies'] = torch.tensor(df.groupby('numlabel').size().sort_values(ascending=False), device=device) + 0.5\n",
    "\n",
    "    init['chordtones'] = torch.zeros([nharms,npcs], device=device) + 0.5\n",
    "    for (numlabel, fifth), grp in df[df.type=='chordtone'].groupby(['numlabel','fifth']):\n",
    "        init['chordtones'][numlabel, utils.fifth_to_index(fifth)] += grp.fifth.count()\n",
    "\n",
    "    init['ornaments'] = torch.zeros([nharms,npcs], device=device) + 0.5\n",
    "    for (numlabel, fifth), grp in df[df.type=='ornament'].groupby(['numlabel','fifth']):\n",
    "        init['ornaments'][numlabel, utils.fifth_to_index(fifth)] += grp.fifth.count()\n",
    "    \n",
    "    init['is_ct'] = torch.tensor([sum(df[df.numlabel==l].type=='chordtone') for l in range(nharms)], device=device) + 1.\n",
    "    init['is_or'] = torch.tensor([sum(df[df.numlabel==l].type=='ornament') for l in range(nharms)], device=device) + 1.\n",
    "    \n",
    "    chord_sizes = df.groupby('chordid').size()-1\n",
    "    init['sum_chords'] = torch.tensor(sum(chord_sizes) + 3., device=device)\n",
    "    init['n_chords'] = torch.tensor(len(chord_sizes) + 1., device=device)\n",
    "    return init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwZ6GC856MNV"
   },
   "source": [
    "After inferring the parameters we save them for easier inspection and reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nukGRzSb6MNV"
   },
   "outputs": [],
   "source": [
    "def save_params(params, chordtypes, name):\n",
    "    torch.save(params, path.join(data_path, \"params\", name+'.pt'))\n",
    "    with open(path.join(data_path, \"params\", name+'.json'), 'w') as f:\n",
    "        json.dump({'params': {key: val.tolist() for key,val in params.items()},\n",
    "                   'chordtypes': chordtypes},\n",
    "                  f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTE5VdZ46MNW"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Inference of the posterior is done via variational inference, i.e. by optimizing the parameters of the guide.\n",
    "The function `infer_posteriors` takes a dataset of observations,\n",
    "performs the optimization, and returns the optimized parameters together with some of their histories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjGIoHmO6MNW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def infer_posteriors(obs, init, chordtypes, cluster_assignment,\n",
    "                     nsteps=5_000, subsamples=10_000, particles=1,\n",
    "                     plot_loss=True, save_as=None):\n",
    "    # optimize the parameters of the guide\n",
    "    pyro.clear_param_store()\n",
    "    pyro.set_rng_seed(1625) # set every time for independent reproducibility\n",
    "    svi = pyro.infer.SVI(model=chord_model,\n",
    "                         guide=chord_guide,\n",
    "                         optim=pyro.optim.Adam({\"lr\": 0.01, \"betas\": (0.95, 0.999)}),\n",
    "                         #optim=pyro.optim.Adadelta({\"lr\": 1.0, \"rho\": 0.9}),\n",
    "                         #optim=pyro.optim.SGD({\"lr\": 0.00005, \"momentum\": 0.9, \"nesterov\": True}),\n",
    "                         loss=pyro.infer.Trace_ELBO(num_particles=particles))\n",
    "\n",
    "    nharms = len(chordtypes)\n",
    "    \n",
    "    # set up histories for the loss and some of the parameters\n",
    "    losses = np.zeros(nsteps)\n",
    "    param_history = {name:np.zeros(nsteps) for name in ['alpha_rate_notes', 'beta_rate_notes']}#, 'alpha_p_ict', 'beta_p_ict']}\n",
    "    root_history = np.zeros((nsteps,nharms))\n",
    "    harm_history = np.zeros((nsteps,nharms))\n",
    "\n",
    "    # run the optimization\n",
    "    for i in tqdm.trange(nsteps):\n",
    "        # update parameters and record loss\n",
    "        losses[i] = svi.step(npcs, nharms, cluster_assignment, obs, subsamples=subsamples, init=init)\n",
    "        \n",
    "        # record values of some parameters\n",
    "        ps = pyro.get_param_store()\n",
    "        root_history[i] = ps.get_param('params_p_chordtones').cpu().detach()[:,fifth_range]\n",
    "        harm_history[i] = ps.get_param('params_p_harmony').cpu().detach()\n",
    "        for (name, value) in ps.items():\n",
    "            if name in param_history:\n",
    "                param_history[name][i] = value.cpu().item()\n",
    "\n",
    "    # plot the loss\n",
    "    if plot_loss:\n",
    "        plt.figure()\n",
    "        plt.plot(losses)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.show()\n",
    "        print(\"loss variance (last 100 steps):\", losses[-100:].var())\n",
    "    \n",
    "    params = dict((name, value.detach().cpu().detach().numpy()) for name, value in pyro.get_param_store().items())\n",
    "    if save_as != None:\n",
    "        save_params(params, chordtypes, save_as)\n",
    "    \n",
    "    return params, param_history, root_history, harm_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use different distance measures for deciding which clusters to merge. By default, we will use the distance between the corresponding distributions' means (`mean_dist`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clust_dist(X,Y):\n",
    "    return 1 - (special.beta((X[0] + Y[0]) / 2, (X[1] + Y[1]) /2)) / math.sqrt(special.beta(X[0],X[1]) * special.beta(Y[0],Y[1]))\n",
    "\n",
    "def mean_dist(X,Y):\n",
    "    return abs(stats.beta.mean(X[0],X[1]) - stats.beta.mean(Y[0],Y[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The clustering itself is performed iteratively, like in hierarchical clustering.\n",
    "Initially, each chord type is its own cluster.\n",
    "In every iteration, the two clusters with the most similar posteriors of `p_ict` are merged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUDFX4Jn6MNX"
   },
   "outputs": [],
   "source": [
    "def iterative_clustering(init, chordtypes, obs, name, dist=mean_dist, show_plots=True):\n",
    "    nharms = len(chordtypes)\n",
    "\n",
    "    # initial clustering\n",
    "    cluster_assignment = dict((i,i) for i in range(nharms))\n",
    "\n",
    "    # run iterative experiments\n",
    "    outputs = []\n",
    "    for it in range(nharms):\n",
    "        nclusters = nharms - it\n",
    "        print(f\"iteration {it} ({nclusters} clusters).\")\n",
    "        print(cluster_assignment)\n",
    "        \n",
    "        # inference\n",
    "        assignment = torch.tensor([cluster_assignment[i] for i in range(nharms)], device=device)\n",
    "        params, hist, roots, harm = infer_posteriors(obs, init, chordtypes, assignment,\n",
    "                                                     nsteps=350, subsamples=None, particles=1,\n",
    "                                                     save_as=f\"{name}_params{nclusters}\",\n",
    "                                                     plot_loss=show_plots)\n",
    "        # record output\n",
    "        outputs.append(dict({\n",
    "            \"params\": params,\n",
    "            \"hist\": hist,\n",
    "            \"roots\": roots,\n",
    "            \"harm\": harm,\n",
    "            \"cluster_assignment\": cluster_assignment,\n",
    "            \"init\": init,\n",
    "        }))\n",
    "        \n",
    "        \n",
    "        if show_plots:\n",
    "            plot_p_ict(params, chordtypes, cluster_assignment, lower=0.6, upper=0.95)\n",
    "        \n",
    "        # compute next clustering / init\n",
    "        if nclusters > 1:\n",
    "            # find closest clusters\n",
    "            alphas = params[\"alpha_p_ict\"]\n",
    "            betas = params[\"beta_p_ict\"]\n",
    "            dists = dict()\n",
    "            for i in range(nclusters):\n",
    "                for j in range(i+1, nclusters):\n",
    "                    dists[(i,j)] = dist((alphas[i], betas[i]), (alphas[j], betas[j]))\n",
    "            min1, min2 = min(dists, key=dists.get)\n",
    "\n",
    "            # map clusters\n",
    "            remaining = [i for i in range(nclusters) if i not in [min1,min2]]\n",
    "            cluster_mapping = {**{min1: 0, min2: 0}, **dict((c,i+1) for i,c in enumerate(remaining))}\n",
    "            \n",
    "            # update assignment\n",
    "            cluster_assignment = dict((h,cluster_mapping[c]) for h,c in cluster_assignment.items())\n",
    "\n",
    "            # update init\n",
    "            # ... with the posterior parameters from the previous run\n",
    "            init = dict()\n",
    "            init['harmonies'] = torch.tensor(params['params_p_harmony'], device=device)\n",
    "            init['chordtones'] = torch.tensor(params['params_p_chordtones'], device=device)\n",
    "            init['ornaments'] = torch.tensor(params['params_p_ornaments'], device=device)\n",
    "            init['sum_chords'] = torch.tensor(params['alpha_rate_notes'], device=device)\n",
    "            init['n_chords'] = torch.tensor(params['beta_rate_notes'], device=device)\n",
    "            # ... and merged clusters\n",
    "            init['is_ct'] = torch.zeros(nclusters-1, dtype=torch.float64, device=device)\n",
    "            init['is_or'] = torch.zeros(nclusters-1, dtype=torch.float64, device=device)\n",
    "            for i in range(nclusters-1):\n",
    "                init['is_ct'][i] += alphas[cluster_mapping[i]]\n",
    "                init['is_or'][i] += betas[cluster_mapping[i]]\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqnikMsp6MNX"
   },
   "source": [
    "## Bayes Factor Inference\n",
    "\n",
    "To find the best cluster assignment, we perform a Bayesian model comparison between the different assignments found by the iterative clustering algorithm.\n",
    "This is achieved by considering each assignment as a \"sub-model\" in a meta model.\n",
    "The full (meta-)model first picks a sub-model $m$ (= cluster assignment) and generates the data from this model:\n",
    "$$ p(D) = p(m) \\cdot p(D \\mid m). $$\n",
    "The posterior probabilities of this \"model choice\" express the relative plausibility of each cluster assignment.\n",
    "The base factor between two models $m_1$ and $m_2$ is\n",
    "$$ \\dfrac{p(D \\mid m_1)}{p(D \\mid m_2)} = \\dfrac{p(m_1 \\mid D)}{p(m_2 \\mid D)} \\dfrac{m_2}{m_1}. $$\n",
    "However, we use a flat prior over the model choice, so $\\dfrac{m_2}{m_1} = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeF4TdSvWWhf"
   },
   "outputs": [],
   "source": [
    "def post_to_init(params):\n",
    "    init = dict()\n",
    "    init['harmonies'] = torch.tensor(params['params_p_harmony'], device=device)\n",
    "    init['chordtones'] = torch.tensor(params['params_p_chordtones'], device=device)\n",
    "    init['ornaments'] = torch.tensor(params['params_p_ornaments'], device=device)\n",
    "    init['sum_chords'] = torch.tensor(params['alpha_rate_notes'], device=device)\n",
    "    init['n_chords'] = torch.tensor(params['beta_rate_notes'], device=device)\n",
    "    init['is_ct'] = torch.tensor(params['alpha_p_ict'], device=device)\n",
    "    init['is_or'] = torch.tensor(params['beta_p_ict'], device=device)\n",
    "    return init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F8Yq_8I0WWhf"
   },
   "outputs": [],
   "source": [
    "def replay_clustering(name, nharms, dist=mean_dist):\n",
    "    # initial clustering\n",
    "    cluster_assignment = dict((i,i) for i in range(nharms))\n",
    "\n",
    "    # run iterative experiments\n",
    "    outputs = []\n",
    "    \n",
    "    for it in range(nharms):\n",
    "        nclusters = nharms - it\n",
    "        print(f\"iteration {it} ({nclusters} clusters).\")\n",
    "        print(cluster_assignment)\n",
    "        \n",
    "        # inference\n",
    "        params_load = torch.load(path.join(data_path, \"params\", f\"{name}_params{nclusters}.pt\"))\n",
    "        params = dict((n, torch.tensor(vals, device=device)) for n, vals in params_load.items())\n",
    "        \n",
    "        # record output\n",
    "        outputs.append(dict({\n",
    "            \"params\": params,\n",
    "            \"cluster_assignment\": cluster_assignment,\n",
    "        }))\n",
    "        \n",
    "        # compute next clustering / init\n",
    "        if nclusters > 1:\n",
    "            # find closest clusters\n",
    "            alphas = params[\"alpha_p_ict\"]\n",
    "            betas = params[\"beta_p_ict\"]\n",
    "            dists = dict()\n",
    "            for i in range(nclusters):\n",
    "                for j in range(i+1, nclusters):\n",
    "                    dists[(i,j)] = dist((alphas[i], betas[i]), (alphas[j], betas[j]))\n",
    "            min1, min2 = min(dists, key=dists.get)\n",
    "\n",
    "            # map clusters\n",
    "            remaining = [i for i in range(nclusters) if i not in [min1,min2]]\n",
    "            cluster_mapping = {**{min1: 0, min2: 0}, **dict((c,i+1) for i,c in enumerate(remaining))}\n",
    "            \n",
    "            # update assignment\n",
    "            cluster_assignment = dict((h,cluster_mapping[c]) for h,c in cluster_assignment.items())\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUZYLIwH6MNY"
   },
   "outputs": [],
   "source": [
    "def bf_meta_model(npcs, nharms, cluster_assignments, data, **kwargs):\n",
    "    nmodels = len(cluster_assignments)\n",
    "    m = pyro.sample(\"model\", Categorical(torch.ones(nmodels, device=device) / nmodels))\n",
    "    if m.dim() == 0:\n",
    "        return chord_model(npcs, nharms, cluster_assignments[m], data, **kwargs, pfx=f\"m{m}\")\n",
    "    else:\n",
    "        for i in m:\n",
    "            chord_model(npcs, nharms, cluster_assignments[i], data, **kwargs, pfx=f\"m{i}\")\n",
    "\n",
    "def bf_meta_guide(npcs, nharmonies, cluster_assignments, data, inits, **kwargs):\n",
    "    nmodels = len(cluster_assignments)\n",
    "    params_model = pyro.param(\"params_model\", torch.ones(nmodels, device=device) / nmodels, constraint=constraints.simplex)\n",
    "    m = pyro.sample(\"model\", Categorical(params_model), infer={'enumerate': 'sequential'})\n",
    "    chord_guide(npcs, nharmonies, cluster_assignments[m], data,\n",
    "                pfx=f\"m{m}\", init=inits[m], optimize=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1VYIZxc6MNY"
   },
   "outputs": [],
   "source": [
    "def bf_infer_model_posterior(obs, outputs, chordtypes,\n",
    "                             nsteps=5_000, subsamples=10_000, particles=1, lr=0.01,\n",
    "                             plot_loss=True, save_as=None):\n",
    "    # optimize the parameters of the guide\n",
    "    pyro.clear_param_store()\n",
    "    pyro.set_rng_seed(1625) # set every time for independent reproducibility\n",
    "    svi = pyro.infer.SVI(model=bf_meta_model,\n",
    "                         guide=bf_meta_guide,\n",
    "                         optim=pyro.optim.Adam({\"lr\": lr, \"betas\": (0.95, 0.999)}),\n",
    "                         #optim=pyro.optim.Adadelta({\"lr\": 1.0, \"rho\": 0.9}),\n",
    "                         #optim=pyro.optim.SGD({\"lr\": 0.00005, \"momentum\": 0.9, \"nesterov\": True}),\n",
    "                         loss=pyro.infer.TraceEnum_ELBO(num_particles=particles))\n",
    "\n",
    "    nharms = len(chordtypes)\n",
    "    nmodels = len(outputs)\n",
    "    assignments = [torch.tensor([outputs[m]['cluster_assignment'][i] for i in range(nharms)], device=device)\n",
    "                           for m in range(nmodels)]\n",
    "    inits = [post_to_init(outputs[i]['params']) for i in range(nmodels)]\n",
    "    \n",
    "    # set up histories for the loss and some of the parameters\n",
    "    losses = np.zeros(nsteps)\n",
    "    param_history = np.zeros((nsteps,nmodels))\n",
    "    \n",
    "    # run the optimization\n",
    "    for i in tqdm.trange(nsteps):\n",
    "        # update parameters and record loss\n",
    "        losses[i] = svi.step(npcs, nharms, assignments, obs, inits=inits, subsamples=subsamples)\n",
    "        \n",
    "        # record values of some parameters\n",
    "        ps = pyro.get_param_store()\n",
    "        param_history[i] = ps.get_param(f'params_model').cpu().detach()\n",
    "\n",
    "        if (plot_loss and (i % 10 == 0)):\n",
    "          display.clear_output(wait=True)\n",
    "          plt.figure()\n",
    "          plt.plot(losses[:i+1])\n",
    "          plt.xlabel(\"iteration\")\n",
    "          plt.ylabel(\"loss\")\n",
    "          plt.show()\n",
    "          plot_param_history(param_history[:i+1])\n",
    "          \n",
    "\n",
    "    # plot the loss\n",
    "    if plot_loss:\n",
    "        plt.figure()\n",
    "        plt.plot(losses)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.show()\n",
    "        print(\"loss variance (last 100 steps):\", losses[-100:].var())\n",
    "    \n",
    "    params = dict((name, value.cpu().detach().numpy()) for name, value in pyro.get_param_store().items())\n",
    "    if save_as != None:\n",
    "        save_params(params, chordtypes, save_as)\n",
    "    \n",
    "    return params, param_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Od4fq5Ks6MNY"
   },
   "outputs": [],
   "source": [
    "def bf_bayes_factors(params):\n",
    "    params = params['params_model']\n",
    "    nmodels = len(params)\n",
    "    bfs = np.array([[params[i] / params[j] for j in range(nmodels)] for i in range(nmodels)])\n",
    "    return bfs, params\n",
    "\n",
    "def plot_bayes_factors(bfs, probs):\n",
    "    nclusters = len(probs)\n",
    "    labels = list(range(nclusters, 0, -1))\n",
    "\n",
    "    sns.heatmap(bfs,\n",
    "                xticklabels=labels, yticklabels=labels,\n",
    "                norm=LogNorm(vmin=bfs.min(), vmax=bfs.max()), center=0)\n",
    "    plt.show()\n",
    "    sns.barplot(x=labels, y=probs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yw9v6s2i6MNZ"
   },
   "outputs": [],
   "source": [
    "def run_bayes_factors(obs, outputs, chordtypes, nsteps=500, lr=0.01, save_as=None):\n",
    "    params, hist = bf_infer_model_posterior(obs, outputs, chordtypes,\n",
    "                                            nsteps=nsteps, subsamples=None, particles=1, lr=lr,\n",
    "                                            plot_loss=True, save_as=save_as)\n",
    "    return params, hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ChZDl273WWhh"
   },
   "outputs": [],
   "source": [
    "def run_bayes_factors_mcmc(obs, outputs, chordtypes, nsamples=1_000, nwarmup=None, jit=False, chains=1):\n",
    "    if nwarmup is None:\n",
    "        nwarmup = nsamples\n",
    "    \n",
    "    nuts_kernel = pyro.infer.NUTS(bf_meta_model, jit_compile=jit)\n",
    "    mcmc = pyro.infer.MCMC(nuts_kernel,\n",
    "                           num_samples=nsamples,\n",
    "                           warmup_steps=nwarmup,\n",
    "                           num_chains=chains)\n",
    "    \n",
    "    nharms = len(chordtypes)\n",
    "    nmodels = len(outputs)\n",
    "    assignments = [torch.tensor([outputs[m]['cluster_assignment'][i] for i in range(nharms)], device=device)\n",
    "                           for m in range(nmodels)]\n",
    "    \n",
    "    mcmc.run(npcs, nharms, assignments, obs, subsamples=None)\n",
    "    return mcmc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_kYeq-9YWWhi",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#dcml_mcmc = run_bayes_factors_mcmc(dcml_obs, dcml_outputs, dcml_chordtypes, nsamples=1)\n",
    "#dcml_mcmc.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gD92nHWn6MNW"
   },
   "source": [
    "## Plotting\n",
    "\n",
    "To inspect the results and the behaviour of the optimization, we define some functions for plotting parameter histories and posterior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBeuSNBT6MNa"
   },
   "outputs": [],
   "source": [
    "# histories\n",
    "\n",
    "def plot_param_history(history):\n",
    "    df = pd.DataFrame(history)\n",
    "    df.plot()\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.show()\n",
    "    \n",
    "def plot_roots(root_history, ylabel='root parameters'):\n",
    "    plt.plot(root_history)\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "def plot_histories(param_history, root_history, harm_history):\n",
    "    plot_param_history(param_history)\n",
    "    plot_roots(root_history)\n",
    "    plot_roots(harm_history, ylabel='chord type parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0sdfFtX6MNb"
   },
   "outputs": [],
   "source": [
    "# posteriors\n",
    "\n",
    "# posterior of 'rate_notes'\n",
    "def plot_note_rate(params, lower=0, upper=10):\n",
    "    alpha = params['alpha_rate_notes']\n",
    "    beta = params['beta_rate_notes']\n",
    "    print(alpha)\n",
    "    print(beta)\n",
    "    x = np.linspace(lower, upper, 200)\n",
    "    y = stats.gamma.pdf(x, alpha, scale=1/beta)\n",
    "    plt.plot(x,y)\n",
    "    plt.xlabel('rate_notes')\n",
    "    plt.show()\n",
    "    xrate = np.linspace(0,10,11)\n",
    "    yrate = stats.nbinom.pmf(xrate, alpha, 1/(1+1/beta))\n",
    "    plt.bar(xrate+1, yrate)\n",
    "    plt.xlabel('nnotes')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_note_rates(phist, n=100, lower=0, upper=10):\n",
    "    alphas = phist['alpha_rate_notes'][-n:]\n",
    "    betas = phist['beta_rate_notes'][-n:]\n",
    "    x = np.linspace(lower, upper, 200)\n",
    "    ys = np.array([stats.gamma.pdf(x, a, scale=1/b) for (a,b) in zip(alphas,betas)]).transpose()\n",
    "    plt.plot(x,ys, color='steelblue', alpha=0.5)\n",
    "    plt.xlabel('rate_notes')\n",
    "    plt.show()\n",
    "    \n",
    "# posterior of 'p_is_chordtone'\n",
    "def plot_p_ict(params, harmtypes, cluster_assignment, lower=0, upper=1):\n",
    "    alphas = params[\"alpha_p_ict\"]\n",
    "    betas  = params[\"beta_p_ict\"]\n",
    "    x = np.linspace(lower, upper, 200)\n",
    "    y = np.array([stats.beta.pdf(x, a, b) for a, b in zip(alphas, betas)]).transpose()\n",
    "    names = dict()\n",
    "    for chord, cluster in cluster_assignment.items():\n",
    "        name = harmtypes[chord]\n",
    "        if(names.get(cluster) == None):\n",
    "            names[cluster] = []\n",
    "        names[cluster].append(name)\n",
    "    # This might work\n",
    "    its = list(names.items())\n",
    "    its.sort()\n",
    "    ns = [str.join(\", \",it[1]) for it in its]\n",
    "    plt.plot(x,y)\n",
    "    plt.xlabel(\"p_is_chordtone\")\n",
    "    plt.legend(ns, bbox_to_anchor=(1., 1), loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "# posterior of chord type probabilities\n",
    "def plot_chord_type_dist(params, labels):\n",
    "    plt.figure(figsize=(6,9))\n",
    "    alphas = params['params_p_harmony']\n",
    "    plt.barh(np.arange(len(alphas)), alphas, tick_label=labels)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel(\"params_p_harmony\")\n",
    "    plt.show()\n",
    "\n",
    "# posteriors of note probabilities\n",
    "def plot_chords(params, labels):\n",
    "    post_chordtones = params['params_p_chordtones']\n",
    "    post_ornaments = params['params_p_ornaments']\n",
    "    for i, name in enumerate(labels):\n",
    "        utils.plot_profile(post_chordtones[i], post_ornaments[i], name)\n",
    "        utils.play_chord(post_chordtones[i])\n",
    "\n",
    "# plot all posteriors\n",
    "def plot_posteriors(params, chordtypes):\n",
    "    plot_note_rate(params)\n",
    "    plot_p_ict(params, chordtypes)\n",
    "    plot_chord_type_dist(params, chordtypes)\n",
    "    plot_chords(params, chordtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yPLcA6w6MNb"
   },
   "source": [
    "# 2. Experiments\n",
    "\n",
    "## Clustering (DCML Corpus)\n",
    "\n",
    "The DCML corpus is a collection of classical pieces with elaborate harmonic annotations.\n",
    "Here we only distinguish the basic harmonic types defined in the annotation standard (triads and seventh chords),\n",
    "since the extra information (inversion, suspensions, added notes etc.) do not change the type of the chord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJxgyhAq6MNb",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# prepare the dataset\n",
    "dcml_df, dcml_obs, dcml_chordtypes = load_dataset('data/dcml.tsv', dcml_chordtype_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GEL8m6X3sYda"
   },
   "outputs": [],
   "source": [
    "dcml_chordtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57HfRmV6NFEK"
   },
   "outputs": [],
   "source": [
    "dcml_init = get_init_params(dcml_df, len(dcml_chordtypes), npcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ucseKp_6MNc"
   },
   "outputs": [],
   "source": [
    "print(dcml_chordtypes)\n",
    "#print(dcml_init)\n",
    "#for init in dcml_inits:\n",
    "#    print(init['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JfQP_EOA6MNc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dcml_outputs = iterative_clustering(dcml_init, dcml_chordtypes, dcml_obs, \"dcml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3DU9km_6MNc"
   },
   "outputs": [],
   "source": [
    "dcml_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7vfaa0F6MNc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the histories the parameters to check convergence\n",
    "for output in dcml_outputs:\n",
    "    plot_histories(output['hist'], output['roots'], output['harm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlhnKAsC6MNd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for output in dcml_outputs:\n",
    "    # plot the posterior distributions of the parameters\n",
    "    #plot_note_rate(output['params'], lower=5.5, upper=5.65)\n",
    "    #plot_note_rates(output['hist'], n=50, lower=5.5, upper=5.65)\n",
    "    plot_p_ict(output['params'], dcml_chordtypes, output['cluster_assignment'], lower=0.7, upper=0.95)\n",
    "    #plot_chord_type_dist(output['params'], dcml_chordtypes)\n",
    "    #plot_chords(output['params'], dcml_chordtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1v_bMkw6yK-"
   },
   "source": [
    "## Bayes Factor (DCML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eCA5j30sWWhp"
   },
   "outputs": [],
   "source": [
    "# reload the data (optional, can be used to skip the clustering)\n",
    "# dcml_outputs = replay_clustering(\"dcml\", 11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVnKTzWh6MNd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dcml_cmp, dcml_cmp_hist = run_bayes_factors(dcml_obs, dcml_outputs, dcml_chordtypes, nsteps=500, lr=0.05, save_as=\"bf_dcml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_C69PJ956MNe"
   },
   "outputs": [],
   "source": [
    "plot_param_history(dcml_cmp_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeGm7fvT6MNe"
   },
   "outputs": [],
   "source": [
    "dcml_bfs, dcml_m_params = bf_bayes_factors(dcml_cmp)\n",
    "plot_bayes_factors(dcml_bfs, dcml_m_params)\n",
    "dcml_bfs[8,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6Bggxu1GHquZ"
   },
   "source": [
    "## Clustering (EWLD Corpus)\n",
    "\n",
    "EWLD is a large subset of Wikifonia, so it shares the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4rvj2Z4VHqub"
   },
   "outputs": [],
   "source": [
    "# prepare the dataset\n",
    "ewld_df, ewld_obs, ewld_chordtypes = load_dataset('data/ewld.tsv', wiki_chordtype_map)\n",
    "ewld_init = get_init_params(ewld_df, len(ewld_chordtypes), npcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gnMTHgcLHquc"
   },
   "outputs": [],
   "source": [
    "print(ewld_chordtypes)\n",
    "#for init in wiki_inits:\n",
    "    #print(init['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ap3gN5vKHqud",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ewld_outputs = iterative_clustering(ewld_init, ewld_chordtypes, ewld_obs, \"ewld\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TAZ_p-P0Hque"
   },
   "outputs": [],
   "source": [
    "# plot the histories the parameters to check convergence\n",
    "for output in ewld_outputs:\n",
    "    plot_histories(output['hist'], output['roots'], output['harm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mUyrSvEHHquf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the posterior distributions of the parameters\n",
    "for output in ewld_outputs:\n",
    "    plot_p_ict(output['params'], ewld_chordtypes, output['cluster_assignment'], lower=0.65, upper=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KKituaxZHquf"
   },
   "source": [
    "## Bayes Factor (EWLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9yhANw78Hquf"
   },
   "outputs": [],
   "source": [
    "ewld_cmp, ewld_cmp_hist = run_bayes_factors(ewld_obs, ewld_outputs, ewld_chordtypes, nsteps=1_000, lr=0.05, save_as=\"bf_ewld\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lOaE48exHqug"
   },
   "outputs": [],
   "source": [
    "plot_param_history(ewld_cmp_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzIrgUdGHquh"
   },
   "outputs": [],
   "source": [
    "ewld_bfs, ewld_m_params = bf_bayes_factors(ewld_cmp)\n",
    "plot_bayes_factors(ewld_bfs, ewld_m_params)\n",
    "ewld_bfs[8,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-GyvF8t6MNe"
   },
   "source": [
    "## Clustering (Wikifonia Corpus, obsolete)\n",
    "\n",
    "The Wikifonia dataset consists of leadsheets, i.e. melodies and chord labels.\n",
    "It uses the chord types set in the MusicXML source of the chord-labels, which can be quite chaotic.\n",
    "Therefore, we normalize the chord-types to the ones defined in the MusicXML standard,\n",
    "removing unclear chord labels (which are rather rare)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ip0rzBT36MNe"
   },
   "outputs": [],
   "source": [
    "# prepare the dataset\n",
    "wiki_df, wiki_obs, wiki_chordtypes = load_dataset('data/wikifonia.tsv', wiki_chordtype_map)\n",
    "wiki_init = get_init_params(wiki_df, len(wiki_chordtypes), npcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WS2FABC76MNf"
   },
   "outputs": [],
   "source": [
    "print(wiki_chordtypes)\n",
    "#for init in wiki_inits:\n",
    "    #print(init['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMugYsRf6MNf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_outputs = iterative_clustering(wiki_init, wiki_chordtypes, wiki_obs, \"wiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Drl122FA6MNg"
   },
   "outputs": [],
   "source": [
    "# plot the histories the parameters to check convergence\n",
    "for output in wiki_outputs:\n",
    "    plot_histories(output['hist'], output['roots'], output['harm'])\n",
    "#plot_histories(whist, wroots, wharm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1d421bK6MNg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the posterior distributions of the parameters\n",
    "for output in wiki_outputs:\n",
    "    plot_p_ict(output['params'], wiki_chordtypes, output['cluster_assignment'], lower=0.65, upper=0.95)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3RNdhp4VzHu"
   },
   "source": [
    "## Bayes Factor (Wikifonia, obsolete)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ii-9gtyNJlf-"
   },
   "outputs": [],
   "source": [
    "wiki_cmp, wiki_cmp_hist = run_bayes_factors(wiki_obs, wiki_outputs, wiki_chordtypes, nsteps=1_000, lr=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "risA6JC4JlgC"
   },
   "outputs": [],
   "source": [
    "plot_param_history(wiki_cmp_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGzoLRVIJlgE"
   },
   "outputs": [],
   "source": [
    "wiki_bfs, wiki_m_params = bf_bayes_factors(wiki_cmp)\n",
    "plot_bayes_factors(wiki_bfs, wiki_m_params)\n",
    "wiki_bfs[8,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgBEgX8wJZJt"
   },
   "source": [
    "# Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THE2I6J16MNg"
   },
   "outputs": [],
   "source": [
    "len(wiki_inits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1uPOfQwb6MNh"
   },
   "outputs": [],
   "source": [
    "wiki_outputs[0]['wiki_params']['params_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d87R_46m6MNh"
   },
   "outputs": [],
   "source": [
    "wiki_inits[5]['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBmn3kDf6MNi"
   },
   "outputs": [],
   "source": [
    "dcml_outputs[0]['dcml_params']['params_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tcbJtPic6MNi"
   },
   "outputs": [],
   "source": [
    "dcml_inits[5]['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqFxxlLN6MNi"
   },
   "outputs": [],
   "source": [
    "dcml_inits[5]['is_ct']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtxuONdy6MNj"
   },
   "outputs": [],
   "source": [
    "dcml_inits[5]['is_or']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKZ4Ff4m6MNj"
   },
   "outputs": [],
   "source": [
    "dcml_outputs[0]['dcml_params']['alpha_p_ict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9zk-T5j6MNk"
   },
   "outputs": [],
   "source": [
    "dcml_outputs[0]['dcml_params']['beta_p_ict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quSv97qd6MNk"
   },
   "outputs": [],
   "source": [
    "stats.beta.mean(dcml_inits[5]['is_ct'][0],dcml_inits[5]['is_or'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sjond3E06MNk"
   },
   "outputs": [],
   "source": [
    "stats.beta.mean(dcml_inits[5]['is_ct'][1],dcml_inits[5]['is_or'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_-cY_-S6MNl"
   },
   "outputs": [],
   "source": [
    "stats.beta.mean(dcml_inits[5]['is_ct'][5],dcml_inits[5]['is_or'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bARshw7E6MNl"
   },
   "outputs": [],
   "source": [
    "stats.beta.mean(dcml_outputs[0]['dcml_params']['alpha_p_ict'][5],dcml_outputs[0]['dcml_params']['beta_p_ict'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xmI7PKS6MNl"
   },
   "outputs": [],
   "source": [
    "stats.beta.mean(dcml_outputs[0]['dcml_params']['alpha_p_ict'][1],dcml_outputs[0]['dcml_params']['beta_p_ict'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1LV_muA6MNm"
   },
   "outputs": [],
   "source": [
    "stats.beta.mean(dcml_inits[13]['is_ct'][6],dcml_inits[13]['is_or'][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9tmzqqB6MNm"
   },
   "outputs": [],
   "source": [
    "stats.beta.mean(dcml_inits[13]['is_ct'][7],dcml_inits[13]['is_or'][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWLMe0Ks6MNm"
   },
   "outputs": [],
   "source": [
    "stats.beta.mean(dcml_inits[13]['is_ct'][9],dcml_inits[13]['is_or'][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwP4uRTG6MNm"
   },
   "outputs": [],
   "source": [
    "stats.beta.mean(dcml_inits[13]['is_ct'][0],dcml_inits[13]['is_or'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ishPsBEK6MNm"
   },
   "outputs": [],
   "source": [
    "stats.beta.mean(dcml_inits[13]['is_ct'][1],dcml_inits[13]['is_or'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUNhaxZY6MNm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "fTE5VdZ46MNW",
    "EqnikMsp6MNX",
    "GdBFIMnS6MNa",
    "1yPLcA6w6MNb",
    "v-GyvF8t6MNe"
   ],
   "name": "model_paper_clusters_iterative_gpu.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dcml-harmony-and-ornamentation",
   "language": "python",
   "name": "dcml-harmony-and-ornamentation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
