{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uh1Hgg6E6MM2"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import pyro\n",
    "from pyro.distributions import *\n",
    "#from collections import Counter\n",
    "import pyro.infer\n",
    "import pyro.optim\n",
    "import pyro.util\n",
    "pyro.enable_validation(True)\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LogNorm\n",
    "import seaborn as sns\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import scipy.special as special\n",
    "import math\n",
    "\n",
    "import os.path as path\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import utils\n",
    "\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CmkL2zHp6MNH"
   },
   "outputs": [],
   "source": [
    "cpu = torch.cuda.is_available():\n",
    "\n",
    "# TODO: set the GPU you want to use\n",
    "gpu_n = 0\n",
    "\n",
    "device = torch.device(f'cuda:{gpu_n}' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fMWmdkVhHGcR"
   },
   "outputs": [],
   "source": [
    "def debug_tensors():\n",
    "    for obj in gc.get_objects():\n",
    "        if torch.is_tensor(obj) and obj.device == torch.device(\"cpu\"):\n",
    "            print(f\"{obj.device}, {obj.dtype}, {obj.shape}\")\n",
    "\n",
    "debug_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z_2bYyV56MNJ"
   },
   "outputs": [],
   "source": [
    "def save_rng_state(name):\n",
    "    fn = name + '-' + datetime.today().isoformat() + '.state'\n",
    "    state = pyro.util.get_rng_state()\n",
    "    with open('rng-' + fn, 'w') as f:\n",
    "        print(state, file=f)\n",
    "    torch.save(state['torch'], 'torch-' + fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QrhA9BQz6MNK"
   },
   "outputs": [],
   "source": [
    "torch.set_default_dtype(torch.float64)\n",
    "\n",
    "# set random seeds\n",
    "pyro.set_rng_seed(0)\n",
    "#torch.set_deterministic(True)\n",
    "#torch.set_num_threads(1)\n",
    "#torch.set_num_interop_threads(1)\n",
    "\n",
    "# fix the range of pitches we consider\n",
    "fifth_range = 2*7                  # 2 diatonics\n",
    "npcs = 2*fifth_range+1             # around C: Cbb to C## on LoF\n",
    "utils.set_fifth_range(fifth_range) # used to make helper functions work correctly\n",
    "nclusters = 14                      # Try clustering down to how many clusters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5_0Ihvk26MNL"
   },
   "source": [
    "# A Cognitive Model or Harmonic Types\n",
    "\n",
    "## Model\n",
    "\n",
    "A chord consists of a number of notes,\n",
    "which are either generated as stable chord tones or as ornaments.\n",
    "We model this process by distinguishing (for each chord type)\n",
    "a distribution of chord tones and a distribution of ornaments.\n",
    "For each generated note, a coin is flipped as to whether the tone is generated as a chord tone or ornament.\n",
    "The pitch is then drawn from the corresponding distribution.\n",
    "Since we don't always know the type of a note, we flip another coin to decide whether the type is observed or not (in which case `unknown` is emitted for both ornaments and chordtones).\n",
    "\n",
    "Priors:\n",
    "- choose $\\vec\\chi \\sim \\text{Dirichlet}(0.5, n_\\text{harmonies})$\n",
    "- choose $\\lambda \\sim \\text{Gamma}(3,1)$\n",
    "- choose $\\theta_c \\sim \\text{Beta}(1,1)$ for each of $n_\\text{clusters}$ clusters\n",
    "- for each chord type $c$:\n",
    "  - choose $\\xi_c \\sim \\text{Categorical}(n_\\text{clusters})$\n",
    "  - choose $\\vec\\phi_{ct}^{(c)} \\sim \\text{Dirichlet}(0.5, n_\\text{pitches})$\n",
    "  - choose $\\vec\\phi_{or}^{(c)} \\sim \\text{Dirichlet}(0.5, n_\\text{pitches})$\n",
    "\n",
    "Generating a single chord (long version):\n",
    "- choose $h \\sim \\text{Categorical}(\\vec\\chi)$\n",
    "- choose $n \\sim \\text{Poisson}(\\lambda) + 1$\n",
    "- for each note $i \\in 1, \\ldots, n$:\n",
    "  - choose $t_i \\sim \\text{Bernoulli}(\\theta_{\\xi_c})$\n",
    "  - choose $p_i \\sim \\begin{cases}\n",
    "                       \\text{Categorical}(\\vec\\phi_{ct}^{(h)}) & \\text{if } t_i = 1\\\\\n",
    "                       \\text{Categorical}(\\vec\\phi_{or}^{(h)}) & \\text{if } t_i = 0\n",
    "                     \\end{cases}$\n",
    "  - choose $o_i \\sim \\text{Bernoulli}(p_\\text{obs})$\n",
    "  - choose $ot_i = \\begin{cases}\n",
    "                     \\text{'chordtone'} & \\text{if } o_i = 1 \\wedge p_i = 1\\\\\n",
    "                     \\text{'ornament'} & \\text{if } o_i = 1 \\wedge p_i = 0\\\\\n",
    "                     \\text{'unknown'} & \\text{if } o_i = 0\\\\\n",
    "                   \\end{cases}$\n",
    "- count $(p_i,ot_i)$ pairs\n",
    "\n",
    "Generating a single chord (compact version)\n",
    "- choose $h \\sim \\text{Catecorical}(\\vec\\chi)$\n",
    "- choose $n \\sim \\text{Poisson}(\\lambda) + 1$\n",
    "- choose $n_{p,ot} \\sim \\text{Multinomial}(n, \\vec\\nu),$ where\n",
    "  - $\\nu_{ct} = p_\\text{obs} \\cdot \\theta_{\\xi_c} \\cdot \\vec\\phi_{ct}^{(h)}$\n",
    "  - $\\nu_{or} = p_\\text{obs} \\cdot (1-\\theta_{\\xi_c}) \\cdot \\vec\\phi_{or}^{(o)}$\n",
    "  - $\\nu_{uk} = (1-p_\\text{obs}) \\cdot \\left( \\theta_{\\xi_c} \\vec\\phi_{ct}^{(h)} + (1-\\theta_{\\xi_c}) \\vec\\phi_{or}^{(h)} \\right)$\n",
    "  - $\\nu = \\text{concat}(\\nu_{ct}, \\nu_{or}, \\nu_{uk})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6kuiOGLz6MNO"
   },
   "outputs": [],
   "source": [
    "def chord_model(npcs, nharmonies, cluster_assignment, data, pfx=None, subsamples=500, pobserve=0.5, **kwargs):\n",
    "    if pfx is None or pfx == \"\":\n",
    "        pfx = \"\"\n",
    "    else:\n",
    "        pfx += \"_\"\n",
    "    nclusters = max(cluster_assignment) + 1\n",
    "    # parameters priors:\n",
    "    # distribution of the harmonies\n",
    "    p_harmony = pyro.sample(f'{pfx}p_harmony', Dirichlet(0.5 * torch.ones(nharmonies, device=device)))\n",
    "    # each cluster \n",
    "    with pyro.plate(f'{pfx}clusters', nclusters) as ind:\n",
    "        # distribution of ornament probability\n",
    "        p_is_chordtone = pyro.sample(f'{pfx}p_is_chordtone', Beta(torch.tensor(1, device=device),torch.tensor(1, device=device)))\n",
    "    # distribution of notes in the harmonies\n",
    "    with pyro.plate(f'{pfx}harmonies', nharmonies) as ind:\n",
    "        #cluster = pyro.sample('cluster', Categorical(torch.ones(nclusters)))\n",
    "        cluster = cluster_assignment[ind]\n",
    "        p_is_chordtone_c = p_is_chordtone[cluster]\n",
    "        #print(p_is_chordtone.shape)\n",
    "        # distribution of notes per note type\n",
    "        p_chordtones = pyro.sample(f'{pfx}p_chordtones', Dirichlet(0.5 * torch.ones(npcs, device=device)))\n",
    "        p_ornaments  = pyro.sample(f'{pfx}p_ornaments', Dirichlet(0.5 * torch.ones(npcs, device=device)))\n",
    "        #print(p_chordtones.shape)\n",
    "        # we build a big categorical out of the chordtones and ornaments,\n",
    "        # including notes of unknown type (marginalizing over the categories)\n",
    "        #p_ct = p_is_chordtone_c       * p_chordtones\n",
    "        #p_or = (1 - p_is_chordtone_c) * p_ornaments\n",
    "        p_ct = torch.mm(torch.diag(p_is_chordtone_c), p_chordtones)\n",
    "        p_or = torch.mm(torch.diag(1 - p_is_chordtone_c), p_ornaments)\n",
    "        p_unobserved = p_ct + p_or\n",
    "        p_tones = torch.cat([pobserve * p_ct, pobserve * p_or, (1-pobserve) * p_unobserved], dim=1)\n",
    "    # distribution of note rate in chords\n",
    "    rate_notes = pyro.sample(f'{pfx}rate_notes', Gamma(torch.tensor(3, device=device),torch.tensor(1, device=device)))\n",
    "    \n",
    "    # sampling the data:\n",
    "    nchords = len(data['c'])\n",
    "    subs = min(nchords,subsamples) if subsamples != None else None\n",
    "    with pyro.plate(f'{pfx}data', nchords, subsample_size=subs) as ind:\n",
    "        # pick a harmony\n",
    "        c = pyro.sample(f'{pfx}c', Categorical(p_harmony), obs=data['c'][ind])\n",
    "        # pick a number of notes\n",
    "        nnotes = 1 + pyro.sample(f'{pfx}n', Poisson(rate_notes), obs=data['n'][ind]).int()\n",
    "        # sample chordtones\n",
    "        # Normally we would sample nnotes notes for each chord, but that doesn't work vectorized.\n",
    "        # However, evaluating the probability ignores n, so we can just provide 1 here.\n",
    "        notes = pyro.sample(f'{pfx}chord', Multinomial(1, p_tones[c], validate_args=False), obs=data['notes'][ind])\n",
    "        chords = {'c': c,\n",
    "                  'n': nnotes,\n",
    "                  'counts': notes.reshape(-1,npcs)}\n",
    "    return chords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0t-yDIwZ6MNO"
   },
   "source": [
    "## Guide\n",
    "\n",
    "A simple guide that assumes the latent variables to be distributed independently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VbmjeUf66MNP"
   },
   "outputs": [],
   "source": [
    "def chord_guide(npcs, nharmonies, cluster_assignment, data, pfx=None, subsamples=500, pobserve=0.5, init=dict(), optimize=True):\n",
    "    if pfx is None or pfx == \"\":\n",
    "        pfx = \"\"\n",
    "    else:\n",
    "        pfx += \"_\"\n",
    "    nclusters = max(cluster_assignment) + 1\n",
    "    \n",
    "    # posterior of p_harmony\n",
    "    params_p_harmony = init['harmonies'].to(device) if 'harmonies' in init else 0.5 * torch.ones(nharmonies, device=device)\n",
    "    if optimize:\n",
    "        params_p_harmony = pyro.param(f'{pfx}params_p_harmony',\n",
    "                                      params_p_harmony,\n",
    "                                      constraint=constraints.positive)\n",
    "    pyro.sample(f'{pfx}p_harmony', Dirichlet(params_p_harmony))\n",
    "    \n",
    "    # posteriors of notes dists in harmonies (parameters)\n",
    "    params_p_chordtones = init['chordtones'].to(device) if  'chordtones' in init else 0.5 * torch.ones(nharmonies, npcs, device=device)\n",
    "    params_p_ornaments = init['ornaments'].to(device) if 'ornaments' in init else 0.5 * torch.ones(nharmonies, npcs, device=device)\n",
    "    if optimize:\n",
    "        params_p_chordtones = pyro.param(f'{pfx}params_p_chordtones',\n",
    "                                         params_p_chordtones,\n",
    "                                         constraint=constraints.positive)\n",
    "        params_p_ornaments = pyro.param(f'{pfx}params_p_ornaments',\n",
    "                                        params_p_ornaments,\n",
    "                                        constraint=constraints.positive)\n",
    "    \n",
    "    # posterior of ornament probability (parameters)\n",
    "    alpha_p_ict = init['is_ct'].to(device) if 'is_ct' in init else torch.ones(nclusters, device=device)\n",
    "    beta_p_ict = init['is_or'].to(device) if 'is_or' in init else torch.ones(nclusters, device=device)\n",
    "    if optimize:\n",
    "        alpha_p_ict = pyro.param(f'{pfx}alpha_p_ict',\n",
    "                                 alpha_p_ict,\n",
    "                                 constraint=constraints.positive)\n",
    "        beta_p_ict = pyro.param(f'{pfx}beta_p_ict',\n",
    "                                beta_p_ict,\n",
    "                                constraint=constraints.positive)\n",
    "    \n",
    "    with pyro.plate(f'{pfx}clusters', nclusters) as ind:\n",
    "        pyro.sample(f'{pfx}p_is_chordtone', Beta(alpha_p_ict, beta_p_ict))\n",
    "        \n",
    "    # posteriors of ornament probability and note distributions\n",
    "    with pyro.plate(f'{pfx}harmonies', nharmonies) as ind:\n",
    "        pyro.sample(f'{pfx}p_chordtones', Dirichlet(params_p_chordtones[ind]))\n",
    "        pyro.sample(f'{pfx}p_ornaments', Dirichlet(params_p_ornaments[ind]))\n",
    "        \n",
    "    #posterior of note rate\n",
    "    alpha_rate_notes = init['sum_chords'].to(device) if 'sum_chords' in init else torch.tensor(3., device=device)\n",
    "    beta_rate_notes = init['n_chords'].to(device) if 'n_chords' in init else torch.tensor(1., device=device)\n",
    "    if optimize:\n",
    "        alpha_rate_notes = pyro.param(f'{pfx}alpha_rate_notes',\n",
    "                                      alpha_rate_notes,\n",
    "                                      constraint=constraints.positive)\n",
    "        beta_rate_notes = pyro.param(f'{pfx}beta_rate_notes',\n",
    "                                     beta_rate_notes,\n",
    "                                     constraint=constraints.positive)\n",
    "    rate_notes = pyro.sample(f'{pfx}rate_notes', Gamma(alpha_rate_notes, beta_rate_notes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fqVFPRdjsdb0"
   },
   "outputs": [],
   "source": [
    "pyro.clear_param_store()\n",
    "gc.collect()\n",
    "chord_guide(3, 5, torch.tensor([0,1,2,3,4], device=device), None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vFNJAYta6MNQ"
   },
   "source": [
    "## Data and Conditioning\n",
    "\n",
    "### Data Format\n",
    "\n",
    "The input data (i.e. the observations that the model is conditioned on) is represented by three tensors:\n",
    "- `c` for the chord labels (as \"categorical\" integers)\n",
    "- `n` for the number of notes in each chord\n",
    "- `notes` for the observed notes in each chord\n",
    "\n",
    "Each of these tensors represents the values for all chords at the same time (i.e. a *vectorized* representation),\n",
    "so the first dimension of each equals `nchords`, the number of chords.\n",
    "`c` and `n` are vectors, i.e. their value for each chord is a scalar.\n",
    "`notes` represents a vector for each chord that contains the counts of all pitch $\\times$ note type pairs in the chord.\n",
    "If we assume 29 pitch classes, we therefore have 87 entries: 29 for the chordtones, 29 for the ornaments, and 29 for the notes of unknown type.\n",
    "As a result, `notes` has dimension `nchords` $\\times$ 87.\n",
    "\n",
    "The values of `c` represent each chord's type, which is distributed according to a categorical distribution.\n",
    "In pyro/torch, categories are represented as integers, so we must convert textual labels into integers.\n",
    "Similarly, the index of a note in `notes` is determined by it's pitch class and type (as outlined above).\n",
    "While we allow negative pitch classes, they can be easily transformed into indices (and *vice versa*) by shifting all values by `npcs // 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOUEowXA6MNR"
   },
   "outputs": [],
   "source": [
    "def chord_tensor(notes):\n",
    "    \"\"\"Takes a list of notes as (fifth, type) pairs and returns a vector of counts.\"\"\"\n",
    "    notetype = {'chordtone': 0, 'ornament': 1, 'unknown': 2}\n",
    "    chord = torch.zeros((3, npcs), device=device)\n",
    "    for (fifth, t) in notes:\n",
    "        chord[notetype[t], utils.fifth_to_index(fifth)] += 1\n",
    "    return chord\n",
    "\n",
    "def annot_data_obs(chords):\n",
    "    \"\"\"Helper function to turn a list of chord dictionary into a dictionary of observation vectors.\"\"\"\n",
    "    obs = {}\n",
    "    obs[\"notes\"] = torch.cat([chord_tensor(c['notes']).reshape((1,-1)) for c in chords], dim=0)\n",
    "    obs[\"c\"] = torch.tensor([c['label'] for c in chords], device=device)\n",
    "    obs[\"n\"] = torch.tensor([len(c['notes']) - 1. for c in chords], device=device)\n",
    "    return obs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MNqDL5HD6MNR"
   },
   "source": [
    "### Loading the Dataset\n",
    "\n",
    "The data is loaded from a TSV file that.\n",
    "The resulting dataframe is converted to the observation format that we pass to the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2H2_wNx6MNT"
   },
   "outputs": [],
   "source": [
    "chordtypes_common = np.array(\n",
    "    [\"major\", \"minor\", \"dominant-7th\", \"diminished\",\n",
    "     \"full-diminished\", \"minor-7th\", \"half-diminished\", \"major-7th\",\n",
    "     \"augmented\", \"minor-major-7th\", \"augmented-7th\"],\n",
    "    dtype=\"object\")\n",
    "dcml_chordtype_map = {\n",
    "    \"M\": \"major\",\n",
    "    \"m\": \"minor\",\n",
    "    \"Mm7\": \"dominant-7th\",\n",
    "    \"o\": \"diminished\",\n",
    "    \"o7\": \"full-diminished\",\n",
    "    \"mm7\": \"minor-7th\",\n",
    "    \"%7\": \"half-diminished\",\n",
    "    \"MM7\": \"major-7th\",\n",
    "    \"+\": \"augmented\",\n",
    "    \"mM7\": \"minor-major-7th\",\n",
    "    \"+7\": \"augmented-7th\",\n",
    "}\n",
    "wiki_chordtype_map = {\n",
    "    \"major\": \"major\",\n",
    "    \"minor\": \"minor\",\n",
    "    \"dominant\": \"dominant-7th\",\n",
    "    \"diminished\": \"diminished\",\n",
    "    \"diminished-seventh\": \"full-diminished\",\n",
    "    \"minor-seventh\": \"minor-7th\",\n",
    "    \"half-diminished\": \"half-diminished\",\n",
    "    \"major-seventh\": \"major-7th\",\n",
    "    \"augmented\": \"augmented\",\n",
    "    \"major-minor\": \"minor-major-7th\",\n",
    "    \"augmented-seventh\": \"augmented-7th\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gZ-19ybr6MNT"
   },
   "outputs": [],
   "source": [
    "def load_dataset(filename, alt_labels):\n",
    "    print(\"loading dataset...\") \n",
    "    df = utils.load_csv(filename)\n",
    "    df = df[df.label.map(lambda l: l in alt_labels)]\n",
    "    sizes = df.groupby(['chordid', 'label']).size()\n",
    "    type_counts = sizes.groupby('label').size().sort_values(ascending=False)\n",
    "    chordtypes = type_counts.index.tolist()\n",
    "    df['numlabel'] = df.label.map(chordtypes.index)\n",
    "    df['alt_label'] = df.label.map(alt_labels)\n",
    "    \n",
    "    # check if precomputed tensor data is available:\n",
    "    prefn = filename + \"_common_precomp.pt\"\n",
    "    if path.exists(prefn) and path.getmtime(prefn) > path.getmtime(filename):\n",
    "        print(\"using precomputed tensor data.\")\n",
    "        obs = torch.load(prefn)\n",
    "        obs[\"notes\"] = obs[\"notes\"].to(device)\n",
    "        obs[\"c\"] = obs[\"c\"].to(device)\n",
    "        obs[\"n\"] = obs[\"n\"].to(device)\n",
    "        print(device)\n",
    "        print([(k, v.device) for k, v in obs.items()])\n",
    "    else:\n",
    "        print('extracting chords...')\n",
    "        chords = [{'label': label, 'notes': list(zip(grp.fifth, grp.type))}\n",
    "                  for (_, label), grp in tqdm.tqdm(df.groupby(['chordid', 'numlabel']))]\n",
    "        print('converting chords to tensors...')\n",
    "        obs = annot_data_obs(chords)\n",
    "        torch.save(obs, prefn)\n",
    "    \n",
    "    print(len(chordtypes), \"chord types\")\n",
    "    print(len(obs[\"c\"]), \"chords\")\n",
    "    return df, obs, [alt_labels[l] for l in chordtypes]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NruYLmo-6MNU"
   },
   "outputs": [],
   "source": [
    "def clust_dist(X,Y):\n",
    "    return 1 - (special.beta((X[0] + Y[0]) / 2, (X[1] + Y[1]) /2)) / math.sqrt(special.beta(X[0],X[1]) * special.beta(Y[0],Y[1]))\n",
    "\n",
    "def mean_dist(X,Y):\n",
    "    return abs(stats.beta.mean(X[0],X[1]) - stats.beta.mean(Y[0],Y[1]))\n",
    "\n",
    "def kl_dist(X,Y):\n",
    "    return []\n",
    "        \n",
    "#dist = mean_dist\n",
    "\n",
    "def get_init_params(df, nharms, npcs):\n",
    "    init = dict()\n",
    "    \n",
    "    init['harmonies'] = torch.tensor(df.groupby('numlabel').size().sort_values(ascending=False), device=device) + 0.5\n",
    "\n",
    "    init['chordtones'] = torch.zeros([nharms,npcs], device=device) + 0.5\n",
    "    for (numlabel, fifth), grp in df[df.type=='chordtone'].groupby(['numlabel','fifth']):\n",
    "        init['chordtones'][numlabel, utils.fifth_to_index(fifth)] += grp.fifth.count()\n",
    "\n",
    "    init['ornaments'] = torch.zeros([nharms,npcs], device=device) + 0.5\n",
    "    for (numlabel, fifth), grp in df[df.type=='ornament'].groupby(['numlabel','fifth']):\n",
    "        init['ornaments'][numlabel, utils.fifth_to_index(fifth)] += grp.fifth.count()\n",
    "    \n",
    "    init['is_ct'] = torch.tensor([sum(df[df.numlabel==l].type=='chordtone') for l in range(nharms)], device=device) + 1.\n",
    "    init['is_or'] = torch.tensor([sum(df[df.numlabel==l].type=='ornament') for l in range(nharms)], device=device) + 1.\n",
    "    \n",
    "    chord_sizes = df.groupby('chordid').size()-1\n",
    "    init['sum_chords'] = torch.tensor(sum(chord_sizes) + 3., device=device)\n",
    "    init['n_chords'] = torch.tensor(len(chord_sizes) + 1., device=device)\n",
    "    return init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwZ6GC856MNV"
   },
   "source": [
    "After inferring the parameters we save them for easier inspection and reuse."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nukGRzSb6MNV"
   },
   "outputs": [],
   "source": [
    "def save_params(params, chordtypes, name):\n",
    "    torch.save(params, name+'.pt')\n",
    "    with open(name+'.json', 'w') as f:\n",
    "        json.dump({'params': {key: val.tolist() for key,val in params.items()},\n",
    "                   'chordtypes': chordtypes},\n",
    "                  f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fTE5VdZ46MNW"
   },
   "source": [
    "## Inference\n",
    "\n",
    "Inference of the posterior is done via variational inference, i.e. by optimizing the parameters of the guide.\n",
    "The function `infer_posteriors` takes a dataset of observations,\n",
    "performs the optimization, and returns the optimized parameters together with some of their histories."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XjGIoHmO6MNW",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def infer_posteriors(obs, init, chordtypes, cluster_assignment,\n",
    "                     nsteps=5_000, subsamples=10_000, particles=1,\n",
    "                     plot_loss=True, save_as=None):\n",
    "    # optimize the parameters of the guide\n",
    "    pyro.clear_param_store()\n",
    "    pyro.set_rng_seed(1625) # set every time for independent reproducibility\n",
    "    svi = pyro.infer.SVI(model=chord_model,\n",
    "                         guide=chord_guide,\n",
    "                         optim=pyro.optim.Adam({\"lr\": 0.01, \"betas\": (0.95, 0.999)}),\n",
    "                         #optim=pyro.optim.Adadelta({\"lr\": 1.0, \"rho\": 0.9}),\n",
    "                         #optim=pyro.optim.SGD({\"lr\": 0.00005, \"momentum\": 0.9, \"nesterov\": True}),\n",
    "                         loss=pyro.infer.Trace_ELBO(num_particles=particles))\n",
    "\n",
    "    nharms = len(chordtypes)\n",
    "    \n",
    "    # set up histories for the loss and some of the parameters\n",
    "    losses = np.zeros(nsteps)\n",
    "    param_history = {name:np.zeros(nsteps) for name in ['alpha_rate_notes', 'beta_rate_notes']}#, 'alpha_p_ict', 'beta_p_ict']}\n",
    "    root_history = np.zeros((nsteps,nharms))\n",
    "    harm_history = np.zeros((nsteps,nharms))\n",
    "\n",
    "    # run the optimization\n",
    "    for i in tqdm.trange(nsteps):\n",
    "        # update parameters and record loss\n",
    "        losses[i] = svi.step(npcs, nharms, cluster_assignment, obs, subsamples=subsamples, init=init)\n",
    "        \n",
    "        # record values of some parameters\n",
    "        ps = pyro.get_param_store()\n",
    "        root_history[i] = ps.get_param('params_p_chordtones').cpu().detach()[:,fifth_range]\n",
    "        harm_history[i] = ps.get_param('params_p_harmony').cpu().detach()\n",
    "        for (name, value) in ps.items():\n",
    "            if name in param_history:\n",
    "                param_history[name][i] = value.cpu().item()\n",
    "\n",
    "    # plot the loss\n",
    "    if plot_loss:\n",
    "        plt.figure()\n",
    "        plt.plot(losses)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.show()\n",
    "        print(\"loss variance (last 100 steps):\", losses[-100:].var())\n",
    "    \n",
    "    params = dict((name, value.detach().cpu().detach().numpy()) for name, value in pyro.get_param_store().items())\n",
    "    if save_as != None:\n",
    "        save_params(params, chordtypes, save_as)\n",
    "    \n",
    "    return params, param_history, root_history, harm_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gD92nHWn6MNW"
   },
   "source": [
    "To inspect the results and the behaviour of the optimization, we define some functions for plotting parameter histories and posterior distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nUDFX4Jn6MNX"
   },
   "outputs": [],
   "source": [
    "def iterative_clustering(init, chordtypes, obs, name, dist=mean_dist, show_plots=True):\n",
    "    nharms = len(chordtypes)\n",
    "\n",
    "    # initial clustering\n",
    "    cluster_assignment = dict((i,i) for i in range(nharms))\n",
    "\n",
    "    # run iterative experiments\n",
    "    outputs = []\n",
    "    for it in range(nharms):\n",
    "        nclusters = nharms - it\n",
    "        print(f\"iteration {it} ({nclusters} clusters).\")\n",
    "        print(cluster_assignment)\n",
    "        \n",
    "        # inference\n",
    "        assignment = torch.tensor([cluster_assignment[i] for i in range(nharms)], device=device)\n",
    "        params, hist, roots, harm = infer_posteriors(obs, init, chordtypes, assignment,\n",
    "                                                     nsteps=350, subsamples=None, particles=1,\n",
    "                                                     save_as=\"dcml_params\"+str(nclusters),\n",
    "                                                     plot_loss=show_plots)\n",
    "        # record output\n",
    "        outputs.append(dict({\n",
    "            \"params\": params,\n",
    "            \"hist\": hist,\n",
    "            \"roots\": roots,\n",
    "            \"harm\": harm,\n",
    "            \"cluster_assignment\": cluster_assignment,\n",
    "            \"init\": init,\n",
    "        }))\n",
    "        \n",
    "        \n",
    "        if show_plots:\n",
    "            plot_p_ict(params, chordtypes, cluster_assignment, lower=0.7, upper=0.95)\n",
    "        \n",
    "        # compute next clustering / init\n",
    "        if nclusters > 1:\n",
    "            # find closest clusters\n",
    "            alphas = params[\"alpha_p_ict\"]\n",
    "            betas = params[\"beta_p_ict\"]\n",
    "            dists = dict()\n",
    "            for i in range(nclusters):\n",
    "                for j in range(i+1, nclusters):\n",
    "                    dists[(i,j)] = dist((alphas[i], betas[i]), (alphas[j], betas[j]))\n",
    "            min1, min2 = min(dists, key=dists.get)\n",
    "\n",
    "            # map clusters\n",
    "            remaining = [i for i in range(nclusters) if i not in [min1,min2]]\n",
    "            cluster_mapping = {**{min1: 0, min2: 0}, **dict((c,i+1) for i,c in enumerate(remaining))}\n",
    "            \n",
    "            # update assignment\n",
    "            cluster_assignment = dict((h,cluster_mapping[c]) for h,c in cluster_assignment.items())\n",
    "\n",
    "            # update init\n",
    "            # ... with the posterior parameters from the previous run\n",
    "            init = dict()\n",
    "            init['harmonies'] = torch.tensor(params['params_p_harmony'], device=device)\n",
    "            init['chordtones'] = torch.tensor(params['params_p_chordtones'], device=device)\n",
    "            init['ornaments'] = torch.tensor(params['params_p_ornaments'], device=device)\n",
    "            init['sum_chords'] = torch.tensor(params['alpha_rate_notes'], device=device)\n",
    "            init['n_chords'] = torch.tensor(params['beta_rate_notes'], device=device)\n",
    "            # ... and merged clusters\n",
    "            init['is_ct'] = torch.zeros(nclusters-1, dtype=torch.float64, device=device)\n",
    "            init['is_or'] = torch.zeros(nclusters-1, dtype=torch.float64, device=device)\n",
    "            for i in range(nclusters-1):\n",
    "                init['is_ct'][i] += alphas[cluster_mapping[i]]\n",
    "                init['is_or'][i] += betas[cluster_mapping[i]]\n",
    "\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EqnikMsp6MNX"
   },
   "source": [
    "## Bayes Factor Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cUZYLIwH6MNY"
   },
   "outputs": [],
   "source": [
    "def bf_meta_model(npcs, nharms, cluster_assignments, data, **kwargs):\n",
    "    nmodels = len(cluster_assignments)\n",
    "    m = pyro.sample(\"model\", Categorical(torch.ones(nmodels, device=device) / nmodels))\n",
    "    return chord_model(npcs, nharms, cluster_assignments[m], data, **kwargs, pfx=f\"m{m}\")\n",
    "\n",
    "def bf_meta_guide(npcs, nharmonies, cluster_assignments, data, inits, **kwargs):\n",
    "    nmodels = len(cluster_assignments)\n",
    "    params_model = pyro.param(\"params_model\", torch.ones(nmodels, device=device) / nmodels, constraint=constraints.simplex)\n",
    "    # pick a model uniformly, but assign the probability correctly using sample(obs=m)\n",
    "    m = pyro.sample(\"model\", Categorical(params_model))\n",
    "    chord_guide(npcs, nharmonies, cluster_assignments[m], data,\n",
    "                pfx=f\"m{m}\", init=inits[m], optimize=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-1VYIZxc6MNY"
   },
   "outputs": [],
   "source": [
    "def bf_infer_model_posterior(obs, outputs, chordtypes,\n",
    "                             nsteps=5_000, subsamples=10_000, particles=1,\n",
    "                             plot_loss=True, save_as=None):\n",
    "    # optimize the parameters of the guide\n",
    "    pyro.clear_param_store()\n",
    "    pyro.set_rng_seed(1625) # set every time for independent reproducibility\n",
    "    svi = pyro.infer.SVI(model=bf_meta_model,\n",
    "                         guide=bf_meta_guide,\n",
    "                         optim=pyro.optim.Adam({\"lr\": 0.01, \"betas\": (0.95, 0.999)}),\n",
    "                         #optim=pyro.optim.Adadelta({\"lr\": 1.0, \"rho\": 0.9}),\n",
    "                         #optim=pyro.optim.SGD({\"lr\": 0.00005, \"momentum\": 0.9, \"nesterov\": True}),\n",
    "                         loss=pyro.infer.Trace_ELBO(num_particles=particles))\n",
    "\n",
    "    nharms = len(chordtypes)\n",
    "    nmodels = len(outputs)\n",
    "    assignments = [torch.tensor([outputs[m]['cluster_assignment'][i] for i in range(nharms)], device=device)\n",
    "                           for m in range(nmodels)]\n",
    "    inits = [outputs[i]['init'] for i in range(nmodels)]\n",
    "    \n",
    "    # set up histories for the loss and some of the parameters\n",
    "    losses = np.zeros(nsteps)\n",
    "    param_history = np.zeros((nsteps,nmodels))\n",
    "    \n",
    "    # run the optimization\n",
    "    for i in tqdm.trange(nsteps):\n",
    "        # update parameters and record loss\n",
    "        losses[i] = svi.step(npcs, nharms, assignments, obs, inits=inits, subsamples=subsamples)\n",
    "        \n",
    "        # record values of some parameters\n",
    "        ps = pyro.get_param_store()\n",
    "        param_history[i] = ps.get_param(f'params_model').cpu().detach()\n",
    "\n",
    "    # plot the loss\n",
    "    if plot_loss:\n",
    "        plt.figure()\n",
    "        plt.plot(losses)\n",
    "        plt.xlabel(\"iteration\")\n",
    "        plt.ylabel(\"loss\")\n",
    "        plt.show()\n",
    "        print(\"loss variance (last 100 steps):\", losses[-100:].var())\n",
    "    \n",
    "    params = dict((name, value.cpu().detach().numpy()) for name, value in pyro.get_param_store().items())\n",
    "    if save_as != None:\n",
    "        save_params(params, chordtypes, save_as)\n",
    "    \n",
    "    return params, param_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Od4fq5Ks6MNY"
   },
   "outputs": [],
   "source": [
    "def bf_bayes_factors(params):\n",
    "    params = params['params_model']\n",
    "    nmodels = len(params)\n",
    "    bfs = np.array([[params[i] / params[j] for j in range(nmodels)] for i in range(nmodels)])\n",
    "    return bfs, params\n",
    "\n",
    "def plot_bayes_factors(bfs, probs):\n",
    "    nclusters = len(probs)\n",
    "    labels = list(range(nclusters, 0, -1))\n",
    "\n",
    "    sns.heatmap(bfs,\n",
    "                xticklabels=labels, yticklabels=labels,\n",
    "                norm=LogNorm(vmin=bfs.min(), vmax=bfs.max()), center=0)\n",
    "    plt.show()\n",
    "    sns.barplot(x=labels, y=probs)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yw9v6s2i6MNZ"
   },
   "outputs": [],
   "source": [
    "def run_bayes_factors(obs, outputs, chordtypes, nsteps=500):\n",
    "    params, hist = bf_infer_model_posterior(obs, outputs, chordtypes,\n",
    "                                            nsteps=nsteps, subsamples=None, particles=1,\n",
    "                                            plot_loss=True, save_as=None)\n",
    "    return params, hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GdBFIMnS6MNa"
   },
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KBeuSNBT6MNa"
   },
   "outputs": [],
   "source": [
    "# histories\n",
    "\n",
    "def plot_param_history(history):\n",
    "    df = pd.DataFrame(history)\n",
    "    df.plot()\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.show()\n",
    "    \n",
    "def plot_roots(root_history, ylabel='root parameters'):\n",
    "    plt.plot(root_history)\n",
    "    plt.xlabel(\"iteration\")\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.show()\n",
    "\n",
    "def plot_histories(param_history, root_history, harm_history):\n",
    "    plot_param_history(param_history)\n",
    "    plot_roots(root_history)\n",
    "    plot_roots(harm_history, ylabel='chord type parameters')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J0sdfFtX6MNb"
   },
   "outputs": [],
   "source": [
    "# posteriors\n",
    "\n",
    "# posterior of 'rate_notes'\n",
    "def plot_note_rate(params, lower=0, upper=10):\n",
    "    alpha = params['alpha_rate_notes']\n",
    "    beta = params['beta_rate_notes']\n",
    "    print(alpha)\n",
    "    print(beta)\n",
    "    x = np.linspace(lower, upper, 200)\n",
    "    y = stats.gamma.pdf(x, alpha, scale=1/beta)\n",
    "    plt.plot(x,y)\n",
    "    plt.xlabel('rate_notes')\n",
    "    plt.show()\n",
    "    xrate = np.linspace(0,10,11)\n",
    "    yrate = stats.nbinom.pmf(xrate, alpha, 1/(1+1/beta))\n",
    "    plt.bar(xrate+1, yrate)\n",
    "    plt.xlabel('nnotes')\n",
    "    plt.show()\n",
    "    \n",
    "def plot_note_rates(phist, n=100, lower=0, upper=10):\n",
    "    alphas = phist['alpha_rate_notes'][-n:]\n",
    "    betas = phist['beta_rate_notes'][-n:]\n",
    "    x = np.linspace(lower, upper, 200)\n",
    "    ys = np.array([stats.gamma.pdf(x, a, scale=1/b) for (a,b) in zip(alphas,betas)]).transpose()\n",
    "    plt.plot(x,ys, color='steelblue', alpha=0.5)\n",
    "    plt.xlabel('rate_notes')\n",
    "    plt.show()\n",
    "    \n",
    "# posterior of 'p_is_chordtone'\n",
    "def plot_p_ict(params, harmtypes, cluster_assignment, lower=0, upper=1):\n",
    "    alphas = params[\"alpha_p_ict\"]\n",
    "    betas  = params[\"beta_p_ict\"]\n",
    "    x = np.linspace(lower, upper, 200)\n",
    "    y = np.array([stats.beta.pdf(x, a, b) for a, b in zip(alphas, betas)]).transpose()\n",
    "    names = dict()\n",
    "    for chord, cluster in cluster_assignment.items():\n",
    "        name = harmtypes[chord]\n",
    "        if(names.get(cluster) == None):\n",
    "            names[cluster] = []\n",
    "        names[cluster].append(name)\n",
    "    # This might work\n",
    "    its = list(names.items())\n",
    "    its.sort()\n",
    "    ns = [str.join(\", \",it[1]) for it in its]\n",
    "    plt.plot(x,y)\n",
    "    plt.xlabel(\"p_is_chordtone\")\n",
    "    plt.legend(ns, bbox_to_anchor=(1., 1), loc='upper left')\n",
    "    plt.show()\n",
    "\n",
    "# posterior of chord type probabilities\n",
    "def plot_chord_type_dist(params, labels):\n",
    "    plt.figure(figsize=(6,9))\n",
    "    alphas = params['params_p_harmony']\n",
    "    plt.barh(np.arange(len(alphas)), alphas, tick_label=labels)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.xlabel(\"params_p_harmony\")\n",
    "    plt.show()\n",
    "\n",
    "# posteriors of note probabilities\n",
    "def plot_chords(params, labels):\n",
    "    post_chordtones = params['params_p_chordtones']\n",
    "    post_ornaments = params['params_p_ornaments']\n",
    "    for i, name in enumerate(labels):\n",
    "        utils.plot_profile(post_chordtones[i], post_ornaments[i], name)\n",
    "        utils.play_chord(post_chordtones[i])\n",
    "\n",
    "# plot all posteriors\n",
    "def plot_posteriors(params, chordtypes):\n",
    "    plot_note_rate(params)\n",
    "    plot_p_ict(params, chordtypes)\n",
    "    plot_chord_type_dist(params, chordtypes)\n",
    "    plot_chords(params, chordtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1yPLcA6w6MNb"
   },
   "source": [
    "## Clustering (DCML Corpus)\n",
    "\n",
    "The DCML corpus is a collection of classical pieces with elaborate harmonic annotations.\n",
    "Here we only distinguish the basic harmonic types defined in the annotation standard (triads and seventh chords),\n",
    "since the extra information (inversion, suspensions, added notes etc.) do not change the type of the chord."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QJxgyhAq6MNb"
   },
   "outputs": [],
   "source": [
    "# prepare the dataset\n",
    "dcml_df, dcml_obs, dcml_chordtypes = load_dataset('data/dcml.tsv', dcml_chordtype_map)\n",
    "gc.collect()\n",
    "debug_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "57HfRmV6NFEK"
   },
   "outputs": [],
   "source": [
    "dcml_init = get_init_params(dcml_df, len(dcml_chordtypes), npcs)\n",
    "gc.collect()\n",
    "debug_tensors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ucseKp_6MNc"
   },
   "outputs": [],
   "source": [
    "print(dcml_chordtypes)\n",
    "#print(dcml_init)\n",
    "#for init in dcml_inits:\n",
    "#    print(init['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JfQP_EOA6MNc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dcml_outputs = iterative_clustering(dcml_init, dcml_chordtypes, dcml_obs, \"dcml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K3DU9km_6MNc"
   },
   "outputs": [],
   "source": [
    "dcml_outputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z7vfaa0F6MNc",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the histories the parameters to check convergence\n",
    "for output in dcml_outputs:\n",
    "    plot_histories(output['hist'], output['roots'], output['harm'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QlhnKAsC6MNd",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for output in dcml_outputs:\n",
    "    # plot the posterior distributions of the parameters\n",
    "    #plot_note_rate(output['params'], lower=5.5, upper=5.65)\n",
    "    #plot_note_rates(output['hist'], n=50, lower=5.5, upper=5.65)\n",
    "    plot_p_ict(output['params'], dcml_chordtypes, output['cluster_assignment'], lower=0.7, upper=0.95)\n",
    "    #plot_chord_type_dist(output['params'], dcml_chordtypes)\n",
    "    #plot_chords(output['params'], dcml_chordtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K1v_bMkw6yK-"
   },
   "source": [
    "## Bayes Factor (DCML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KVnKTzWh6MNd"
   },
   "outputs": [],
   "source": [
    "dcml_cmp, dcml_cmp_hist = run_bayes_factors(dcml_obs, dcml_outputs, dcml_chordtypes, nsteps=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_C69PJ956MNe"
   },
   "outputs": [],
   "source": [
    "plot_param_history(dcml_cmp_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YeGm7fvT6MNe"
   },
   "outputs": [],
   "source": [
    "dcml_bfs, dcml_m_params = bf_bayes_factors(dcml_cmp)\n",
    "plot_bayes_factors(dcml_bfs, dcml_m_params)\n",
    "dcml_bfs[8,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "v-GyvF8t6MNe"
   },
   "source": [
    "## Clustering (Wikifonia Corpus)\n",
    "\n",
    "The Wikifonia dataset consists of leadsheets, i.e. melodies and chord labels.\n",
    "It uses the chord types set in the MusicXML source of the chord-labels, which can be quite chaotic.\n",
    "Therefore, we normalize the chord-types to the ones defined in the MusicXML standard,\n",
    "removing unclear chord labels (which are rather rare)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ip0rzBT36MNe"
   },
   "outputs": [],
   "source": [
    "# prepare the dataset\n",
    "wiki_df, wiki_obs, wiki_chordtypes = load_dataset('data/wikifonia.tsv', wiki_chordtype_map)\n",
    "wiki_init = get_init_params(wiki_df, len(wiki_chordtypes), npcs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WS2FABC76MNf"
   },
   "outputs": [],
   "source": [
    "print(wiki_chordtypes)\n",
    "#for init in wiki_inits:\n",
    "    #print(init['cluster'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMugYsRf6MNf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wiki_outputs = iterative_clustering(wiki_init, wiki_chordtypes, wiki_obs, \"wiki\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Drl122FA6MNg"
   },
   "outputs": [],
   "source": [
    "# plot the histories the parameters to check convergence\n",
    "for output in wiki_outputs:\n",
    "    plot_histories(output['hist'], output['roots'], output['harm'])\n",
    "#plot_histories(whist, wroots, wharm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D1d421bK6MNg",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot the posterior distributions of the parameters\n",
    "for output in wiki_outputs:\n",
    "    # plot the posterior distributions of the parameters\n",
    "    #plot_note_rate(output['params'], lower=2.35, upper=2.45)\n",
    "    #plot_note_rates(output['hist'], n=50, lower=2.35, upper=2.45)\n",
    "    plot_p_ict(output['params'], wiki_chordtypes, output['cluster_assignment'], lower=0.65, upper=0.95)\n",
    "    #plot_chord_type_dist(output['params'], wiki_chordtypes)\n",
    "    #plot_chords(output['params'], wiki_chordtypes)\n",
    "#plot_note_rate(wiki_params, lower=2.35, upper=2.45)\n",
    "#plot_note_rates(whist, n=100, lower=2.35, upper=2.45)\n",
    "#plot_p_ict(wiki_params, wiki_chordtypes, lower=0.65, upper=0.95)\n",
    "#plot_chord_type_dist(wiki_params, wiki_chordtypes)\n",
    "#plot_chords(wiki_params, wiki_chordtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E3RNdhp4VzHu"
   },
   "source": [
    "## Bayes Factor (Wikifonia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ii-9gtyNJlf-"
   },
   "outputs": [],
   "source": [
    "wiki_cmp, wiki_cmp_hist = run_bayes_factors(wiki_obs, wiki_outputs, wiki_chordtypes, nsteps=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "risA6JC4JlgC"
   },
   "outputs": [],
   "source": [
    "plot_param_history(wiki_cmp_hist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SGzoLRVIJlgE"
   },
   "outputs": [],
   "source": [
    "wiki_bfs, wiki_m_params = bf_bayes_factors(wiki_cmp)\n",
    "plot_bayes_factors(wiki_bfs, wiki_m_params)\n",
    "wiki_bfs[8,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FgBEgX8wJZJt"
   },
   "source": [
    "# Rest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "THE2I6J16MNg"
   },
   "outputs": [],
   "source": [
    "len(wiki_inits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1uPOfQwb6MNh"
   },
   "outputs": [],
   "source": [
    "wiki_outputs[0]['wiki_params']['params_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d87R_46m6MNh"
   },
   "outputs": [],
   "source": [
    "wiki_inits[5]['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BBmn3kDf6MNi"
   },
   "outputs": [],
   "source": [
    "dcml_outputs[0]['dcml_params']['params_cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tcbJtPic6MNi"
   },
   "outputs": [],
   "source": [
    "dcml_inits[5]['cluster']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqFxxlLN6MNi"
   },
   "outputs": [],
   "source": [
    "dcml_inits[5]['is_ct']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JtxuONdy6MNj"
   },
   "outputs": [],
   "source": [
    "dcml_inits[5]['is_or']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gKZ4Ff4m6MNj"
   },
   "outputs": [],
   "source": [
    "dcml_outputs[0]['dcml_params']['alpha_p_ict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J9zk-T5j6MNk"
   },
   "outputs": [],
   "source": [
    "dcml_outputs[0]['dcml_params']['beta_p_ict']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "quSv97qd6MNk"
   },
   "outputs": [],
   "source": [
    "stats.beta.mean(dcml_inits[5]['is_ct'][0],dcml_inits[5]['is_or'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sjond3E06MNk"
   },
   "outputs": [],
   "source": [
    "stats.beta.mean(dcml_inits[5]['is_ct'][1],dcml_inits[5]['is_or'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V_-cY_-S6MNl"
   },
   "outputs": [],
   "source": [
    "stats.beta.mean(dcml_inits[5]['is_ct'][5],dcml_inits[5]['is_or'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bARshw7E6MNl"
   },
   "outputs": [],
   "source": [
    "stats.beta.mean(dcml_outputs[0]['dcml_params']['alpha_p_ict'][5],dcml_outputs[0]['dcml_params']['beta_p_ict'][5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_xmI7PKS6MNl"
   },
   "outputs": [],
   "source": [
    "stats.beta.mean(dcml_outputs[0]['dcml_params']['alpha_p_ict'][1],dcml_outputs[0]['dcml_params']['beta_p_ict'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r1LV_muA6MNm"
   },
   "outputs": [],
   "source": [
    "stats.beta.mean(dcml_inits[13]['is_ct'][6],dcml_inits[13]['is_or'][6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9tmzqqB6MNm"
   },
   "outputs": [],
   "source": [
    "stats.beta.mean(dcml_inits[13]['is_ct'][7],dcml_inits[13]['is_or'][7])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pWLMe0Ks6MNm"
   },
   "outputs": [],
   "source": [
    "stats.beta.mean(dcml_inits[13]['is_ct'][9],dcml_inits[13]['is_or'][9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rwP4uRTG6MNm"
   },
   "outputs": [],
   "source": [
    "stats.beta.mean(dcml_inits[13]['is_ct'][0],dcml_inits[13]['is_or'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ishPsBEK6MNm"
   },
   "outputs": [],
   "source": [
    "stats.beta.mean(dcml_inits[13]['is_ct'][1],dcml_inits[13]['is_or'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUNhaxZY6MNm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "@webio": {
   "lastCommId": null,
   "lastKernelId": null
  },
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "fTE5VdZ46MNW",
    "EqnikMsp6MNX",
    "GdBFIMnS6MNa",
    "1yPLcA6w6MNb",
    "E3RNdhp4VzHu"
   ],
   "name": "model_paper_clusters_iterative.ipynb",
   "private_outputs": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "dcml-harmony-and-ornamentation",
   "language": "python",
   "name": "dcml-harmony-and-ornamentation"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
